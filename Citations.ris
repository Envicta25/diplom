TY  - CONF
TI  - A Novel MLCNN for Aircraft Target Classification in Optical Remote Sensing Images with Small Samples
T2  - 2022 34th Chinese Control and Decision Conference (CCDC)
SP  - 6098
EP  - 6101
AU  - X. Tian
AU  - Y. Liu
AU  - X. Ji
PY  - 2022
KW  - Deep learning
KW  - Transfer learning
KW  - Optical computing
KW  - Optical fiber networks
KW  - Optical imaging
KW  - Optical sensors
KW  - Optical reflection
KW  - Aircraft target classification
KW  - MLCNN
KW  - Optical remote sensing Images
KW  - Small samples
DO  - 10.1109/CCDC55256.2022.10033476
JO  - 2022 34th Chinese Control and Decision Conference (CCDC)
IS  - 
SN  - 1948-9447
VO  - 
VL  - 
JA  - 2022 34th Chinese Control and Decision Conference (CCDC)
Y1  - 15-17 Aug. 2022
AB  - In this paper, the use of deep learning method for optical remote sensing image classification of aircraft targets is studied. Deep neural networks show great power in the field of image processing and analysis mainly by using a very large number of training samples. However, sufficient samples cannot be obtained in the field of many image analysis fields including optical remote sensing image classification. Image classification based on small samples as a research direction in the field of image analysis has become one of the research hotspots. In this paper, a novel multi-layer convolutional neural network (MLCNN) is proposed for optical remote sensing image classification of aircraft targets with small samples. The MLCNN has only 3 convolutional layers, which shows more effective for image classification with small samples. In the experiment, transfer learning is first performed on the Alexnet network to achieve image classification tasks, then, the MLCNN is executed. And different numbers of samples from OPT-Aircraft _v1.0 are used to test the performance of Alexnet network and the proposed network. Experimental results demonstrate that the proposed MLCNN achieves higher classification accuracy.
ER  - 


TY  - CONF
TI  - Research on Aircraft Type Recognition from Remote Sensing Images in Complex Scenes
T2  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
SP  - 995
EP  - 1000
AU  - J. Wang
AU  - L. Xia
AU  - S. Li
PY  - 2021
KW  - Information science
KW  - Image recognition
KW  - Image resolution
KW  - Target recognition
KW  - Feature extraction
KW  - Classification algorithms
KW  - Aircraft
KW  - aircraft inspection
KW  - Type identification
KW  - YOLOv5
KW  - MMAL-Net
DO  - 10.1109/CISAI54367.2021.00199
JO  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
Y1  - 17-19 Sept. 2021
AB  - Aircraft targets in remote sensing images are important research objects. Accurately identifying different types of aircraft will bring the greatest value of intelligence into play. In this paper, we combine the YOLOv5 detection algorithm and the MMAL-Net classification algorithm to detect the aircraft target and identify the aircraft type in the remote sensing image. Firstly, we use the YOLOv5 algorithm to determine the area where the aircraft target is located in the complex scene. Then the MMAL-Net algorithm is used to classify the aircraft in the area at the fine-grained level, and the type of aircraft target is identified. Through the experiment, the method proposed in this paper can accurately detect the aircraft target and identify the aircraft type, and the accuracy is 73. 2%.
ER  - 


TY  - CONF
TI  - Aircraft Type Classification in Remote Sensing Images using Deep Learning
T2  - 2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)
SP  - 1
EP  - 6
AU  - Y. B. Youssef
AU  - M. Merrouchi
AU  - E. Abdelmounim
AU  - T. Gadi
PY  - 2020
KW  - Aircraft
KW  - Training
KW  - Remote sensing
KW  - Convolution
KW  - Feature extraction
KW  - Testing
KW  - Task analysis
KW  - Computer vision
KW  - Machine learning
KW  - Deep learning
KW  - Convolutional Neural Network
KW  - Classification
DO  - 10.1109/ICECOCS50124.2020.9314611
JO  - 2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)
Y1  - 2-3 Dec. 2020
AB  - In aeronautic, redundancy of data is strongly desired to make optimal decisions. A large data source is available with the development of the technology used in remote sensing images. Remote Sensing Image Classification (RSIC) is widely exploited in military and civil fields. To improve the performance of multi-label classification, we addressed the problem of RSIC based on the Convolutional Neural Network (CNN) for remote sensing images of aircraft types. Previous studies have used intensive preprocessing which limits the rate of classification. We improved the network structure to make it more accurate and to limit underfitting or overfitting problems. A recent public dataset called Multi-Type Aircraft Remote Sensing Images (MTARSI) is used in this work to validate our method. Extensive experiments prove the effectiveness of the proposed method in terms of accuracy.
ER  - 


TY  - CONF
TI  - Algorithms for radar image identification and classification
T2  - 2010 International Conference on High Performance Computing & Simulation
SP  - 418
EP  - 424
AU  - V. Zeljković
AU  - C. Tameze
AU  - R. Vincelette
PY  - 2010
KW  - Aircraft
KW  - Classification algorithms
KW  - Shape
KW  - Radar imaging
KW  - Pixel
KW  - Airborne radar
KW  - ISAR images
KW  - Doppler shifts
KW  - radar pulse
KW  - classification
KW  - pattern recognition
DO  - 10.1109/HPCS.2010.5547098
JO  - 2010 International Conference on High Performance Computing & Simulation
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2010 International Conference on High Performance Computing & Simulation
Y1  - 28 June-2 July 2010
AB  - We present and compare two different novel methods for classification of aircraft categories of Inverse Synthetic Aperture Radar (ISAR) images. The first method forms numerical equivalents to shape, size and other aircraft features as critical criteria to constitute the algorithm for their correct classification. The second method compares each ISAR image to unions of images of the different aircraft categories. We computer simulated five different categories of ISAR images and took two more from the internet. ISAR images are constructed based on the Doppler shifts of various parts, caused by the rotation of the aircraft and the radar reflection pulse shape which includes the size or duration of the radar pulse. The proposed classification algorithms were tested on these seven categories. All seven different aircraft models are flying a holding pattern. The aim of both algorithms is to quickly match and determine the similarity of the captured aircraft to the seven different categories where the aircraft is in any position of a prescribed holding pattern. Our experimental results clearly indicate that in most parts of the holding pattern the category of the aircraft can be successfully identified with both proposed methods. The union method shows more successful identification results and is superior to the results we obtained in the first proposed method.
ER  - 


TY  - CONF
TI  - Identification and Classification of Drop Zones and Helicopter Landing Zones in Images Obtained by Small Size Remotely Piloted Aircraft Systems
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 7906
EP  - 7909
AU  - M. G. Lacerda
AU  - A. De Carvalho Paulino
AU  - E. H. Shiguemori
AU  - A. J. Damiao
AU  - L. N. F. Guimaraes
AU  - C. S. dos Anjos
PY  - 2018
KW  - Helicopters
KW  - Decision trees
KW  - Feature extraction
KW  - Image segmentation
KW  - Disaster management
KW  - Military aircraft
KW  - Disaster Management
KW  - Remotely Piloted Aircraft Systems
KW  - Helicopter Landing Zone
KW  - Decision Tree
DO  - 10.1109/IGARSS.2018.8517753
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - In disaster management activities or conflict, it is of utmost importance to have accurate information about the terrain. Due to the practicality of the Remotely Piloted Aircraft Systems (RPAS), this paper proposes the use of these tools at the identification of Drop Zones (DZs), designated as areas into which troops or supplies are dropped by parachute, and Helicopter Landing Zones (HLZs), that can play an important role in the rescue of victims. The use of digital image processing enables the automatic generation, in a short amount of time and with good precision, of an orthorectified mosaic and a Digital Surface Model (DSM), which are fundamental to the terrain's comprehension. Afterward, new attributes are generated from the original image and such information is used by the decision trees to determine the areas of interest. The obtained results indicate that all the areas extracted really can be useful for the purpose.
ER  - 


TY  - CONF
TI  - Convolutional Modulated Scattering Feature Network for Aircraft Classification in SAR Images
T2  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
SP  - 9329
EP  - 9332
AU  - Z. Ye
AU  - X. Xiao
AU  - H. Wang
PY  - 2024
KW  - Convolution
KW  - Target recognition
KW  - Scattering
KW  - Feature extraction
KW  - Optical imaging
KW  - Radar polarimetry
KW  - Adaptive optics
KW  - Synthetic Aperture Radar
KW  - scattering feature
KW  - Convolutional Modulation
KW  - Aircraft Classification
DO  - 10.1109/IGARSS53475.2024.10641325
JO  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 7-12 July 2024
AB  - Synthetic aperture radar (SAR) images are widely utilized for the detection and recognition of aircraft targets. Unlike optical images, SAR images possess the advantage of being applicable in all weather conditions and at all times of the day. However, in contrast to optical images, aircraft typically appear as discrete points in SAR images, and their outlines are not distinctly clear. To effectively use the scatter information of aircraft in SAR images, a convolutional modulation scattering feature network (CMSF) is proposed in this paper. Firstly, a scattering feature extraction module is introduced to make full advantage of scattering information. Secondly, following convolution processing, the convolution modulation module is employed to generate a similar fraction matrix. Thirdly, the fusion of scattering features and convolution features is achieved through convolutional modulation and matrix multiplication. Finally, a four-stage convolutional processing is employed to recognize the aircraft target. Extensive experiments conducted on the SARAircraft-1.0 dataset demonstrate the effectiveness of the convolutional modulated scattering feature network for aircraft target classification in SAR images.
ER  - 


TY  - CONF
TI  - Combining wavelet transforms and neural networks for image classification
T2  - 2009 41st Southeastern Symposium on System Theory
SP  - 44
EP  - 48
AU  - M. Lotfi
AU  - A. Solimani
AU  - A. Dargazany
AU  - H. Afzal
AU  - M. Bandarabadi
PY  - 2009
KW  - Wavelet transforms
KW  - Neural networks
KW  - Image classification
KW  - Color
KW  - Testing
KW  - Shape
KW  - Data mining
KW  - Feature extraction
KW  - Image databases
KW  - Aircraft
KW  - Wavelet Transform
KW  - Neural Network
KW  - Image Classification
KW  - Color Moment
DO  - 10.1109/SSST.2009.4806819
JO  - 2009 41st Southeastern Symposium on System Theory
IS  - 
SN  - 2161-8135
VO  - 
VL  - 
JA  - 2009 41st Southeastern Symposium on System Theory
Y1  - 15-17 March 2009
AB  - A new approach for image classification based on the color information, shape and texture is presented. In this work, we use the three RGB bands of a color image in RGB model to extract the describing features. All the images in image database are divided into 6 parts. We use the Daubechies 4 wavelet transform and first order color moments to obtain the necessary information from each part of the image. The proposed image classification system is based on Back propagation neural network with one hidden layer. Color moments and wavelet decomposition coefficients from each part of the image are used as an input vector of neural network. 150 color images of aircrafts were used for training and 250 for testing. The best efficiency of 98% was obtained for training set, and 90% for the testing set.
ER  - 


TY  - CONF
TI  - Hierarchical features learning with convolutional neural networks based on aircraft recognition on images from remote sensing image
T2  - 2016 IEEE International Conference on Consumer Electronics-China (ICCE-China)
SP  - 1
EP  - 5
AU  - L. Jian-min
AU  - Y. Min-hua
AU  - X. Yu
PY  - 2016
KW  - Feature extraction
KW  - Neural networks
KW  - Aircraft
KW  - Remote sensing
KW  - Unsupervised learning
KW  - Conferences
KW  - Image recognition
KW  - Hierarchical Features
KW  - Convolutional Neural Networks
KW  - Aircraft Recognition
DO  - 10.1109/ICCE-China.2016.7849736
JO  - 2016 IEEE International Conference on Consumer Electronics-China (ICCE-China)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 IEEE International Conference on Consumer Electronics-China (ICCE-China)
Y1  - 19-21 Dec. 2016
AB  - The features construction steps of classic machine learning algorithms must be completed by trained researchers with professional knowledge. We mainly analyzed on the results of each layer of unsupervised learning of hierarchical representations with convolutional neural network on aircraft detection. The experiment results showed unsupervised learning features having obviously distinguishing ability and helping aircraft recognition on remote sensing image.
ER  - 


TY  - CONF
TI  - Comparison of aircraft target recognition methods based on ISAR images
T2  - 2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
SP  - 1291
EP  - 1295
AU  - F. Ma
AU  - Y. He
AU  - Y. Li
PY  - 2022
KW  - Support vector machines
KW  - Image recognition
KW  - Target tracking
KW  - Target recognition
KW  - Military aircraft
KW  - Classification algorithms
KW  - Aircraft
KW  - ISAR images
KW  - aircraft targets
KW  - KNN
KW  - SVM
DO  - 10.1109/ITAIC54216.2022.9836715
JO  - 2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
IS  - 
SN  - 2693-2865
VO  - 10
VL  - 10
JA  - 2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
Y1  - 17-19 June 2022
AB  - Air warfare is a complex and dynamic environment. In order to gain the initiative in warfare, all kinds of preparations must be made thoroughly before the battle. For example, the acquisition of battlefield information, the processing of battlefield information, the identification and tracking of enemy targets and so on. With absolute information superiority and one-way transparency on the battlefield, it is easier to hit targets faster and more accurately. Therefore, image based information acquisition, processing and target identification and tracking technology has become a popular military research topic internationally. By imaging the aircraft target data through MATLAB, the ISAR images of different aircraft targets can be compared about two recognition methods, KNN and SVM, using Python. For this idea, the comparison of aircraft target recognition methods is proposed to compare different aircraft target data recognition and verify the recognition effect of aircraft targets under different classifiers.
ER  - 


TY  - JOUR
TI  - SRARNet: A Unified Framework for Joint Superresolution and Aircraft Recognition
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 327
EP  - 336
AU  - W. Tang
AU  - C. Deng
AU  - Y. Han
AU  - Y. Huang
AU  - B. Zhao
PY  - 2021
KW  - Aircraft
KW  - Image recognition
KW  - Task analysis
KW  - Remote sensing
KW  - Generative adversarial networks
KW  - Aircraft manufacture
KW  - Generators
KW  - Aircraft recognition
KW  - multitask GAN
KW  - superresolution
DO  - 10.1109/JSTARS.2020.3037225
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 14
VL  - 14
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2021
AB  - Aircraft recognition in high-resolution remote sensing images has rapidly progressed with the advance of convolutional neural networks (CNNs). However, the previous CNN-based methods may not work well for recognizing aircraft in low-resolution remote sensing images because the blurred aircraft in these images offer insufficient details to distinguish them from similar types of targets. An intuitive solution is to introduce superresolution preprocessing. However, conventional superresolution methods mainly focus on reconstructing natural images with detailed texture rather than constructing a high-resolution object with strong discriminative information for the recognition task. To address these problems, we propose a unified framework for joint superresolution and aircraft recognition (Joint-SRARNet) that tries to improve the recognition performance by generating discriminative, high-resolution aircraft from low-resolution remote sensing images. Technically, this network integrates superresolution and recognition tasks into the generative adversarial network (GAN) framework through a joint loss function. The generator is constructed as a joint superresolution and refining subnetwork that can upsample small blurred images into high-resolution ones and restore high-frequency information. In the discriminator, we introduce a new classification loss function that forces the discriminator to distinguish between real and fake images while recognizing the type of aircraft. In addition, the classification loss function is back-propagated to the generator to obtain high-resolution images with discriminative information for easier recognition. Extensive experiments on the challenging multitype aircraft of remote sensing images (MTARSI) dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a small blurred image and significant improvement in the recognition performance. To our knowledge, this is the first work on joint superresolution and aircraft recognition tasks.
ER  - 


TY  - CONF
TI  - LR-CNN for fine-grained classification with varying resolution
T2  - 2015 IEEE International Conference on Image Processing (ICIP)
SP  - 3101
EP  - 3105
AU  - M. Chevalier
AU  - N. Thome
AU  - M. Cord
AU  - J. Fournier
AU  - G. Henaff
AU  - E. Dusch
PY  - 2015
KW  - Image resolution
KW  - Feature extraction
KW  - Aircraft
KW  - Context
KW  - Training
KW  - Instruments
KW  - Computer architecture
KW  - Fisher vectors
KW  - Convolutional neural networks
KW  - Fine-grained classification
KW  - Image recognition
DO  - 10.1109/ICIP.2015.7351374
JO  - 2015 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE International Conference on Image Processing (ICIP)
Y1  - 27-30 Sept. 2015
AB  - In this work, we present an extended study of image representations for fine-grained classification with respect to image resolution. Understudied in literature, this parameter yet presents many practical and theoretical interests, e.g. in embedded systems where restricted computational resources prevent treating high-resolution images. It is thus interesting to figure out which representation provides the best results in this particular context. On this purpose, we evaluate Fisher Vectors and deep representations on two significant finegrained oriented datasets: FGVC Aircraft [1] and PPMI [2]. We also introduce LR-CNN, a deep structure designed for classification of low-resolution images with strong semantic content. This net provides rich compact features and outperforms both pre-trained deep features and Fisher Vectors.
ER  - 


TY  - CONF
TI  - Feature-based aircraft identification in Hi-Res airborne ISAR images
T2  - 2012 Tyrrhenian Workshop on Advances in Radar and Remote Sensing (TyWRRS)
SP  - 26
EP  - 29
AU  - N. Ricardi
AU  - A. Aprile
AU  - F. Dell'Acqua
PY  - 2012
KW  - Feature extraction
KW  - Radar imaging
KW  - Aircraft
KW  - Radar cross section
KW  - Airborne radar
KW  - Military aircraft
KW  - Target classification
KW  - aircrafts
KW  - airborne radar
KW  - Inverse synthetic aperture radar
DO  - 10.1109/TyWRRS.2012.6381097
JO  - 2012 Tyrrhenian Workshop on Advances in Radar and Remote Sensing (TyWRRS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2012 Tyrrhenian Workshop on Advances in Radar and Remote Sensing (TyWRRS)
Y1  - 12-14 Sept. 2012
AB  - In this paper we propose a new method for aircraft identification and classification using high-resolution ISAR images acquired from an airborne radar sensor. The proposed method takes advantage of some of the geometric features extracted from the image, and some signal features as well (like the Jet Engine Modulation phenomenon that allows us detecting and analyzing the engines of the unknown aircraft). In the next session we will provide a brief explanation of the problem, in order to show the proposed method. An ad hoc classification database has been developed, containing physical characteristics of some aircrafts. Features extracted from radar data have been compared with the database content for classification and some preliminary results have already been obtained. These results appear encouraging and will be shown in the paper.
ER  - 


TY  - CONF
TI  - A novel technique for feature-based aircraft identification from high resolution airborne ISAR images
T2  - 2012 IEEE International Geoscience and Remote Sensing Symposium
SP  - 2082
EP  - 2085
AU  - N. Ricardi
AU  - A. Aprile
AU  - F. Dell'Acqua
PY  - 2012
KW  - Feature extraction
KW  - Radar imaging
KW  - Aircraft
KW  - Radar cross section
KW  - Airborne radar
KW  - Military aircraft
KW  - Target classification
KW  - aircrafts
KW  - airborne radar
KW  - Inverse synthetic aperture radar
DO  - 10.1109/IGARSS.2012.6350962
JO  - 2012 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2012 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2012
AB  - In this paper we propose a new method for aircraft identification and classification using high-resolution ISAR images acquired from an airborne radar sensor. The proposed method takes advantage of some of the geometric features extracted from the image, and some signal features as well (like the Jet Engine Modulation phenomenon that allows us detecting and analyzing the engines of the unknown aircraft). In the next session we will provide a brief explanation of the problem, in order to show the proposed method. An ad hoc classification database has been developed, containing physical characteristics of some aircrafts. Features extracted from radar data have been compared with the database content for classification and some preliminary results have already been obtained. These results appear encouraging and will be shown in the paper.
ER  - 


TY  - CONF
TI  - Aircraft recognition in remote sensing images based on deep learning
T2  - 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)
SP  - 895
EP  - 899
AU  - J. Lin
AU  - X. Li
AU  - H. Pan
PY  - 2018
KW  - Aircraft
KW  - Feature extraction
KW  - Training
KW  - Convolution
KW  - Image recognition
KW  - Remote sensing
KW  - Military aircraft
KW  - HOG feature
KW  - Deep learning
KW  - Aircraft recognition
DO  - 10.1109/YAC.2018.8406498
JO  - 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)
Y1  - 18-20 May 2018
AB  - Object recognition is one of the fundamental issues in the field of computer vision. In traditional methods, invariant features are extracted from segmented targets for recognition. However, there is no common method for segmentation of aircraft targets so far due to the complex backgrounds, illuminations, noise and other practical factors. Therefore, in this paper, we propose a method for aircraft identification in remote sensing images based on HOG and deep learning features. We train two classifiers, one is the SVM classifier based on HOG feature, and the other is a classifier based on deep convolutional neural network VGGNet. First, we use the SVM classifier to identify the aircraft in the picture roughly, then we use the deep learning classifier to exclude misidentified targets. In this way, this coarse to fine framework can significantly improve the speed and accuracy of aircraft recognition in remote sensing images. At the same time, our method has a better generalization capability than the traditional methods. Experimental results demonstrate the robustness of our method.
ER  - 


TY  - CONF
TI  - Fusion of Enhancing Target Features and Focusing Slice Features for Soil Image Classification
T2  - 2023 6th International Conference on Software Engineering and Computer Science (CSECS)
SP  - 1
EP  - 7
AU  - X. Zheng
AU  - J. Wang
AU  - X. Bai
PY  - 2023
KW  - Computational modeling
KW  - Supervised learning
KW  - Focusing
KW  - Soil
KW  - Feature extraction
KW  - Classification algorithms
KW  - Image classification
KW  - soil classification
KW  - fine-grained image classification
KW  - object localization
KW  - self-attention
KW  - channel attention
DO  - 10.1109/CSECS60003.2023.10428636
JO  - 2023 6th International Conference on Software Engineering and Computer Science (CSECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 6th International Conference on Software Engineering and Computer Science (CSECS)
Y1  - 22-24 Dec. 2023
AB  - Soil classification based on the identification of soil surface and physical properties is a key point in calculating indicators such as stability, bearing capacity, and uniformity of construction sites in soil exploration. In this paper, soil image classification is regarded as a sub task of fine-grained image classification, and a soil image classification algorithm model integrating target feature enhancement and slice feature focusing is proposed. This model constructs a three branch network for Supervised learning. First, the backbone network obtains the overall characteristics of the image; Then, the image position is obtained through cross level feature fusion to reduce background noise interference and enhance target features; Finally, based on the feature mapping relationship of the target image, a local region with the highest degree of differentiation and the lowest degree of redundancy is proposed to achieve slice feature focusing. The proposed model has achieved competitive results on CUB-200-2011, Stanford Cars, FGVC-Aircraft and Soil.
ER  - 


TY  - JOUR
TI  - SHAP-Assisted Resilience Enhancement Against Adversarial Perturbations in Optical and SAR Image Classification
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 1
EP  - 5
AU  - A. H. Oveis
AU  - A. Cantelli-Forti
AU  - E. Giusti
AU  - M. Soltanpour
AU  - N. Rojhani
AU  - M. Martorella
PY  - 2025
KW  - Feature extraction
KW  - Perturbation methods
KW  - Training
KW  - Aircraft
KW  - Target recognition
KW  - Logistics
KW  - Calibration
KW  - Accuracy
KW  - Robustness
KW  - Radar imaging
KW  - Adversarial attacks
KW  - automatic target recognition (ATR)
KW  - classification
KW  - Shapley additive explanations (SHAP)
DO  - 10.1109/LGRS.2025.3536005
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 
SN  - 1558-0571
VO  - 22
VL  - 22
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - 2025
AB  - The increasing reliance on convolutional neural networks (CNNs) for automatic target recognition (ATR) in critical applications necessitates robust defenses against adversarial attacks, which can undermine their reliability. To address this challenge, this letter proposes a novel classification framework that enhances CNN robustness for ATR under adversarial perturbations. Although CNNs are renowned for their high recognition accuracy, their performance can be compromised by subtle adversarial perturbations designed to deceive the classifier. Our methodology is based on extracting specific features from Shapley additive explanations (SHAP) analysis within and outside the detected target area. These features are then used to train a multinomial logistic regression model using the training labels, and the trained regressor performs the classification. The key strength of our framework relies on robustness enhancement against adversarial attacks, particularly designed by the fast gradient sign method (FGSM). We validate our findings through extensive evaluations using two publicly available datasets: the multitype aircraft remote sensing images (MTARSI) dataset, which contains optical images of various aircraft types, and the moving and stationary target acquisition and recognition (MSTAR) dataset, which contains radar images.
ER  - 


TY  - CONF
TI  - Fine-grained classification for aero-engine borescope images based on the fusion of local and global features
T2  - 2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)
SP  - 389
EP  - 394
AU  - R. Huang
AU  - J. Zeng
AU  - X. Cheng
AU  - J. Wei
PY  - 2024
KW  - Codes
KW  - Accuracy
KW  - Federated learning
KW  - Diversity reception
KW  - Feature extraction
KW  - Proposals
KW  - Aircraft propulsion
KW  - Fine-grained image classification
KW  - aero-engine borescope image
KW  - aero-engine
KW  - feature fusion
KW  - image classification
DO  - 10.1109/CSCWD61410.2024.10580812
JO  - 2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)
IS  - 
SN  - 2768-1904
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)
Y1  - 8-10 May 2024
AB  - Borescope image classification is an essential preprocessing step for damage detection in aero-engines, which plays a crucial role in setting the detection threshold for later stages. However, traditional image classification methods such as ResNet or fine-grained image classification methods like FBSD perform poorly in classifying borescope images. To address this issue, we propose a novel and effective fine-grained classification method for aero-engine borescope images based on the fusion of local and global features. Our approach leverages a region proposal network to identify key local regions and combines the local features with global features for better classification. This enables us to distinguish between regions with high similarity and small inter-class differences. Additionally, we have created an aero-engine borescope image dataset with 5,158 images from 14 aero-engine components. Our method achieves a high classification accuracy of 98.31% on this dataset, outperforming other image classification and fine-grained image classification methods. Our implementation code is available at https://github.com/LonelyProceduralApe/LG_fusion
ER  - 


TY  - CONF
TI  - Image Classification and Text Identification in Inspecting Military Aircrafts Logos: Application of Convolutional Neural Network
T2  - 2022 IEEE International Symposium on Robotic and Sensors Environments (ROSE)
SP  - 01
EP  - 06
AU  - S. Edhah
AU  - A. Awadallah
AU  - M. Madboly
AU  - H. Dawed
AU  - N. Werghi
PY  - 2022
KW  - Machine learning algorithms
KW  - Optical character recognition
KW  - Inspection
KW  - Military aircraft
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Air traffic control
KW  - Convolutional Neural Network
KW  - Classification
KW  - Deep Learning
KW  - Logo Detection
KW  - Transfer Learning
KW  - Character Region Awareness for Text Detection
KW  - Optical Character Recognition
DO  - 10.1109/ROSE56499.2022.9977418
JO  - 2022 IEEE International Symposium on Robotic and Sensors Environments (ROSE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Symposium on Robotic and Sensors Environments (ROSE)
Y1  - 14-15 Nov. 2022
AB  - Object detection and inspection using images or videos have been receiving increased attention in many applications such as traffic control, brand monitoring, trademark compliance, and product authentication. A particular application that is currently a topic of interest is aircraft logo detection, which aims at automating the visual inspection carried out manually by aircraft engineers. Aircraft logos should meet a large set of requirements that include geometric constraints on the logo elements and patterns, and constraints on the position and orientation with respect to specific references. This work considers the design of a high accuracy convolutional neural network to detect and classify aircraft logos as either adequate or inadequate based on specified criteria. The performance of the developed network is compared to a number of classical machine learning algorithms to demonstrate its effectiveness. Adequate logos are then processed further by extracting them from a frame using robust features extraction algorithm and determining their orientation angle with respect to the horizontal reference axis. Afterward, a text detection technique using a character region awareness for text detection algorithm implemented on a pre-trained network is carried out, along with optical character recognition tool to detect and extract the text from the logos for further processing in other applications. The developed network is tested on actual aircraft logos, captured from the field, where satisfactory results are obtained.
ER  - 


TY  - CONF
TI  - Aircraft Target Detection in Remote Sensing Images Based on Improved Faster R-CNN
T2  - 2023 IEEE 5th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)
SP  - 302
EP  - 306
AU  - D. Wang
AU  - X. Li
AU  - M. Hao
PY  - 2023
KW  - Image quality
KW  - Object detection
KW  - Feature extraction
KW  - Classification algorithms
KW  - Sensors
KW  - Safety
KW  - Aircraft
KW  - faster R-CNN
KW  - remote sensing image
KW  - airplane
KW  - detection
DO  - 10.1109/ICCASIT58768.2023.10351602
JO  - 2023 IEEE 5th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 5th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)
Y1  - 11-13 Oct. 2023
AB  - This paper proposes an improved Faster R-CNN algorithm to improve the accuracy of aircraft detection in remote sensing images. The detection accuracy of existing algorithms is low in the case of complex background, dense target and poor image quality, so a more efficient and accurate algorithm is needed to solve this problem. The improved algorithm first improves the feature extraction network by using an efficient attention fusion module, thus improving the accuracy and efficiency of feature extraction. The Area Suggestion Network (RPN) is then used to generate candidate regions to improve the positioning accuracy of aircraft targets. Finally, the classification regression network of Faster R-CNN is used to obtain the detection results of aircraft targets to further improve the accuracy. In this paper, the UCAS-AOD dataset is used for experimental evaluation and comparison with other similar algorithms. The experimental results show that the algorithm achieves an average detection accuracy of 92.56% and an FPS of 41.97 with a few additional parameters, which proves that the algorithm has excellent performance and generalization ability in aircraft target detection in remote sensing images.
ER  - 


TY  - CONF
TI  - Rapid Aircraft Classification in Satellite Imagery using Fully Convolutional Residual Network
T2  - 2020 International Conference on Emerging Trends in Smart Technologies (ICETST)
SP  - 1
EP  - 5
AU  - S. N. Khan
AU  - S. I. A. Khan
AU  - Z. U. Abideen
AU  - M. S. Khan
AU  - S. Anwar
PY  - 2020
KW  - Deep Learning
KW  - CNN
KW  - Satellite Imagery
KW  - Image Classification
DO  - 10.1109/ICETST49965.2020.9080734
JO  - 2020 International Conference on Emerging Trends in Smart Technologies (ICETST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Emerging Trends in Smart Technologies (ICETST)
Y1  - 26-27 March 2020
AB  - Advancement in high-performance computing technology has paved way for development of Deep Learning algorithms for computer vision to provide unprecedented performance both in terms of accuracy and speed. Image recognition, a subfield of computer vision, is one of the key application areas in which deep learning based Convolutional Neural Networks (CNN have achieved ground-breaking performance). Majority of the algorithms of object classification in CNN are focused on street view imagery that is of high resolution and have small size. The problem with satellite imagery is that it has objects in small size and images are especially in large size. There are two main objectives of this research: time reduction of processing large dimensions satellite images, and achieving acceptable accuracy of classifying small aircrafts. For this purpose, ResNet-50 has been modified such that it can process high-resolution satellite imagery of large dimension in one go instead of processing it in small patches sequentially, without affecting the accuracy of object classification. ResNet-50 with sliding-window scanning technique and the proposed model trained on satellite imagery are compared. The proposed method reduces the processing time by 99.9% by keeping the accuracy at the same level.
ER  - 


TY  - CONF
TI  - Fine-Grained Image Classification Based on Self-Distillation and RFCAConv
T2  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
SP  - 378
EP  - 383
AU  - P. Wang
AU  - Y. Gu
AU  - Z. Tang
AU  - F. Zhou
PY  - 2024
KW  - Visualization
KW  - Accuracy
KW  - Filters
KW  - Convolution
KW  - Neural networks
KW  - Receivers
KW  - Feature extraction
KW  - Vectors
KW  - Standards
KW  - Image classification
KW  - fine-grained images classification
KW  - ResNet34
KW  - self-knowledge distillation
KW  - receptive field coordinated attention convolution
KW  - dynamic temperature
DO  - 10.1109/EIECS63941.2024.10799953
JO  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
Y1  - 27-29 Sept. 2024
AB  - To capture the details of fine-grained images accurately and address the challenges of significant intra-class variations and subtle inter-class similarities, a fine-grained image classification model based on self-knowledge distillation and receptive field coordinated attention convolution is proposed. This model aims to improve the classification accuracy of fine-grained images with subtle differences. The ResNet34 neural network is employed as the feature extraction backbone to extract features from fine-grained images. A self-distillation module is introduced to distill knowledge to soften hard targets, and the model is trained again with previously acquired knowledge to improve the sustainable utilization of meaningful knowledge. Besides, the receptive field coordinated attention convolutional module is utilized to replace the standard convolution operation to filters out redundant feature information and focuses on critical regions to distinguish fine-grained images, which further improve the performance of network. Furthermore, a dynamic temperature hyperparameter is indicated to achieve a better distillation performance. Finally, the effectiveness of the proposed model is validated on 3 benchmark datasets, FGVC-Aircraft, Stanford_Cars, and Stanford_Dogs, with classification accuracies of 96.29%, 98.81 %, and 99.71%, respectively. The expected calibration errors (ECE) can reach 1.25, 1.46, and 1.54, and the areas under the receiver operator characteristic (AURC) curves can reach 16.71, 17.07, and 17.65, respectively.
ER  - 


TY  - JOUR
TI  - Characterization of Magneto-Optic Imaging Data for Aircraft Inspection
T2  - IEEE Transactions on Magnetics
SP  - 3228
EP  - 3230
AU  - Y. Deng
AU  - X. Liu
AU  - Y. Fan
AU  - Z. Zeng
AU  - L. Udpa
AU  - W. Shih
PY  - 2006
KW  - Aircraft
KW  - Inspection
KW  - Magnetooptic effects
KW  - Eddy currents
KW  - Sensor phenomena and characterization
KW  - Surface cracks
KW  - Corrosion
KW  - Skin
KW  - Instruments
KW  - Image sensors
KW  - Eddy current
KW  - Faraday rotation
KW  - magneto-optic imaging
KW  - nondestructive evaluation
DO  - 10.1109/TMAG.2006.878419
JO  - IEEE Transactions on Magnetics
IS  - 10
SN  - 1941-0069
VO  - 42
VL  - 42
JA  - IEEE Transactions on Magnetics
Y1  - Oct. 2006
AB  - The magneto-optic imager (MOI) is widely used in detecting surface and subsurface cracks and corrosion in aircraft skins. The instrument provides analog images of the anomalies based on eddy current induction techniques and a magneto-optic (MO) sensor using the Faraday rotation effect. The merits of the MOI that make it attractive include rapid and large-area inspection, insensitivity to liftoff variations, and easy interpretation in contrast to the complex impedance data of conventional eddy current inspections. The current MOI system lacks the capability of providing a quantitative measure of the defects. In addition, the presence of noise due to the underlying domain structures in the MO sensor can lead to inconsistent accept or reject decisions by the inspector. This paper presents an image processing and automated classification algorithm for MO image analysis and also provides a quantitative basis for characterizing these images
ER  - 


TY  - JOUR
TI  - SCAN: Scattering Characteristics Analysis Network for Few-Shot Aircraft Classification in High-Resolution SAR Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 17
AU  - X. Sun
AU  - Y. Lv
AU  - Z. Wang
AU  - K. Fu
PY  - 2022
KW  - Synthetic aperture radar
KW  - Scattering
KW  - Task analysis
KW  - Radar polarimetry
KW  - Aircraft
KW  - Deep learning
KW  - Training
KW  - Aircraft
KW  - automatic target recognition (ATR)
KW  - classification
KW  - deep learning
KW  - few-shot learning (FSL)
KW  - scattering characteristics
KW  - synthetic aperture radar (SAR)
DO  - 10.1109/TGRS.2022.3166174
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - Recently, deep learning in synthetic aperture radar (SAR) automatic target recognition (ATR) has made significant progress, but the sample limitation problem in the SAR field is still obvious. Compared with the optical remote sensing images, the SAR images are insufficient, especially those containing the geospatial targets with certain target attitude angles (TAAs). To solve these problems, a novel few-shot learning framework named scattering characteristics analysis network (SCAN) is proposed in this article. First, a scattering extraction module (SEM) is designed to combine the target imaging mechanism with the network, which learns the number and distribution of the scattering points for each target type via explicit supervision. Besides, considering the imaging variability of SAR targets, a TAA-guided metalearning network consisting of an angle self-adaption classifier (ASC) and a frequency embedded module (FEM) is designed. ASC guides the network to focus on the positive sample pairs with different TAAs. FEM combines pulse cosine transform (PCT) with the network training process effectively to enrich frequency-domain information. In addition, a new dataset named SAR aircraft category dataset is constructed for the experiments. Compared with other few-shot SAR target classification approaches, our model efficiently integrates the scattering characteristics with the learning process, and the test accuracy for 5-way 1-shot has been improved by 4.74%. Finally, the experimental results are provided to demonstrate the validity of the proposed method.
ER  - 


TY  - CONF
TI  - Evaluation of MTARSI2 Dataset for Aircraft Type Recognition in Remote Sensing Images
T2  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
SP  - 1
EP  - 9
AU  - D. Hejji
AU  - O. Gouda
AU  - A. Bouridane
AU  - M. Abu Talib
PY  - 2022
KW  - Image recognition
KW  - Surveillance
KW  - Benchmark testing
KW  - Military aircraft
KW  - Aircraft navigation
KW  - Reproducibility of results
KW  - Aircraft
DO  - 10.1109/ICNS54818.2022.9771536
JO  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
IS  - 
SN  - 2155-4951
VO  - 
VL  - 
JA  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
Y1  - 5-7 April 2022
AB  - The potential of implementing Remote sensing in terms of applications has been extended due to the significant enhancement in the spatial and spectral resolution. One of the most significant application in remote sensing image interpretation is Aircraft Type Recognition (ATR), which has been extensively exploited in both civil and military applications. ATR via remote sensing images has attracted the attention of scientists and researchers due to its critical role in aerospace applications and intelligence information. The field of research in this area has been improved as the knowledge is narrowed to evaluate different private datasets due to the unavailability of a benchmark dataset. Thus, the level of obtained certainty of the overall performances achieved in this application area is remarkably jeopardized, therefore, reducing the reproducibility of results significantly. Recently, two benchmark datasets, namely: Multi-Type Aircraft Remote Sensing Images (MTARSI) and its extended version MTARSI2, have been published in 2020 and 2021, respectively. The researchers are still in the exploratory stage to develop methods with high level of accuracy. Therefore, the scope of work of this research is to conduct a detailed analysis on the extended data set MTARSI2 to evaluate the dataset’s characteristics, strength, weakness, and performance against well-known algorithms. Moreover, the study will target the latest limitations of ATR in remote sensing images and recommend possible scenarios and solution to be studied in further detailed future research work.
ER  - 


TY  - CONF
TI  - Aircraft recognition in remote sensing images based on saliency and invariant moments
T2  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
SP  - 500
EP  - 505
AU  - J. Fu
AU  - L. Fan
AU  - Z. Yang
PY  - 2016
KW  - Feature extraction
KW  - Target recognition
KW  - Aircraft
KW  - Remote sensing
KW  - Algorithm design and analysis
KW  - Classification algorithms
KW  - Image color analysis
KW  - 8 neighborhood
KW  - saliency map
KW  - affine invariant moments
KW  - Pseudo-Zernike invariant moments
DO  - 10.1109/FSKD.2016.7603224
JO  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
Y1  - 13-15 Aug. 2016
AB  - The traditional aircraft target recognition algorithms in remote sensing images are mostly based on specific theories. In the case of less interference, traditional algorithms can work well. But however, there are a large number of interfering factors in the remote sensing images actually, such as background, noise and so on. At this time, traditional algorithms fail because of low recognition accuracy and large time spent. Aiming at the shortcomings of traditional methods, this research has proposed a new kind of aircraft target recognition algorithm based on saliency images and invariant moments. The algorithm uses Itti algorithm to extract salient targets after pretreatment, then uses the 8 neighborhood searching method to find connected regions in binary images for determining the numbers and location of the candidate targets. Finally, identify the candidate targets by using the combined moments based on affine invariant moments and Pseudo-Zernike moments. The experiment results show that this algorithm has high detection accuracy, less time spent, low rate of false alarm.
ER  - 


TY  - CONF
TI  - Research on Recognition Technology of Low-Altitude Low- Speed and Small-Target Aircraft Based on Block Diagonal Feature
T2  - 2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)
SP  - 214
EP  - 218
AU  - X. Xie
AU  - S. Li
AU  - J. Wang
PY  - 2021
KW  - Support vector machines
KW  - Industries
KW  - Histograms
KW  - Image recognition
KW  - Conferences
KW  - Feature extraction
KW  - Military aircraft
KW  - low-altitude low-speed and small-target aircraft
KW  - block diagonalization
KW  - low-rank feature
KW  - support vector machine
DO  - 10.1109/BDAI52447.2021.9515305
JO  - 2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)
Y1  - 2-4 July 2021
AB  - With the development of Low-altitude Low-Speed and Small-Target Aircraft (hereinafter referred to as LLS Aircraft), the number of dangerous accidents caused by them increases sharply. There is an urgent need to carry out research on the defense technology of LLS Aircraft. In the defense process of LLS Aircraft, the most important part is to recognize LLS Aircraft. Only on the basis of accurate recognition can we further achieve the accurate tracking, interception and disposal of different LLS Aircraft. Proposed in this paper, we study the LLS Aircraft recognition technology based on block-diagonal feature. We firstly extract the Histogram of Oriented Gradients (HOG) feature of LLS Aircraft images. Then the HOG feature is diagonalized by using low-rank recovery technology. The low-rank feature representation of LLS Aircraft is constructed by introducing a block-diagonal sparse regular term to increase the discrimination of LLS Aircraft feature. Based on the block-diagonal HOG feature, we use Support Vector Machine (SVM) to classify the LLS Aircraft, to enhance the accuracy of LLS Aircraft recognition.
ER  - 


TY  - CONF
TI  - Fine-grained Image Recognition Based on Attention Map and Image Sampling
T2  - 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
SP  - 244
EP  - 248
AU  - J. Ma
AU  - J. Ji
AU  - M. Xiong
AU  - H. Xiong
PY  - 2020
KW  - Feature extraction
KW  - Image classification
KW  - Convolutional neural networks
KW  - Training
KW  - Image recognition
KW  - Shape
KW  - Image color analysis
KW  - grained classification
KW  - visual attention
KW  - image sampling
DO  - 10.1109/IHMSC49165.2020.10133
JO  - 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
Y1  - 22-23 Aug. 2020
AB  - The difference between fine-grained image classification and general image classification is that the difference between fine-grained images is small, so in fine-grained image classification, the details of the images are extremely important. In this paper, we proposed a network that can retain both the overall image information and the local image information. Our network structure is composed as follows: First using the convolutional layer to obtain the feature map of the image, and then use the trilinear attention method to process the feature map to obtain the average attention map and the single-channel attention map, and then using selective sampling. The sampled image is obtained according to the two attention maps above, and finally the original image and the sampled two images are input to the convolutional neural network for discrimination. Our entire network can be trained end-to-end. We used this network structure to conduct a large number of experiments on the CUB-2011-200, FGVC aircraft and Stanford Cars datasets, and the experimental results all proved the effectiveness of the method.
ER  - 


TY  - JOUR
TI  - ST-Net: Scattering Topology Network for Aircraft Classification in High-Resolution SAR Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 17
AU  - Y. Kang
AU  - Z. Wang
AU  - H. Zuo
AU  - Y. Zhang
AU  - Z. Yang
AU  - X. Sun
AU  - K. Fu
PY  - 2023
KW  - Scattering
KW  - Aircraft
KW  - Feature extraction
KW  - Topology
KW  - Radar polarimetry
KW  - Synthetic aperture radar
KW  - Solid modeling
KW  - Aircraft classification
KW  - context attention excitation (CAE)
KW  - scattering point
KW  - synthetic aperture radar (SAR)
KW  - topology structure
DO  - 10.1109/TGRS.2023.3236987
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 61
VL  - 61
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2023
AB  - Aircraft classification in synthetic aperture radar (SAR) images plays a considerable role in global region management and surveillance. Recently, deep learning has been applied to solve the classification problem and made significant progress. Due to the imaging variability at different angles and component scattering discreteness in SAR images, previous works have had difficulty in achieving desirable classification results. To address these issues, we study the positional and semantic relationship between the scattering points and propose an innovative scattering topology network (ST-Net) in this article. First, considering the diversity of imaging results caused by different target attitude angles, we extract and transform the scattering cluster centers to update the information of various categories. It can guide the model to strengthen the discriminative features and mitigate the impact of imaging variability on classification performance. Second, a novel scattering topology module (STM) is introduced to model the spatial relationships and semantic information interaction of discrete scattering points. In this process, the topology relations and scattering characteristics are enhanced for further accurate classification. Third, context attention excitation (CAE) is designed to capture significant global and semantic information, which is conducive to suppressing background interference and reducing category confusion. In conclusion, the ST-Net is presented with the SAR imaging mechanism and the topology geometric representation of aircraft. We construct the SAR aircraft category dataset (SAR-ACD) and conduct extensive experiments on it to show the effectiveness of ST-Net, which illustrates that our method achieves superior classification performance.
ER  - 


TY  - CONF
TI  - Hierarchical Multi-Label Ship Recognition in Remote Sensing Images Using Label Relation Graphs
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 4968
EP  - 4971
AU  - J. Chen
AU  - Y. Qian
PY  - 2021
KW  - Measurement
KW  - Image recognition
KW  - Satellites
KW  - Image resolution
KW  - Neural networks
KW  - Military aircraft
KW  - Probabilistic logic
KW  - Hierarchical Multi-label Classification
KW  - Neural Network
DO  - 10.1109/IGARSS47720.2021.9554687
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - Hierarchical multi-label classification (HMC) aims to assign multiple labels to every instance with the labels organized under hierarchical relations. In the application of ship recognition in remote sensing images, a ship can own coarse-to-fine hierarchical labels, e.g., the military ship, aircraft carrier, and nimitz class aircraft carrier. In this paper, we propose to combine two forms of loss functions to solve the HMC problem based on the neural network. The first probabilistic classification loss is to encode the hierarchical knowledge by introducing hierarchy and exclusion (HEX) graphs to impose constraints on hierarchical labels. The second cross-entropy loss imposes the softmax normalization on leaf nodes in the hierarchy to discriminate fine-grained classes. We evaluate our method on the high resolution satellite image dataset for ship recognition (HRSC), in which hierarchical labels are organized as the three-level tree. The proposed method shows comparative results compared to state-of-art HMC models.
ER  - 


TY  - CONF
TI  - Aircraft recognition based on nonparametrical statistics
T2  - 2011 Seventh International Conference on Natural Computation
SP  - 1602
EP  - 1606
AU  - L. Wang
AU  - C. Xing
AU  - J. Yan
PY  - 2011
KW  - Shape
KW  - Histograms
KW  - Fourier series
KW  - Aircraft
KW  - Three dimensional displays
KW  - Pattern analysis
KW  - Image recognition
KW  - nonparametric statistics
KW  - aircraft recognition
KW  - Fourier series
KW  - curvature
DO  - 10.1109/ICNC.2011.6022377
JO  - 2011 Seventh International Conference on Natural Computation
IS  - 
SN  - 2157-9563
VO  - 3
VL  - 3
JA  - 2011 Seventh International Conference on Natural Computation
Y1  - 26-28 July 2011
AB  - The problem of aircraft recognition in a single image is analyzed. A novel method combined Fourier series representation and nonparametric statistics is introduced to do pattern recognition for aircraft. Silhouettes obtained by image segmentation are described by Fourier series, after which empirical distribution function of curvature is computed. Fast matching is performed with coarse hypothesis test for partial classification. The re#ned hypothesis test is used to get detailed matching result. Experimental results show the effectiveness of the algorithm for shape recognition.
ER  - 


TY  - CONF
TI  - Infrared image registration of damage in the aircraft skin based on lie group machine learning
T2  - The 26th Chinese Control and Decision Conference (2014 CCDC)
SP  - 2104
EP  - 2108
AU  - Y. Luo
AU  - Z. Yan
AU  - K. Wang
AU  - L. Wang
PY  - 2014
KW  - Classification algorithms
KW  - Training
KW  - Matrix decomposition
KW  - Skin
KW  - Machine learning algorithms
KW  - Accuracy
KW  - Aircraft
KW  - Infrared Imagery
KW  - Nondestructive Testing
KW  - Lie Group Machine Learning
KW  - Classifier of Symplectic Group
KW  - Image Registration
DO  - 10.1109/CCDC.2014.6852514
JO  - The 26th Chinese Control and Decision Conference (2014 CCDC)
IS  - 
SN  - 1948-9447
VO  - 
VL  - 
JA  - The 26th Chinese Control and Decision Conference (2014 CCDC)
Y1  - 31 May-2 June 2014
AB  - The method of nondestructive testing for aircraft skin composite defects using infrared thermography is very effective. But, against the problem of how to rapidly and accurately identify for the defects of specific types needs to be further study. Based on the analysis of the existed classifier for skin damages, a complex group classifier based on lie group machine learning algorithm is introduced in this paper. According to the damage infrared thermal images obtained by the infrared thermal imager, the feature of internal defects of the skin specific defects is extracted and a discriminant function is established, and then a direct classification for the input image is realized. A simulation result proves that the algorithm given in this paper can satisfy the identification accuracy, and shows the effective of the algorithm.
ER  - 


TY  - CONF
TI  - Hyperspectral and color-infrared imaging from ultralight aircraft: Potential to recognize tree species in urban environments
T2  - 2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)
SP  - 1
EP  - 5
AU  - G. Mozgeris
AU  - S. Gadal
AU  - D. Jonikavičius
AU  - L. Straigytė
AU  - W. Ouerghemmi
AU  - V. Juodkienė
PY  - 2016
KW  - Vegetation
KW  - Hyperspectral imaging
KW  - Urban areas
KW  - Cameras
KW  - Monitoring
KW  - hyperspectral imaging
KW  - color-infrared images
KW  - ultra-light aircraft
KW  - urban tree species identification
KW  - discriminant analysis
DO  - 10.1109/WHISPERS.2016.8071756
JO  - 2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)
IS  - 
SN  - 2158-6276
VO  - 
VL  - 
JA  - 2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)
Y1  - 21-24 Aug. 2016
AB  - Imaging system based on simultaneous use of Rikola hyperspectral and RGB/NIR cameras installed on a manned ultra-light aircraft is introduced in this study. Simultaneously acquired hyperspectral and color-infrared (CIR) images were tested for their potential to identify deciduous tree species and estimate tree health in Kaunas city, Lithuania. Six urban deciduous tree species were separated using tree crown level statistics, extracted from 16 visible-near infrared spectral band hyperspectral images, and discriminant analyses with an overall classification accuracy of 63.1 %. Classification accuracy increased by 3 percent when hyperspectral images were integrated with simultaneously acquired CIR images. The accuracy in identifying tree health using fused hyperspectral and CIR images, ranged from poor to moderate.
ER  - 


TY  - CONF
TI  - Multi-Spectral Data Fusion Using a Markov Random Field Model : Application to Satellite Image Classification
T2  - IEEE Seventh SP Workshop on Statistical Signal and Array Processing
SP  - 401
EP  - 404
AU  - D. Murray
AU  - J. Zerubia
PY  - 1994
KW  - Markov random fields
KW  - Satellites
KW  - Multispectral imaging
KW  - Simulated annealing
KW  - Layout
KW  - Image classification
KW  - Business
KW  - Aircraft
KW  - Iterative algorithms
KW  - Iterative methods
DO  - 10.1109/SSAP.1994.572527
JO  - IEEE Seventh SP Workshop on Statistical Signal and Array Processing
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE Seventh SP Workshop on Statistical Signal and Array Processing
Y1  - 26-29 June 1994
AB  - 
ER  - 


TY  - CONF
TI  - Defects Recognition Algorithm on Magneto-Optic Image of Aging Aircraft Skin
T2  - The Proceedings of the Multiconference on "Computational Engineering in Systems Applications"
SP  - 927
EP  - 931
AU  - Z. Xing
AU  - Q. Gao
PY  - 2006
KW  - Image recognition
KW  - Aging
KW  - Aircraft
KW  - Skin
KW  - Feature extraction
KW  - Inspection
KW  - Corrosion
KW  - Magnetooptic effects
KW  - Surface cracks
KW  - Aerospace control
KW  - aircraft skin
KW  - magneto-optic image
KW  - defect
KW  - recognition
KW  - SVM
DO  - 10.1109/CESA.2006.4281783
JO  - The Proceedings of the Multiconference on "Computational Engineering in Systems Applications"
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - The Proceedings of the Multiconference on "Computational Engineering in Systems Applications"
Y1  - 4-6 Oct. 2006
AB  - A recognition algorithm on magneto-optic image of surface & subsurface defects in aging aircraft is proposed in this paper. The star vector method is adopted to extract the defect feature of rivet's profile in aircraft skin. And the extracted features of rivet's profile in aircraft skin are used as training samples to train the support vector classifier machine (SVCM) and produce a super defect classifier plane. Radial basis kernel function in the SVCM training process and grid method is selected to optimize the SVCM model. The fuzzy membership function is introduced to the algorithm to solve the wrong & refusal recognition problems in multi-type classifier. The experiment result shows that, the algorithm can detect both the existence and the direction of the defect exactly and performs a better capability of recognition
ER  - 


TY  - CONF
TI  - Characterization of Magneto-Optic Imaging Data for Aircraft Inspection
T2  - 2006 IEEE International Magnetics Conference (INTERMAG)
SP  - 898
EP  - 898
AU  - Y. Deng
AU  - X. Liu
AU  - Y. Fan
AU  - Z. Zeng
AU  - L. Udpa
AU  - W. Shih
PY  - 2006
KW  - Aircraft
KW  - Inspection
KW  - Image edge detection
KW  - Image processing
KW  - Classification algorithms
KW  - Surface cracks
KW  - Skin
KW  - Frequency
KW  - Magnetic separation
KW  - Filtering
DO  - 10.1109/INTMAG.2006.374929
JO  - 2006 IEEE International Magnetics Conference (INTERMAG)
IS  - 
SN  - 2150-4601
VO  - 
VL  - 
JA  - 2006 IEEE International Magnetics Conference (INTERMAG)
Y1  - 8-12 May 2006
AB  - An image processing and automated classification algorithm that classifies the magneto-optic images for both surface and subsurface cracks in aircraft skins under various excitation frequencies was presented. Motion based filtering was performed as a first step for image detection and classification of interior magneto-optic images. The next step was detection of the structural edge of the images.
ER  - 


TY  - CONF
TI  - Imaging of commercial aircraft by inverse synthetic aperture radar and their classification in a Near-Range Radar Network (NRN)
T2  - Proceedings of the 1997 IEEE National Radar Conference
SP  - 19
EP  - 24
AU  - T. Sauer
AU  - K. . -H. Bethke
AU  - F. Buettner
AU  - B. Roede
AU  - A. Schroth
PY  - 1997
KW  - Aircraft
KW  - Inverse synthetic aperture radar
KW  - Radar imaging
KW  - Radar detection
KW  - Navigation
KW  - Vehicle detection
KW  - Synthetic aperture radar
KW  - Radio frequency
KW  - Airports
KW  - Radar equipment
DO  - 10.1109/NRC.1997.588104
JO  - Proceedings of the 1997 IEEE National Radar Conference
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of the 1997 IEEE National Radar Conference
Y1  - 13-15 May 1997
AB  - The Near-Range Radar Network is a sensor for the surveillance of the ground traffic on airports. It localizes targets, evaluates the vectors of velocity and classifies the targets. For the latter task microwave images of the moving objects obtained by the principle of inverse synthetic aperture radar (ISAR) are utilized. The most important processing steps are the range alignment and the pre-phase compensation of the measured range profiles, which are accomplished by a Hough transform algorithm and an autofocusing technique, respectively. For the final classification projections of the pixel intensities onto two orthogonal axes are extracted and evaluated by a correlator.
ER  - 


TY  - CONF
TI  - SAR-PC2AC: integrated phase congruency and CNN for aircraft classification in SAR imagery
T2  - IET International Radar Conference (IRC 2023)
SP  - 1509
EP  - 1514
AU  - R. Luo
AU  - L. Zhao
AU  - Y. Yang
AU  - Z. Zhou
AU  - K. Ji
PY  - 2023
DO  - 10.1049/icp.2024.1309
JO  - IET International Radar Conference (IRC 2023)
IS  - 
SN  - 
VO  - 2023
VL  - 2023
JA  - IET International Radar Conference (IRC 2023)
Y1  - 3-5 Dec. 2023
AB  - Aircraft classification based on synthetic aperture radar (SAR) imagery plays a critical role in both military and civilian applications. However, the challenges arise from the sensitive attitude and discrete appearance of aircraft, which significantly impact the performance of aircraft classification in SAR images. To overcome these obstacles, an integrated phase congruency and CNN framework for aircraft classification (SAR-PC2AC) is proposed in this paper. The SAR-PC2AC consists of three main components: a basic deep classification network (BDCN), a phase congruency neural network (PCNN), and a feature fusion module (FFM). First, the BDCN takes the raw SAR image as input, which automatically gathers high-level semantic information from SAR images. Simultaneously, the PCNN is responsible for extracting phase congruency information from SAR images to enhance the correlation among the scattering points of the aircraft. Finally, PCNN is used via FFM to guide the BDCN in capturing inter-class and intra-class relation constraints, thereby improving classification performance. Experiments on the public SAR-ACD dataset with 1m resolution demonstrate the superiority of the proposed method over the baseline.
ER  - 


TY  - CONF
TI  - Classification of organic quinoa crops using multispectral aerial imagery and machine learning techniques
T2  - 2022 IEEE International Conference on Automation/XXV Congress of the Chilean Association of Automatic Control (ICA-ACCA)
SP  - 1
EP  - 6
AU  - A. Flores
PY  - 2022
KW  - Deep learning
KW  - Support vector machines
KW  - Reflectivity
KW  - Biomedical optical imaging
KW  - Crops
KW  - Vegetation mapping
KW  - Optical imaging
KW  - Aerial images
KW  - agriculture
KW  - crop classification
KW  - image processing
KW  - semantic segmentation
KW  - machine learning
KW  - quinoa crops
DO  - 10.1109/ICA-ACCA56767.2022.10006196
JO  - 2022 IEEE International Conference on Automation/XXV Congress of the Chilean Association of Automatic Control (ICA-ACCA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Automation/XXV Congress of the Chilean Association of Automatic Control (ICA-ACCA)
Y1  - 24-28 Oct. 2022
AB  - Crop mapping is a vital tool for agricultural management and food security that can benefit from remote sensing data. The purpose of this research is to use machine learning (ML) techniques to classify quinoa crops from multispectral images. The spectral reflectance of five optical bands is used to develop classification models that are tested for diverse quinoa phenological phases. Deep learning methods Segnet and Unet were investigated, as well as Decision Trees, Discriminant Analysis, K nearest Neighbor, Support Vector Machines, Adaboost and Random Forest. Data was collected from quinoa crop fields in Cabana, Puno region in Peru. The multispectral images were captured using an Unmanned Aircraft System (UAS) from a height of 50 meters. Deep learning methods leave behind other approaches in the classification job, according to the results.
ER  - 


TY  - CONF
TI  - Impact Of Segmentation Parameters On The Classification Of VHR Images Acquired By RPAS
T2  - 2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)
SP  - 28
EP  - 33
AU  - M. G. Lacerda
AU  - E. H. Shiguemori
AU  - A. J. Damião
AU  - C. S. Anjos
AU  - M. Habermann
PY  - 2020
KW  - Image segmentation
KW  - Indexes
KW  - Image resolution
KW  - Shape
KW  - Cameras
KW  - Planning
KW  - Software
KW  - Image Classification
KW  - Segmentation Parameters
KW  - RPA
KW  - Very High Resolution Images
DO  - 10.1109/LAGIRS48042.2020.9165637
JO  - 2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)
Y1  - 22-26 March 2020
AB  - RPAs (Remotely Piloted Aircrafts) have been used in many Remote Sensing applications, featuring high-quality imaging sensors. In some situations, the images are interpreted in an automated fashion using object-oriented classification. In this case, the first step is segmentation. However, the setting of segmentation parameters such as scale, shape, and compactness may yield too many different segmentations, thus it is necessary to understand the influence of those parameters on the final output. This paper compares 24 segmentation parameter sets by taking into account classification scores. The results indicate that the segmentation parameters exert influence on both classification accuracy and processing time.
ER  - 


TY  - CONF
TI  - An adaptive-weight regularization method for multi-classifier fusion decision
T2  - 2014 International Conference on Mechatronics and Control (ICMC)
SP  - 343
EP  - 346
AU  - Z. Xufeng
AU  - M. Biao
AU  - G. Guanjun
PY  - 2014
KW  - Aircraft
KW  - Support vector machines
KW  - Aircraft manufacture
KW  - Testing
KW  - Image recognition
KW  - Target recognition
KW  - Training
KW  - aircraft type recognition
KW  - adaptive-weight
KW  - multi-classifier
KW  - decision-level fusion
DO  - 10.1109/ICMC.2014.7231575
JO  - 2014 International Conference on Mechatronics and Control (ICMC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2014 International Conference on Mechatronics and Control (ICMC)
Y1  - 3-5 July 2014
AB  - The difficulties of aircraft type recognition methods are introduced and the necessity of multi-classifier fusion decision method is discussed. The some kinds of invariants: Hu moments, Affine moments, Zernike moments, Wavelet moments, are used for constructing four SVM classifiers. Based on the above four classifiers, an adaptive-weight regularization method is proposed for improving aircraft type classification performance. Experiments are shown that, the recognition rate by the proposed method in this paper is better than any classifier of the above four classifiers, the fixed-weight multi-classifier fusion method and the majority multi-classifier fusion method.
ER  - 


TY  - CONF
TI  - Unsupervised Classification of Aviris-NG Hyperspectral Images
T2  - 2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)
SP  - 1
EP  - 5
AU  - K. Cui
AU  - R. J. Plemmons
PY  - 2021
KW  - Satellites
KW  - Conferences
KW  - Clustering methods
KW  - Wildlife
KW  - Forestry
KW  - Data mining
KW  - Aircraft
KW  - Unsupervised hyperspectral classification
KW  - clustering
KW  - randomized computations
KW  - endmembers
KW  - unlabeled AVIRIS-NG data
KW  - forests
KW  - India
DO  - 10.1109/WHISPERS52202.2021.9484006
JO  - 2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)
IS  - 
SN  - 2158-6276
VO  - 
VL  - 
JA  - 2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)
Y1  - 24-26 March 2021
AB  - In hyperspectral imaging for remote sensing, learning from unlabeled data by unsupervised methods is very challenging and it is the subject of considerable recent interest since the collection of large datasets by aircraft, UAVs and satellites has become ubiquitous. We experiment with unsupervised endmember extraction and classification of hyperspectral data collected over India by NASA's AVIRIS-NG airborne remote sensor. We have downloaded some of this data from the NASA-JPL portal in Pasadena, CA, for the purpose of studying land cover and land usage, and especially forests, in India. We report on results from our experiments with unsupervised endmember-based methods and clustering methods for classifying images from a mixed forest region that we selected from the Shoolpaneshwar Wildlife Sanctuary in Western In-dia. Randomized numerical methods are used to speed up the large-scale computations.
ER  - 


TY  - JOUR
TI  - Contrastive Learning for Fine-Grained Ship Classification in Remote Sensing Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 16
AU  - J. Chen
AU  - K. Chen
AU  - H. Chen
AU  - W. Li
AU  - Z. Zou
AU  - Z. Shi
PY  - 2022
KW  - Task analysis
KW  - Marine vehicles
KW  - Annotations
KW  - Remote sensing
KW  - Measurement
KW  - Visualization
KW  - Training
KW  - Contrastive learning (CL)
KW  - fine-grained classification
KW  - remote sensing (RS)
KW  - ship classification
DO  - 10.1109/TGRS.2022.3192256
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - Fine-grained image classification can be considered as a discriminative learning process where images of different subclasses are separated from each other while the same subclass images are clustered. Most existing methods perform synchronous discriminative learning in their approaches. Although achieving promising results in fine-grained visual classification (FGVC) in natural images, these methods may fail in fine-grained ship classification (FGSC) problem in remote sensing (RS) images due to the highly “imbalanced fineness” and “imbalanced appearances” of ships among subclasses. To tackle the issue, we propose an asynchronous contrastive learning-based method for effective FGSC. The proposed method, which we refer to as “Push-and-Pull Network (P2Net)”, includes a “push-out stage” and a “pull-in stage”, where the first stage forces all the instances to be decorrelated and then the second one groups them into each subclass. A dual-branch network is designed to separate/decorrelate the images with each other, while an integration module is designed to aggregate the decorrelated images into their corresponding subclass together with a proxy-based module designed for acceleration. In this way, the correlation between subclasses can be decoupled, which in turn makes the final classification much easier. Our method can be trained end-to-end and requires no additional annotations other than category information. Extensive experiments are conducted on two large-scale FGSC datasets (FGSC-23 and FGSCR-42). Our method outperforms other state-of-the-art approaches. Ablation experiments also suggest the effectiveness of our design. Our code is available at https://github.com/WindVChen/Push-and-Pull-Network.
ER  - 


TY  - CONF
TI  - Aircraft Rivets Defect Recognition Method Based on Magneto-optical Images
T2  - 2010 International Conference on Machine Vision and Human-machine Interface
SP  - 788
EP  - 791
AU  - B. Li
AU  - X. Wang
AU  - H. Yang
AU  - Z. Zhou
PY  - 2010
KW  - Aircraft
KW  - Image recognition
KW  - Magnetooptic effects
KW  - Power engineering and energy
KW  - Aerospace engineering
KW  - Magnetooptic devices
KW  - Feature extraction
KW  - Frequency
KW  - Inspection
KW  - Support vector machines
KW  - magneto-optic image
KW  - star radial vector
KW  - support vector machine
DO  - 10.1109/MVHI.2010.52
JO  - 2010 International Conference on Machine Vision and Human-machine Interface
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2010 International Conference on Machine Vision and Human-machine Interface
Y1  - 24-25 April 2010
AB  - With magneto-optical images for the aircraft rivets, a new automated recognition algorithm of inspecting the existence and the direction of cracks based on fuzzy support vector machine (FSVM) is presented. The binary image of rivet is obtained by preprocessing the magneto-optic image, the star radial vector method is used to acquire the feature of rivet edge, and the approximate center of rivet is got according to the threshold method. The kernel parameter and the penalty content are optimized by using the grid method, and FSVM is adopted to avoid the refusal classification and the false classification in multi-classifier. The inspection effect and the real-time performance are proved by these experiments.
ER  - 


TY  - CONF
TI  - SAR Image Matching Area Selection Based on Actual Flight Real-Time Image
T2  - 2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)
SP  - 1
EP  - 5
AU  - W. Jianmei
AU  - W. Zhong
AU  - Z. Shaoming
AU  - F. Tiantian
AU  - D. Jihui
PY  - 2018
KW  - Feature extraction
KW  - Real-time systems
KW  - Support vector machines
KW  - Synthetic aperture radar
KW  - Image edge detection
KW  - Aircraft navigation
KW  - Aircraft
KW  - INS/SAR integrated navigation
KW  - suitability area selection
KW  - suitability features
KW  - real-time image
KW  - feature selection
DO  - 10.1109/PRRS.2018.8486416
JO  - 2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)
IS  - 
SN  - 2377-0198
VO  - 
VL  - 
JA  - 2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)
Y1  - 19-20 Aug. 2018
AB  - Matching suitability analysis is a key issue of INS/SAR integrated navigation mode. The existing suitability area selection methods use the simulated real-time image to calculate the matching probability of the scene area and further label it “suitability” or “unsuitability”. If the imaging mode of the simulated image is the same as that of the real image, the suitability area selection model based on the simulated real-time image works well. Otherwise, the model is impractical. In order to address this issue, a novel method is proposed in this paper. The sample dataset is built on the actual flight real-time images, and a hybrid feature selection method based on D-Score and SVM is used to select the suitability features and build the suitability area selection model simultaneously. Experimental results show that the consistency between the prediction results of the model and the ones experts label reaches 81.92%.
ER  - 


TY  - CONF
TI  - A simple and effective semi-supervised learning framework for hyperspectral image classification
T2  - 2016 IEEE Systems and Technologies for Remote Sensing Applications Through Unmanned Aerial Systems (STRATUS)
SP  - 14
EP  - 18
AU  - A. Elshamli
AU  - G. W. Taylor
AU  - S. Areibi
PY  - 2016
KW  - Support vector machines
KW  - Training
KW  - Benchmark testing
KW  - Clustering algorithms
KW  - Hyperspectral imaging
KW  - Hyperspectral imaging
KW  - semisupervised learning
KW  - classification
KW  - spatial correlation
KW  - machine learning
KW  - clustering
DO  - 10.1109/STRATUS.2016.7811138
JO  - 2016 IEEE Systems and Technologies for Remote Sensing Applications Through Unmanned Aerial Systems (STRATUS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 IEEE Systems and Technologies for Remote Sensing Applications Through Unmanned Aerial Systems (STRATUS)
Y1  - 28-28 Oct. 2016
AB  - Semisupervised learning (SSL) is often used when the number of labeled samples is very small compared to the number of unlabeled samples. It permits the exploitation of structure within unlabeled samples during the learning task. Like many other applications, remote sensing images suffer from the limited number of ground-truth samples and therefore semisupervised techniques may be used to overcome this limitation. In this paper, a semisupervised framework is proposed for classification of hyperspectral images with scarce labeled samples. Our method, which we call SSL-CC, utilizes fuzzy spectral clustering to label spatially neighboring samples. SSL-CC is implemented and tested on two benchmark hyperspectral datasets. Fuzzy clustering is compared to traditional crisp clustering (k-means) and the obtained results indicate that fuzzy clustering can significantly improve classification accuracy. SSL-CC achieves on average 60% improvement over a baseline SVM classifier.
ER  - 


TY  - CONF
TI  - Target Recognition and Classification of Underwater Airplane and Sunken Ship Images Based on Convolutional Neural Network with Side-scan Sonar
T2  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
SP  - 1367
EP  - 1371
AU  - Q. Yang
AU  - L. Yan
AU  - X. Liu
AU  - H. Zhu
PY  - 2023
KW  - Airplanes
KW  - Image recognition
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Aircraft manufacture
KW  - Side-scan sonar
KW  - Marine vehicles
KW  - Side-scan sonar image
KW  - underwater airplanes
KW  - sunken ships
KW  - image recognition
KW  - Convolutional Neural Network
DO  - 10.1109/ITAIC58329.2023.10409057
JO  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
IS  - 
SN  - 2693-2865
VO  - 11
VL  - 11
JA  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
Y1  - 8-10 Dec. 2023
AB  - The traditional analysis of side-scan sonar images faces several challenges, such as the strong subjectivity of manual interpretation, low efficiency, high labor costs, time consumption, and over-reliance on experience. Additionally, machine learning methods require manual feature selection, leading to a lack of adaptability and robustness. To address these issues, this study attempts to employ Convolutional Neural Networks (CNN) for the analysis of side-scan sonar images. This method can automatically extract features from side-scan sonar images of underwater aircraft and sunken ships and accomplish classification and recognition. In the study, we used a total of 1140 pre-processed side-scan sonar images of the seabed, underwater aircraft, and sunken ships from the Seabed Objects (ships and aircraft) dataset to train and test the constructed CNN model. The experimental results demonstrate that the trained CNN model can accurately classify and recognize side-scan sonar images of the seabed, underwater aircraft, and sunken ships, with an average accuracy rate reaching as high as 93.70%. This confirms that the method has high efficiency, high accuracy, and strong robustness, significantly enhancing the classification and recognition ability of side-scan sonar images of underwater aircraft and sunken ships.
ER  - 


TY  - CONF
TI  - Autonomous segmentation and neural network texture classification of IR image sequences
T2  - IEE Colloquium on Image Processing for Remote Sensing
SP  - 6/1
EP  - 6/6
AU  - J. F. Haddon
AU  - J. F. Boyce
AU  - M. Strens
PY  - 1996
KW  - Remote sensing
KW  - Geophysical measurements
KW  - Geophysical signal processing
KW  - Image texture analysis
KW  - Image classification
KW  - Infrared imaging
KW  - Optical data processing
KW  - Image segmentation
KW  - Neural networks
KW  - Image sequence analysis
DO  - 10.1049/ic:19960160
JO  - IEE Colloquium on Image Processing for Remote Sensing
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEE Colloquium on Image Processing for Remote Sensing
Y1  - 13-13 Feb. 1996
AB  - Presents a technique for the texture classification of segmented regions in sequences of infrared images taken from low flying aircraft. Sequences of IR images have been segmented using a technique which integrates edge detection and region segmentation into a single, inherently parallel process. A temporal component is used to ensure that the segmentation is consistent in time. The texture of the segmented regions is characterized using discrete Hermite functions and property co-occurrence matrices. This results in a low order feature vector in which the zeroth coefficient describes the Gaussian noise of the region while the higher orders describe the texture. Neural networks are used to perform an initial classification of the segmented regions, this is then refined using spatio-temporal relaxation labelling to ensure consistency of interpretation, both spatially within an image and temporal within the sequence.
ER  - 


TY  - CONF
TI  - Relative performance of correlation-based and feature-based classifiers of aircraft using radar range profiles
T2  - Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)
SP  - 1828
EP  - 1832 vol.2
AU  - C. Nieuwoudt
AU  - E. C. Botha
PY  - 1998
KW  - Airborne radar
KW  - Radio access networks
KW  - Bandwidth
KW  - Aerospace electronics
KW  - Radar imaging
KW  - Frequency
KW  - Aircraft propulsion
KW  - Target recognition
KW  - Neural networks
KW  - Aerospace engineering
DO  - 10.1109/ICPR.1998.712086
JO  - Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)
IS  - 
SN  - 1051-4651
VO  - 2
VL  - 2
JA  - Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)
Y1  - 20-20 Aug. 1998
AB  - A surveillance system which automatically classifies aircraft can be very useful, especially with the problem of congestion at large airports. This paper evaluates the performance of correlation- and feature-based classifiers on a set of four simulated radar targets over a wide range of target orientations. Experiments are performed for a range of radar bandwidths in order to determine the effect of radar bandwidth on the relative classification performance. Only 1D radar range profiles are considered since it is assumed that the aircraft are classified using few profiles and the orientation of the aircraft in each profile is known only approximately. The results suggest that feature-based classifiers outperform correlation-based classifiers and that classification performance is highly dependent on the orientation of the aircraft, but that accurate classification of approaching aircraft is possible.
ER  - 


TY  - CONF
TI  - A Neural Network-based Method for Tracking and Locating the Carrier-based Aircraft
T2  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
SP  - 1222
EP  - 1225
AU  - C. Zhao
AU  - Y. Shao
AU  - D. Yao
PY  - 2023
KW  - Deep learning
KW  - Target tracking
KW  - Azimuth
KW  - Target recognition
KW  - Streaming media
KW  - Classification algorithms
KW  - Aircraft
KW  - Deep learning
KW  - Neural network
KW  - target tracking
KW  - Carrier-based aircraft
DO  - 10.1109/ITAIC58329.2023.10409059
JO  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
IS  - 
SN  - 2693-2865
VO  - 11
VL  - 11
JA  - 2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
Y1  - 8-10 Dec. 2023
AB  - In order to improve the efficiency and accuracy of information acquisition such as target position and azimuth of the Carrier-based aircraft, this paper proposes a target tracking and location method based on NeturalNetwork technology. This method uses deep learning technology to classify and locate the aircraft in the ship deck video, uses Hungarian algorithm to correlate and track targets, calculates target orientation, and uses BP neural network model to transform the physics coordinates. The experimental results show that this method can improve the real-time and accuracy of target position and azimuth information, and the accuracy of target recognition tracking and target orientation reaches 100%.
ER  - 


TY  - CONF
TI  - Aircraft Target Recognition: A novel approach for features extraction from ISAR images
T2  - 2009 International Radar Conference "Surveillance for a Safer World" (RADAR 2009)
SP  - 1
EP  - 5
AU  - M. N. Saidi
AU  - A. Toumi
AU  - B. Hoeltzener
AU  - A. Khenchaf
AU  - D. Aboutajdine
PY  - 2009
KW  - Aircraft
KW  - Target recognition
KW  - Feature extraction
KW  - Shape
KW  - Data mining
KW  - Support vector machines
KW  - Support vector machine classification
KW  - Radar imaging
KW  - Image segmentation
KW  - Level set
KW  - KDD
KW  - ATR
KW  - ISAR
KW  - SUSAN
KW  - Level set
KW  - shape extraction
KW  - Classification
DO  - 
JO  - 2009 International Radar Conference "Surveillance for a Safer World" (RADAR 2009)
IS  - 
SN  - 1097-5764
VO  - 
VL  - 
JA  - 2009 International Radar Conference "Surveillance for a Safer World" (RADAR 2009)
Y1  - 12-16 Oct. 2009
AB  - In this paper, we present a system for Automatic Target Recognition (ATR) based on ISAR images. The methodology used is based on knowledge discovery from data (KDD process) process adapted to radar field. The shape extraction is the most important step in recognition system. However, we propose a new approach for Target shape extraction based on combination of Smallest Univalue Segment Assimilating Nucleus (SUSAN) method and Variational Level Set (VLS). The feature vector is then represented by Fourier descriptors of each target shape. Finally, recognition scheme is achieved by both: Support Vectors Machine (SVM) and K Nearest Neighbors (KNN) classifiers.
ER  - 


TY  - CONF
TI  - Target recognition using IFFT and MUSIC ISAR images
T2  - 2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)
SP  - 596
EP  - 600
AU  - A. Toumi
AU  - A. Khenchaf
PY  - 2016
KW  - Target recognition
KW  - Training
KW  - Aircraft
KW  - Indexes
KW  - Multiple signal classification
KW  - Image reconstruction
KW  - Inverse Synthetic Aperture Radar
KW  - retrieval images
KW  - target recognition
DO  - 10.1109/ATSIP.2016.7523151
JO  - 2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)
Y1  - 21-23 March 2016
AB  - We present in this paper an approach to achieve the aircraft target recognition task using Inverse Synthetic Aperture Radar (ISAR) images reconstructed using IFFT (Inverse Fast Furrier Transform) and MUSIC2D (multiple signal characterization) methods. The first goal of this work is to propose efficient features for target recognition using a polar mapping procedure, to make a polar signature, with well-designed classifier. The second goal of this work is to compare the recognition rate using the both reconstruction methods. Finally, the obtained results are given and compared for the two types ISAR images in order to validate our proposed feature vectors and retrieval scheme.
ER  - 


TY  - CONF
TI  - Classification of air targets based on range-Doppler diagrams
T2  - 2016 European Radar Conference (EuRAD)
SP  - 89
EP  - 92
AU  - J. J. M. de Wit
AU  - P. van Dorp
AU  - A. G. Huizing
PY  - 2016
KW  - Radar imaging
KW  - Helicopters
KW  - Imaging
KW  - Rotors
KW  - Radar tracking
KW  - Feature extraction
KW  - target classification
KW  - range-Doppler imaging
DO  - 
JO  - 2016 European Radar Conference (EuRAD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 European Radar Conference (EuRAD)
Y1  - 5-7 Oct. 2016
AB  - In urban and littoral environments, a wide variety of moving objects such as vehicles, birds, ultralights, drones, and small aircraft may lead to a large number of closely spaced radar detections that can cause severe problems in the tracking process such as false tracks, merged tracks and track loss. By extracting non-kinematic features such as micro-Doppler signatures or 1-D or 2-D spatial target images from the radar signal, the radar detections can be classified in different target classes to mitigate the problems in the tracking process and improve situation awareness. However, micro-Doppler signature analysis and high resolution imaging techniques need a long time-on-target that may be incompatible with the large number of targets in urban or littoral environments. Furthermore, micro-Doppler features are not always observable and target images can be distorted by moving parts and multipath on the target body. In this paper, range-Doppler imaging for air target classification is assessed.
ER  - 


TY  - JOUR
TI  - DAF-Net: Dual-Aperture Feature Fusion Network for Aircraft Detection on Complex-Valued SAR Image
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 1
EP  - 17
AU  - Q. Meng
AU  - Y. Wu
AU  - Y. Suo
AU  - T. Miao
AU  - Q. Ke
AU  - X. Gao
AU  - X. Sun
PY  - 2025
KW  - Aircraft
KW  - Feature extraction
KW  - Scattering
KW  - Synthetic aperture radar
KW  - Apertures
KW  - Radar polarimetry
KW  - Accuracy
KW  - Object detection
KW  - Azimuth
KW  - Image resolution
KW  - Synthetic Aperture Radar
KW  - Object Detection
KW  - Sub-Aperture
KW  - Anisotropic Object
DO  - 10.1109/JSTARS.2025.3549134
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 
VL  - 
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 
AB  - Aircraft detection in synthetic aperture radar (SAR) images plays a crucial role in supporting essential tasks like airport management and airspace monitoring. Most existing SAR aircraft detection algorithms are predominantly designed based on the scattering characteristics of full-aperture images, which provide high-resolution and rich detail information. However, in full-aperture images, responses from different angles are aliased, making it challenging to disentangle angular-specific details for anisotropic aircraft. Furthermore, the extended synthetic aperture time exacerbates the defocusing of slow-moving aircraft, increasing the likelihood of missed detections. Considering the above challenges, for targets exhibiting anisotropy and dispersion under high resolution, their non-stationary characteristics can be extracted by decomposing sub-apertures and employed as additional information to enhance detection accuracy. Therefore, we propose a dual-aperture feature fusion network (DAF-Net) designed to improve aircraft detection by mining, enhancing, and fusing features from both full-aperture and sub-aperture images.Specifically, a siamese backbone network with position-consistent weight supervision is introduced to efficiently and accurately extract features from the parallel input of full-aperture and sub-aperture images.Subsequently, an aperture-based feature enhancement module (AFEM) is proposed to leverage the scattering characteristics of aircraft and their context, thereby reinforcing the unique semantic information present in both full-aperture and sub-aperture images. Finally, a region-aware cross-attention fusion module (RCFM) is developed, which adaptively associates and fuses full-aperture and sub-aperture features based on spatial mapping relationships. Furthermore, experiments conducted on Complex-valued SAR Aircraft dataset (CSAR-A) demonstrate the effectiveness of the proposed method.DAF-Net improves AP50 and AP75 by 5.8% and 5.3% compared to the baseline, respectively, and achieves optimal performance compared to other detectors.
ER  - 


TY  - CONF
TI  - Methodology for Classifying Types of Underlying Surfaces Using Radar Frames in a Spatially Distributed System of Small-Sized Radar Stations
T2  - 2024 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
SP  - 1
EP  - 5
AU  - V. A. Nenashev
AU  - I. G. Khanykov
PY  - 2024
KW  - Laser radar
KW  - Surface waves
KW  - Airborne radar
KW  - Radar
KW  - Radar imaging
KW  - Optical imaging
KW  - Radar antennas
KW  - classification of territories by type
KW  - underlying surface
KW  - high-resolution radar images
KW  - remote sensing of the earth
KW  - image segmentation
KW  - small-sized airborne radars
KW  - direct and inverse tasks
KW  - formation of a complex location image
KW  - mapping of the earth’s surface
KW  - multi-position systems
DO  - 10.1109/WECONF61770.2024.10564605
JO  - 2024 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
IS  - 
SN  - 2769-3538
VO  - 
VL  - 
JA  - 2024 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
Y1  - 3-7 June 2024
AB  - A methodology is being developed for classifying types of underlying surfaces based on radar frames (RF) of the flow in the forward viewing area of small-sized airborne radar stations of a multi-position system. Direct and inverse classification problems are solved to determine new reference models in order to construct maps of underlying surface zones based on statistical characteristics without the use of optical location visual information. To construct standards, it is necessary to solve the inverse problem, in which the initial data is the data from the combination of the optical and radar layers of the frame. The solution to the inverse problem depends on the quality of the result of solving the direct problem. When implementing the direct task, radar and optical images are combined. A radar flow is formed from reflected signals from the underlying surface based on a modification of the method of radar synthesis of the antenna aperture with a frequency comparable to the video frequency. The optical image can, for example, be a digital terrain map pre-loaded into the aircraft’s memory, or an optical image recorded in an optical location system. As a result of this combination, a complex location image is formed, which is the input data for solving the inverse problem. The purpose of this research is to develop a methodology for classifying territories according to radar flow for constructing an appropriate (classification) map of the area based on segmentation methods, combining heterogeneous images, methods of remote sensing of the Earth, methods of mapping the earth’s surface and searching for reference mathematical (statistical) models of echo signals, which are basis for classification of small-sized airborne radars in spatially distributed systems. The practical significance of these studies lies in the fact that a new methodology for classifying territories is proposed, implemented in the equipment of multi-position systems on small aircraft for solving problems of environmental monitoring, as well as assessing vegetation cover and land use, in particular forestry, agriculture, as well as coastal, aquatic and marshy areas. boundaries of divisions of various zones to unlock the potential of using territories.
ER  - 


TY  - CONF
TI  - Improved Remote Sensing Classification and Detection Model Based on YOLOv7-Tiny
T2  - 2023 5th International Conference on Geoscience and Remote Sensing Mapping (GRSM)
SP  - 203
EP  - 209
AU  - J. Cheng
AU  - Z. Zhang
AU  - W. Shao
AU  - J. Yuan
AU  - B. Zhao
PY  - 2023
KW  - Target recognition
KW  - Object detection
KW  - Optical imaging
KW  - Adaptive optics
KW  - Optical sensors
KW  - Remote sensing
KW  - Image classification
KW  - Remote sensing image
KW  - target detection
KW  - YOLOv7-tiny
KW  - rotating frame
KW  - loss function
DO  - 10.1109/GRSM60169.2023.10425618
JO  - 2023 5th International Conference on Geoscience and Remote Sensing Mapping (GRSM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 5th International Conference on Geoscience and Remote Sensing Mapping (GRSM)
Y1  - 13-15 Oct. 2023
AB  - Aiming at the problem that due to the complex background of optical remote sensing images, non-rotating frame target detection is easy to introduce background information, resulting in image classification that is prone to errors and missed detections, a classification model based on improved YOLOv7-tiny was proposed. On the one hand, this method introduces rotating frame detection to YOLOv7-tiny, which avoids the prediction frame containing background information and effectively improves the classification recognition rate of the model. On the other hand, it proposes a dual-scale loss function, which effectively solves the problem of the rotation angle of the square prediction frame. The problem cannot be optimized, and the high-precision coverage target of the rotation prediction frame is achieved. This method has been tested on remote sensing ship, aircraft, and car classification data sets. The experimental results show that the average detection accuracy of the improved network is increased by about 8 %.
ER  - 


TY  - CONF
TI  - Object Detection and Identification Applied to Planes and Aircraft for Airport Surveillance
T2  - 2008 23rd International Conference Image and Vision Computing New Zealand
SP  - 1
EP  - 6
AU  - F. Robert-Inacio
AU  - S. Bussone
AU  - G. Chassaing
AU  - Y. Maziere
PY  - 2008
KW  - Object detection
KW  - Aircraft
KW  - Airports
KW  - Surveillance
KW  - Image sampling
KW  - Image segmentation
KW  - Acoustic signal detection
KW  - Filters
KW  - Labeling
KW  - Databases
KW  - Object detection
KW  - object classification
KW  - aerial image
KW  - airport surveillance
DO  - 10.1109/IVCNZ.2008.4762083
JO  - 2008 23rd International Conference Image and Vision Computing New Zealand
IS  - 
SN  - 2151-2205
VO  - 
VL  - 
JA  - 2008 23rd International Conference Image and Vision Computing New Zealand
Y1  - 26-28 Nov. 2008
AB  - For surveillance of an airport, it is obviously helpful to detect and identify planes and aircraft parked in each part of the tarmac to survey. A study has been achieved on a particular database and it has resulted in a classification of detected objects. The whole process includes an object detection stage, followed by a classification stage. The detection is set up on aerial views of the airport for different sampling steps. Then the classification stage sorts the detected objects into several families: small planes, jets, aircraft and so on. Experimental results are given in order to show the algorithm efficiency.
ER  - 


TY  - JOUR
TI  - A Human–Computer Fusion Framework for Aircraft Recognition in Remote Sensing Images
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 297
EP  - 301
AU  - X. Li
AU  - B. Jiang
AU  - S. Wang
AU  - L. Shen
AU  - Y. Fu
PY  - 2020
KW  - Aircraft
KW  - Feature extraction
KW  - Image recognition
KW  - Gaze tracking
KW  - Image segmentation
KW  - Encoding
KW  - Heating systems
KW  - Aircraft recognition
KW  - convolutional neural network (CNN)
KW  - eye tracking
KW  - feature encoding
KW  - remote sensing
DO  - 10.1109/LGRS.2019.2918955
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 2
SN  - 1558-0571
VO  - 17
VL  - 17
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - Feb. 2020
AB  - Existing aircraft recognition methods usually regard recognition as an isolated classification problem, supposing that aircraft detection has finished. These methods use image slices each containing single aircraft as input, which is often not the case in practice. In order to recognize aircraft in remote sensing images that contain multiple objects and background, we propose a human-computer fusion framework that combines the advantages of human and computer. First, we propose candidate aircraft using human eye tracking, making use of the efficient and accurate search ability of human. Then, we propose a two-step recognition method, which simulates the recognition process of image analysts, to identify the types of candidate aircraft. In the first step, we recognize aircraft using fully connected features of a fine-tuned convolutional neural network (CNN). If it does not work, we go to the second step. In the second step, we use convolutional features extracted from the fine-tuned CNN for recognition. To improve the representational ability of convolutional features, we introduce the Fisher Vector encoding. Thorough experiments demonstrate that the proposed framework is effective, surpassing state-of-the-art methods on recognition performance.
ER  - 


TY  - JOUR
TI  - Aircraft Classification Based on PCA and Feature Fusion Techniques in Convolutional Neural Network
T2  - IEEE Access
SP  - 161683
EP  - 161694
AU  - F. Azam
AU  - A. Rizvi
AU  - W. Z. Khan
AU  - M. Y. Aalsalem
AU  - H. Yu
AU  - Y. B. Zikria
PY  - 2021
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Remote sensing
KW  - Principal component analysis
KW  - Airplanes
KW  - Satellites
KW  - Support vector machines
KW  - Aircraft classification
KW  - CNN
KW  - feature extraction
KW  - feature fusion
KW  - identification of aircraft
DO  - 10.1109/ACCESS.2021.3132062
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - The characterization of aircraft in remote sensing satellite imagery has many armed and civil applications. For civil purposes, such as in tragedy and emergency aircraft searching, airport scrutiny and aircraft identification from satellite images are very important. This study presents an automated methodology based on handcrafted and deep convolutional neural network (DCNN) features. The presented aircraft classification technique consists of the following steps. The handcrafted features achieved from a local binary pattern (LBP) and DCNN are fused by feature fusion techniques. The DCNN features are extracted from Alexnet and Inception V3. Then we adopted a feature selection technique called principal component analysis (PCA). PCA removes the redundant and irrelevant information and improves the classification performance. Then, Famous supervised methodologies categorize these selected features. We chose the best classifier based on its highest accuracy. The proposed technique is executed on the multi-type aircraft remote sensing images (MTARSI) dataset, and the overall highest accuracy that we achieved from our proposed method is 96.8% by the linear support vector machine (SVM) classifier.
ER  - 


TY  - JOUR
TI  - Fine-Grained Image Classification Network Based on Reinforcement and Complementary Learning
T2  - IEEE Access
SP  - 28810
EP  - 28817
AU  - H. Jing
AU  - W. Meng-Yao
AU  - W. Fei
AU  - Z. Ru-Min
AU  - L. Bing-Quan
PY  - 2024
KW  - Feature extraction
KW  - Image classification
KW  - Convolutional neural networks
KW  - Training
KW  - Semantics
KW  - Reinforcement learning
KW  - Neural networks
KW  - Grain boundaries
KW  - Data models
KW  - Fine-grained image classification
KW  - inception-V3
KW  - complementary reinforcement learning
KW  - complementary learning
KW  - inter-class gap
DO  - 10.1109/ACCESS.2024.3368379
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - There are subtle differences between single regions of the same subcategory in fine-grained images. At present, many fine-grained image classification networks often focus on a single region to determine the target category. However, in many cases, most discriminative features in fine-grained images are distributed in multiple local regions of the image, and it is not often enough for fine-grained image to rely solely on one region.To solve these problems, a new method is proposed. This method generates discriminative features through reinforcement learning and obtains complementary regions through complementary network. The reinforcement network and the complementary network learn through adversarial learning and improve the accuracy of fine-grained images classification.The method is tested on CUB200-2011,fine-grained Visual Classification of Aircraft, and Stanford dogs datasets and the results show adequate performance.
ER  - 


TY  - JOUR
TI  - HiReNet: Hierarchical-Relation Network for Few-Shot Remote Sensing Image Scene Classification
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 10
AU  - F. Tian
AU  - S. Lei
AU  - Y. Zhou
AU  - J. Cheng
AU  - G. Liang
AU  - Z. Zou
AU  - H. -C. Li
AU  - Z. Shi
PY  - 2024
KW  - Remote sensing
KW  - Feature extraction
KW  - Scene classification
KW  - Task analysis
KW  - Measurement
KW  - Metalearning
KW  - Adaptation models
KW  - Convolutional neural networks (CNNs)
KW  - few-shot learning
KW  - hierarchical relation
KW  - remote sensing images
DO  - 10.1109/TGRS.2023.3348464
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 62
VL  - 62
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2024
AB  - Few-shot scene classification aims to develop models that can quickly adapt to new scenes with only a few labeled samples that are not present in training sets. In recent years, convolutional neural networks (CNNs) have made significant advancements in few-shot remote sensing image scene classification tasks. However, most existing approaches focus solely on utilizing high-level embeddings of remote sensing images to learn similarity relations, while neglecting intrinsic hierarchical representations that could be crucial in distinguishing scenes with substantial interclass similarities. To address this limitation, we propose a novel few-shot scene classification method for remote sensing images called hierarchical-relation network (HiReNet). This approach leverages the hierarchical features of a query sample and its corresponding support sample to learn discriminative representations. HiReNet consists of an embedding network and a relation network. The embedding network employs a Siamese architecture to extract representations, while the relation network utilizes these representations for classification. Within the relation network, we introduce a hierarchical relation learning (HRL) structure to capture the hierarchical relations among query and support samples. Additionally, to extract stronger features, we introduce a feature aggregation module that concatenates multilevel features and employs channel attention to re- weight these features. Experimental results demonstrate the superior performance of our HiReNet compared to several state-of-the-art few-shot scene classification methods.
ER  - 


TY  - CONF
TI  - Multi-Sensor Aircraft Classification
T2  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
SP  - 796
EP  - 800
AU  - S. Bolton
AU  - R. Dill
AU  - M. R. Grimaila
AU  - D. D. Hodson
PY  - 2023
KW  - Solid modeling
KW  - Meteorological radar
KW  - Three-dimensional displays
KW  - Soft sensors
KW  - Kinematics
KW  - Computer architecture
KW  - Radar imaging
KW  - Multivariate Long Short-Term Memory - Fully Convolutional Network
KW  - Automatic Dependent Surveillance-Broadcast
KW  - open-source data
KW  - classification
KW  - machine learning
KW  - sensor fusion
DO  - 10.1109/CSCE60160.2023.00136
JO  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
Y1  - 24-27 July 2023
AB  - Automatic Dependent Surveillance-Broadcast (ADS-B) is a useful tool to for air traffic controllers, military and other sources that are invested in understanding a national or global air picture. While it is highly available, it can sometimes lack integrity due to hacking, spoofing or, even, unintentional inaccuracies in the broadcast. Unlike primary radar, ADS-B's lack of trustworthiness makes it not feasible to rely on it alone. Fusing other data sources with ADS-B can help confirm the accuracy of the broadcasts or allow ADS-B to act as a surrogate for primary radar and bolster the information that primary radar can provide. This paper presents an effective method of using ADS-B data as a surrogate for primary 3D radar by combining the kinematic information that ADS-B data provides with weather and aircraft images to make predictions about aircraft characteristics.
ER  - 


TY  - CONF
TI  - Unmanned ground operations using semantic image segmentation through a Bayesian network
T2  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 868
EP  - 877
AU  - M. Coombes
AU  - W. Eaton
AU  - W. -H. Chen
PY  - 2016
KW  - Image segmentation
KW  - Image color analysis
KW  - Semantics
KW  - Military aircraft
KW  - Aircraft
KW  - Bayes methods
KW  - Feature extraction
KW  - Unmanned Ground Operations
KW  - Semantic Image Segmentation
KW  - Bayesian Network
KW  - Domain Knowledge
DO  - 10.1109/ICUAS.2016.7502572
JO  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 7-10 June 2016
AB  - This paper discusses the machine vision element of a system designed to allow automated taxiing for Unmanned Aerial System (UAS) around civil aerodromes. The purpose of the computer vision system is to provide direct sensor data which can be used to validate vehicle position, in addition to detect potential collision risks. This is achieved through the use of a singular monocular sensor. Untrained clustering is used to segment the visual feed before descriptors of each cluster (primarily colour and texture) are then used to estimate the class. As the competency of each individual estimate can vary based on multiple factors (number of pixels, lighting conditions and even surface type) a Bayesian network is used to perform probabilistic data fusion, in order to improve the classification results. This result is shown to perform accurate image segmentation in real-world conditions, providing information viable for map matching.
ER  - 


TY  - CONF
TI  - Object Recognition in Remote Sensing Images Using Combined Deep Features
T2  - 2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)
SP  - 606
EP  - 610
AU  - B. Jiang
AU  - X. Li
AU  - L. Yin
AU  - W. Yue
AU  - S. Wang
PY  - 2019
KW  - Feature extraction
KW  - Aircraft
KW  - Object recognition
KW  - Marine vehicles
KW  - Remote sensing
KW  - Image recognition
KW  - Training
KW  - object recognition
KW  - aircraft recognition
KW  - ship recognition
KW  - fine-grained recognition
KW  - convolutional neural network (CNN)
KW  - remote sensing
DO  - 10.1109/ITNEC.2019.8729392
JO  - 2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)
Y1  - 15-17 March 2019
AB  - Object recognition, which is also referred as object classification or object type recognition, aims at discriminating object types in remote sensing images. With the availability of high resolution remote sensing images, object recognition attracts more and more attention. Different from traditional methods mainly using hand-crafted features, we propose an object recognition method that combines deep features extracted from a convolutional neural network (CNN) to recognize aircrafts and ships in remote sensing images. The proposed method consists of two stages. In the training stage, images of objects with different types and corresponding labels are exploited to fine-tune a pre-trained CNN. Convolutional features are extracted from a convolutional layer of the fine-tuned CNN and pooled by Fisher Vector, and fully-connected features are extracted from a fully-connected layer of the CNN. These features are combined by concatenation and used to train a support vector machine (SVM). In the test stage, the type of each object is determined by the trained SVM using its combined features. Experiments on two data sets collected from Google Earth demonstrate the effectiveness of our method.
ER  - 


TY  - CONF
TI  - Classification of aerospace targets using superresolution ISAR images
T2  - Proceedings of COMSIG '94 - 1994 South African Symposium on Communications and Signal Processing
SP  - 138
EP  - 145
AU  - E. C. Botha
PY  - 1994
KW  - Image resolution
KW  - Aircraft
KW  - Target recognition
KW  - Multiple signal classification
KW  - Frequency measurement
KW  - Aerospace engineering
KW  - Strips
KW  - Neural networks
KW  - Signal resolution
KW  - Africa
DO  - 10.1109/COMSIG.1994.512452
JO  - Proceedings of COMSIG '94 - 1994 South African Symposium on Communications and Signal Processing
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of COMSIG '94 - 1994 South African Symposium on Communications and Signal Processing
Y1  - 4-4 Oct. 1994
AB  - This paper describes investigations into a recognition system that takes the 2-D ISAR (inverse synthetic aperture radar) image of an aerospace target as input and classifies the target based on features calculated from the image. Four types of features were implemented, namely geometrical moments, invariant features based on moments, shape features, and quantized energy strips. Nearest-neighbour and neural-net classifiers are considered.
ER  - 


TY  - CONF
TI  - Improved Attentive Pairwise Interaction (API-Net) for Fine-Grained Image Classification
T2  - 2021 Emerging Technology in Computing, Communication and Electronics (ETCCE)
SP  - 1
EP  - 6
AU  - O. Z. Yet
AU  - T. H. Rassem
AU  - M. A. Rahman
AU  - M. M. Rahman
PY  - 2021
KW  - Training
KW  - Shape
KW  - Brightness
KW  - Gray-scale
KW  - Pattern recognition
KW  - Aircraft
KW  - Image classification
DO  - 10.1109/ETCCE54784.2021.9689910
JO  - 2021 Emerging Technology in Computing, Communication and Electronics (ETCCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 Emerging Technology in Computing, Communication and Electronics (ETCCE)
Y1  - 21-23 Dec. 2021
AB  - Fine-grained classification is a challenging problem as one has to deal with a similar class of objects but with various types of variations. For more elaboration, they are almost similar and have subtle differences, and are confusing. In this study, aircraft will be the fine-grained object to be focused on. Aircraft which has almost similar shapes and patterns can be hardly recognized even for humans, especially those who haven not gone through any training. In recent years, a lot of proposed methods addressed to solve the difficulties in fine-grained problems by learning contrastive clues from an image. This study aims to increase the accuracy of the Attentive Pairwise Interaction Network (API-Net) by introducing data augmentation into the network structure. Some of the previous studies proved that data augmentation does help improve a network. So, this study is going to modify the API-Net with different data augmentation settings. In this study, various settings have been introduced to the API-Net. Several experiments had been done with a simple modification where a portion of the train dataset’s images will randomly convert into greyscale images. These settings are, only brightness & contrast 0.2, only grayscale 0.3, only grayscale 0.5, brightness & contrast 0.2 with grayscale 0.3, and brightness & contrast 0.2 with grayscale 0.5. As a result, the proposed modification achieved with 92.74% with brightness & contrast 0.2, 92.80% on brightness & contrast 0.2 with grayscale 0.5, and 92.86% on brightness & contrast 0.2 with grayscale 0.3. While grayscale 0.3 alone achieve 93.25% and grayscale 0.5 alone achieve 93.46% compared with the original results which reached 92.77%.
ER  - 


TY  - CONF
TI  - Marine Bird Detection Based on Deep Learning using High-Resolution Aerial Images
T2  - OCEANS 2019 - Marseille
SP  - 1
EP  - 7
AU  - L. B. Boudaoud
AU  - F. Maussang
AU  - R. Garello
AU  - A. Chevallier
PY  - 2019
KW  - Birds
KW  - Training
KW  - Computational modeling
KW  - Deep learning
KW  - Computer vision
KW  - Aircraft
KW  - Task analysis
KW  - deep learning
KW  - CNN
KW  - marine life
KW  - detection
KW  - classification
DO  - 10.1109/OCEANSE.2019.8867242
JO  - OCEANS 2019 - Marseille
IS  - 
SN  - 
VO  - 
VL  - 
JA  - OCEANS 2019 - Marseille
Y1  - 17-20 June 2019
AB  - The overarching goal of this paper is to find an automatic bird detection and counting method on aerial images of the ocean. Most of the existing works in the literature are based on heuristic handcrafted feature design, which in most cases affect the effectiveness (the accuracy of classification) and the efficiency (spending much time). In this paper, we propose a method built on a systematic feature learning based classification adopting a new deep Convolutional Neural Network (CNN) architecture. Through this architecture, the feature learning is automated from a multi-dimensional raw input images, by a training step leveraging the JONATHAN dataset via supervised learning. Performances evaluation show that the CNN based architecture classifier achieves an accuracy of 95% on the JONATHAN test data and the overall detection method achieves a classification rate (true positives: birds) of 98% on real images.
ER  - 


TY  - CONF
TI  - Short-Wave Aviation Communication Signal Analysis and Aircraft Classification
T2  - 2008 International Multi-symposiums on Computer and Computational Sciences
SP  - 114
EP  - 118
AU  - F. Liu
AU  - X. Li
AU  - Z. Liang
PY  - 2008
KW  - Signal analysis
KW  - Aircraft propulsion
KW  - Aerospace engineering
KW  - Aerospace safety
KW  - National security
KW  - Radar imaging
KW  - Radar detection
KW  - Monitoring
KW  - Geophysics
KW  - Marine safety
KW  - Short-wave Aviation Communication
KW  - HHT
KW  - EMD
KW  - EEMD
KW  - SVM
DO  - 10.1109/IMSCCS.2008.24
JO  - 2008 International Multi-symposiums on Computer and Computational Sciences
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2008 International Multi-symposiums on Computer and Computational Sciences
Y1  - 18-20 Oct. 2008
AB  - This paper investigates how to classify the type of the aircraft based on the aviation communication signal. Hilbert-Huang Transform (HHT) is introduced to adaptively decompose the cockpit voice into several IMFs, then the IMFs can be used as the features of the aircraft for identifying the type. To alleviate the problem of mode mixing caused by the signal intermittency in EMD, a novel method EEMD is applied here in decomposing short-wave aviation communication. The results show that the performance of HHT based on EEMD is better than that of original EMD in decomposition and classification.
ER  - 


TY  - CONF
TI  - Fine-grained image classification based on the combination of artificial features and deep convolutional activation features
T2  - 2017 IEEE/CIC International Conference on Communications in China (ICCC)
SP  - 1
EP  - 6
AU  - Q. Zheng
AU  - M. Yang
AU  - Q. Zhang
AU  - X. Zhang
PY  - 2017
KW  - Feature extraction
KW  - Image classification
KW  - Support vector machines
KW  - Classification algorithms
KW  - Convolution
KW  - Training
KW  - Kernel
DO  - 10.1109/ICCChina.2017.8330485
JO  - 2017 IEEE/CIC International Conference on Communications in China (ICCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 IEEE/CIC International Conference on Communications in China (ICCC)
Y1  - 22-24 Oct. 2017
AB  - Fine-grained image classification is a challenging research topic in the field of computer vision, whose goal is to identify subclasses, such as distinguishing between different kinds of dogs. In order to overcome this problem, we combine the advantages of artificial feature with deep convolutional activation feature and design support vector machines (SVM) based on the importance of features. In this paper, we first use the bilinear neural network model to extract the deep convolutional activation feature of samples and combine it with artificial feature. The bilinear form simplifies gradient computation and allows end-to-end training. Then, multi-kernel SVM based on weighted features are trained to complete the image classification task. Finally, we present experiments and visualizations on FGVC-Aircraft and Stanford Dogs databases that analyze the effects of combined features and the multi-kernel SVM on the fine-grained object classification. The 83.8% and 66.1% accuracy proves the effectiveness of our strategy.
ER  - 


TY  - JOUR
TI  - Adap-EMD: Adaptive EMD for Aircraft Fine-Grained Classification in Remote Sensing
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 1
EP  - 5
AU  - Y. Nie
AU  - C. Bian
AU  - L. Li
PY  - 2022
KW  - Aircraft
KW  - Remote sensing
KW  - Feature extraction
KW  - Task analysis
KW  - Military aircraft
KW  - Mathematical models
KW  - Earth
KW  - Attention mechanism
KW  - earth mover’s distance (EMD) distance
KW  - few-shot learning (FSL)
KW  - fine-grained classification
DO  - 10.1109/LGRS.2022.3168581
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 
SN  - 1558-0571
VO  - 19
VL  - 19
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - 2022
AB  - Aircraft classifiers in remotely sensed images based on deep convolutional neural networks play a significant role in military. However, in practical applications, there is a lack of remote sensing fine-grained aircraft data. In this study, we demonstrate that few-shot learning (FSL) can be effectively used for fine-grained identification of aircraft and propose a new classifier-adaptive earth mover’s distance (Adap-EMD) for recognition of few-sample fine-grained aircraft. Adap-EMD consists of an efficient block attention mechanism (EBAM) and an adaptive feature measurement filter (AFMF). The EBAM effectively fuses channel and spatial correlation to capture global features with more pixel-wise relevance and contextual information. The non-parametric AFMF expresses the key information from the adapted emphasizing feature map to achieve a more accurate similarity measurement. Our model outperforms state-of-the-art models on a major few-shot aircraft fine-grained recognition benchmark dataset, introducing only a few additional computations.
ER  - 


TY  - CONF
TI  - Learning to Focus and Discriminate for Fine-Grained Classification
T2  - 2019 IEEE International Conference on Image Processing (ICIP)
SP  - 415
EP  - 419
AU  - Z. Feng
AU  - K. Fu
AU  - Q. Zhao
PY  - 2019
KW  - Feature extraction
KW  - Training
KW  - Aircraft
KW  - Automobiles
KW  - Birds
KW  - Testing
KW  - Proposals
KW  - Fine-grained classification
KW  - region proposal
KW  - discriminative region localization
KW  - attention
KW  - convolutional neural networks
DO  - 10.1109/ICIP.2019.8803005
JO  - 2019 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Image Processing (ICIP)
Y1  - 22-25 Sept. 2019
AB  - Existing state-of-the-art fine-grained classification methods usually use separated networks for discriminative region localization and feature learning/classification, and are thus complicated to implement and optimize. In this paper, we aim to provide a compact solution by deepening the collaboration between the region localization, feature learning and classification modules during the learning process of fine-grained classification. We thus propose a method that can learn to simultaneously localize discriminative regions and extract discriminative features by exploring the localization ability of classification convolutional neural networks and joint optimization of different modules. Our method, while being built upon a single backbone network and trained with only softmax losses, achieves state-of-the-art performance on three benchmark fine-grained datasets, which proves that our method is simple but effective for fine-grained classification.
ER  - 


TY  - CONF
TI  - Convolutional Neural Network based Deep Feature Extraction in Remote Sensing Images
T2  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
SP  - 441
EP  - 444
AU  - P. Dharani Devi
AU  - R. Thanuja
PY  - 2020
KW  - Feature extraction
KW  - Machine learning
KW  - Object recognition
KW  - Remote sensing
KW  - Aircraft
KW  - Convolutional neural networks
KW  - Image segmentation
KW  - Object recognition
KW  - Deep learning
KW  - Aircraft recognition
KW  - Remote sensing
KW  - Convolutional Neural Network (CNN)
DO  - 10.1109/ICOSEC49089.2020.9215274
JO  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
Y1  - 10-12 Sept. 2020
AB  - Remote sensing is an important process that helps in the research and development of any country across the globe. Likewise, object recognition and object classification techniques provide major contributions in the development of artificial intelligence and machine learning algorithms. The main aim of these methods is to teach the computers to acquire, process and predict the data for future developments. Object identification and classification methods are used everywhere these days. For example, industry, medical imaging, space technologies, forensic etc. One the field that highly utilizing the remote sensing and object recognition method is the defense system. This paper proposes a method to identify the aircraft from the remote sensing images by using deep learning and image processing techniques. The process consists of two stages training stages and test stages. The proposed system could identify and classify the aircraft from the remote sensing images by using the convolutional neural network and deep learning technique.
ER  - 


TY  - JOUR
TI  - An Experimental Testbed for the Study of Visual Based Navigation Docking of Two Vertical Compound Aircraft
T2  - IEEE Access
SP  - 75035
EP  - 75048
AU  - D. Wang
AU  - Q. Hong
AU  - J. Wang
AU  - H. Sun
AU  - L. Cheng
AU  - M. Li
AU  - C. Wang
AU  - X. Huang
AU  - Z. Wang
AU  - J. Li
PY  - 2021
KW  - Aircraft
KW  - Aircraft navigation
KW  - Compounds
KW  - Visualization
KW  - Gears
KW  - Tires
KW  - Process control
KW  - Compound aircraft
KW  - collision avoidance
KW  - docking process
KW  - visual based navigation
DO  - 10.1109/ACCESS.2021.3081507
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - Compound aircraft, merging the advantages of two or more aircraft, is now a promising concept for designing modern aircraft. How to achieve successful docking of two aircraft in flight is an important issue for the applications of vertical compound aircraft. In order to resolve such an issue, this paper proposes a new visual based navigation docking scheme, which is different from rendezvous and docking operations in space applications. The present scheme uses a monocular camera mounted on the chaser aircraft with a certain installation angle to collect the visual features of the target aircraft, then the position and attitude of the landing gear of the target aircraft can be determined. Moreover, a six degrees of freedom docking mechanism mounted on the chaser aircraft is designed to catch hold of the landing gear tires of the target aircraft. In order to avoid undesired collision during the docking process, Z-direction position of the target landing gear is predicted and the proposed Z-direction speed control algorithm is applied to the chaser platform. The robustness of the present scheme has been validated numerically and experimentally by means of a chaser platform and a target platform on the ground testbed. Successful docking of the chaser platform and the target landing gear has been achieved, respectively, when the target platform performs a Z-direction movement and a compound movement.
ER  - 


TY  - CONF
TI  - A Corrosion Energy Feature Extract Algorithm of Aircraft Skin Based on Wavelet
T2  - 2010 International Conference on Intelligent System Design and Engineering Application
SP  - 3
EP  - 7
AU  - G. Qing-ji
AU  - Z. Lei
PY  - 2010
KW  - Corrosion
KW  - Support vector machine classification
KW  - Wavelet transforms
KW  - Feature extraction
KW  - Aircraft
KW  - Skin
KW  - magneto optic image (MOI)
KW  - corrosion
KW  - wavelet transform
KW  - energy feature
DO  - 10.1109/ISDEA.2010.75
JO  - 2010 International Conference on Intelligent System Design and Engineering Application
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2010 International Conference on Intelligent System Design and Engineering Application
Y1  - 13-14 Oct. 2010
AB  - The mechanism of aircraft skin corrosion is studied and a corrosion detection algorithm based on wavelet analysis is proposed. First of all, the magneto-optic image is decomposed by the wavelet analysis. In the next place a feature vector is assigned whose components represent energy in each sub-image. Lastly, a 1-nearest neighbor method classifier is applied to classify the proceeding feature vector as either corresponding to a region of corrosion or corresponding to a region of non-corrosion. The experimental results demonstrate that the proposed algorithm is insensitive to noise and the features are easy to extract. The algorithm has high recognition ratios and robustness for corrosion detection and the most important is that it can meet the real-time request.
ER  - 


TY  - CONF
TI  - Channel Interaction Attention Networks for Fine-Grained Visual Classification
T2  - 2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)
SP  - 606
EP  - 611
AU  - Q. Chen
AU  - W. Wei
PY  - 2022
KW  - Training
KW  - Visualization
KW  - Correlation
KW  - Annotations
KW  - Benchmark testing
KW  - Feature extraction
KW  - Boosting
KW  - channel interaction structure
KW  - attention mechanism
KW  - fine-grained visual classification
DO  - 10.1109/CVIDLICCEA56201.2022.9824611
JO  - 2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)
Y1  - 20-22 May 2022
AB  - The high similarity between subcategories and low similarity within subcategories in fine-grained visual classification leads to difficulties in classification. Most existing methods take a single image into a model and perform image classification by locating discriminative regions and learning fine-grained features, ignoring the contrast information between images which also facilitates classification. A method named CIANet was proposed, which includes: (1) channel interaction structure, using the bilinear operation to obtain channel correlation between images, and integrating original features to extract complementary features. (2) The attention boosting and suppression module takes the channel features with the highest weight as the attention map for boosting features and suppressing significant regions of the image, guiding the model to learn more fine-grained differentiating features. CIA-Net does not rely on extra bounding boxes and partial annotations for end-to-end training. Experiments were conducted on three benchmark datasets (CUB-200-2011, FGVC-Aircraft, and Stanford Cars) and the results showed that CIA-Net has higher classification accuracy.
ER  - 


TY  - CONF
TI  - Remote Sensing Image Aircraft Detection Technology Based on Deep Learning
T2  - 2019 11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
SP  - 173
EP  - 177
AU  - W. Wei
AU  - J. Zhang
PY  - 2019
KW  - Remote sensing image, shadow processing, deep learning, target detection
DO  - 10.1109/IHMSC.2019.00048
JO  - 2019 11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2019 11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
Y1  - 24-25 Aug. 2019
AB  - How to use high-resolution remote sensing images to quickly and accurately obtain high-value target information such as aircraft and ships has become a hot topic in the research of automatic target detection. This paper uses Google Earth image to make aircraft data set, then detect the aircraft targets respectively based on the two deep learning models YOLOv3 and Faster_R_CNN. At the same time, in order to significantly improve the aircraft detection accuracy, this paper proposes a shadow processing algorithm with double threshold random sampling to conduct data preprocessing for aircraft targets in remote sensing images. The experimental results show that both deep learning models can effectively detect aircraft targets and have great application potential in automatic detection of remote sensing image targets. The shadow processing algorithm can effectively eliminate the projected shadow of the aircraft, restore the texture characteristics of the shaded area, greatly improve the overall quality of the remote sensing image, and lay a good data foundation for subsequent aircraft detection.
ER  - 


TY  - CONF
TI  - Automatic target recognition of aircraft models based on ISAR images
T2  - 2009 IEEE International Geoscience and Remote Sensing Symposium
SP  - IV-685
EP  - IV-688
AU  - M. N. Saidi
AU  - K. Daoudi
AU  - A. Khenchaf
AU  - B. Hoeltzener
AU  - D. Aboutajdine
PY  - 2009
KW  - Target recognition
KW  - Aircraft
KW  - Shape
KW  - Support vector machines
KW  - Support vector machine classification
KW  - Inverse synthetic aperture radar
KW  - Airborne radar
KW  - Radar imaging
KW  - Synthetic aperture radar
KW  - Data mining
KW  - Inverse synthetic aperture radar
KW  - Automatic target recognition
KW  - shapes extraction
KW  - Classification
KW  - Information fusion
DO  - 10.1109/IGARSS.2009.5417469
JO  - 2009 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 4
VL  - 4
JA  - 2009 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 12-17 July 2009
AB  - In this paper, we present a system for aircraft automatic target recognition (ATR) using inverse synthetic aperture radar (ISAR) and based on knowledge discovery from data process adapted to radar domain. We propose a method for target shape extraction from ISAR images based on the combination of two methods, SUSAN modified and active deformable contours via level set. In the second part of this work, we propose to fuse two commonly used shape descriptors algorithms based on moments invariant and Fourier descriptors. We have investigated the impact of the information fusion on the recognition rate. The classification scheme is ensured using support vector machine (SVM) classifier. Several combination strategies are compared at score/decision/feature level. Experimental results of the proposed method are provided and discussed.
ER  - 


TY  - CONF
TI  - Detection of clouds in sky/cloud and aerial images using moment based texture segmentation
T2  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1124
EP  - 1133
AU  - D. Tulpan
AU  - C. Bouchard
AU  - K. Ellis
AU  - C. Minwalla
PY  - 2017
KW  - Clouds
KW  - Meteorology
KW  - Feature extraction
KW  - Image color analysis
KW  - Visualization
KW  - Aircraft
KW  - Image segmentation
DO  - 10.1109/ICUAS.2017.7991380
JO  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 13-16 June 2017
AB  - Unmanned aircraft flying beyond line of sight in uncontrolled airspace need to maintain adequate separation from local inclement weather patterns for regulatory compliance and operational safety. Although commercial solutions for `weather avoidance' exist, they are tailored to manned aviation and as such either lack the accuracy or the size, weight, and power (SWaP) requirements of small Unmanned Aerial System (UAS). Detection and ranging to the cloud ceiling is a key component of weather avoidance. Proposed herein is a computer vision approach to cloud detection consisting of feature extraction and machine learning. Six image moments on local texture regions were extracted and fused within a classification algorithm for discrimination of cloud pixels. Three different popular classifiers were evaluated for efficacy. Two publicly available datasets of all-sky images were utilized for training and test datasets. The proposed approach was compared to five well-known thresholding techniques via quantitative analysis. Results indicate that our method consistently outperformed the popular thresholding methods across all tested images. Comparison between the classification techniques indicated random forests to possess the highest training accuracy, while multilayer perceptrons showed better prediction accuracy on the test dataset. Upon extending the method to realistic images including background clutter, the random forest classifier demonstrated the best training accuracy of 100% and the best prediction accuracy of 96%. Although computationally more expensive, the random forest classifier also produced the fewest number of false positives. A sensitivity analysis for window sizes is presented for robust validation of the chosen approach, which showed that detection accuracy improved in proportion to window size at the expense of computation time.
ER  - 


TY  - CONF
TI  - An Infrared image process approach for Military Aircraft Recognition with single training sample
T2  - 2022 China Automation Congress (CAC)
SP  - 4177
EP  - 4180
AU  - Y. Ouyang
AU  - P. Deng
AU  - X. Huang
AU  - R. Xiao
AU  - B. Shi
AU  - Y. Jiang
PY  - 2022
KW  - Training
KW  - Image recognition
KW  - Image coding
KW  - Dictionaries
KW  - Target recognition
KW  - Shape
KW  - Military aircraft
KW  - Infrared image recognition
KW  - Single training sample
KW  - Sparse coding
DO  - 10.1109/CAC57257.2022.10054935
JO  - 2022 China Automation Congress (CAC)
IS  - 
SN  - 2688-0938
VO  - 
VL  - 
JA  - 2022 China Automation Congress (CAC)
Y1  - 25-27 Nov. 2022
AB  - Military target recognition task often faces the situation of lack of training samples. In some extreme conditions, each category of military target may have only one training sample. This paper proposed a method to deal with the military aircraft recognition task with only one training sample, first the HOG and Hu moment invariant features are extracted, and then sparse based classification framework is used as the recognition algorithm. Experimental results show that the proposed method can achieve better recognition accuracy rate than the existing methods when there is only one single training sample.
ER  - 


TY  - JOUR
TI  - Detection of Curved Rows and Gaps in Aerial Images of Sugarcane Field Using Image Processing Techniques
T2  - IEEE Canadian Journal of Electrical and Computer Engineering
SP  - 303
EP  - 310
AU  - B. Moraes Rocha
AU  - G. S. Vieira
AU  - A. U. Fonseca
AU  - N. M. Sousa
AU  - H. Pedrini
AU  - F. Soares
PY  - 2022
KW  - Crops
KW  - Remotely piloted aircraft
KW  - Proposals
KW  - Sugar industry
KW  - Soil
KW  - Image segmentation
KW  - Area measurement
KW  - Crop rows
KW  - image segmentation
KW  - intelligent agriculture
KW  - remotely piloted aircraft (RPA)
DO  - 10.1109/ICJECE.2022.3178749
JO  - IEEE Canadian Journal of Electrical and Computer Engineering
IS  - 3
SN  - 2694-1783
VO  - 45
VL  - 45
JA  - IEEE Canadian Journal of Electrical and Computer Engineering
Y1  - Summer 2022
AB  - Sugarcane is one of the main crops in the world due to its economic value promoted by the sale of its derivatives, such as bioethanol and sugar. In order to achieve greater economic performance and productivity in the sugarcane field, several digital image processing studies have been conducted on sugarcane field images. However, mapping and measuring gaps in the planting rows are still being performed manually on-site to determine whether to replant the entire area or only the gaps. High cost of time and manpower is required to perform the manual measurement. Based on that, the aim of this study is to present a novel method to detect crop rows and measure gaps in crop fields. Our method is also able to deal with curved crop rows, which is a real problem and substantially limits numerous solutions in practical applications. The proposed method is evaluated using a mosaic of real scene image that was prepared with the support of a small remotely piloted aircraft. Experimental tests showed a low relative error of approximately 1.65% compared to manual mapping in the planting regions, even for regions with gaps in the curved crop rows. It means that our proposal can identify and measure crop rows accurately, which enables automated inspections with high-precision measurements.
ER  - 


TY  - JOUR
TI  - m-ABCNet: Multi-Modal Aircraft Motion Behavior Classification Network at Airport Ramps
T2  - IEEE Access
SP  - 133982
EP  - 133993
AU  - S. Lee
PY  - 2024
KW  - Airports
KW  - Aircraft propulsion
KW  - Cameras
KW  - Ground support
KW  - Aircraft navigation
KW  - Air traffic control
KW  - Object detection
KW  - Multimodal sensors
KW  - Aircraft behavior attribute classification
KW  - airport object detection
KW  - multimodal sensors
KW  - airport ramp
DO  - 10.1109/ACCESS.2024.3462096
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Self-driving baggage tractors on airport ramps signify emerging trends for enhancing operational procedures at airports and contribute to the growth of the aviation industry. Airport ramps are characterized by unique mobility requirements, including layout, population, demand, and traffic patterns. Among these, accurate estimation of aircraft movements is paramount for safety and compliance with airport operational regulations. Contrary to the dynamic nature of other environments, airport ramps have predominantly static and slow movements. Even when an aircraft is parked and stationary, other operational vehicles must exercise caution or halt when the airplane is preparing to push back from or into a parking space. This aspect of airport operations has not been addressed adequately in existing research, and prior studies have rarely considered the detection of airplanes on ramps. This work introduces a context-aware multimodal approach for detecting airplane intentions on airport ramps using RGB and thermal cameras. The proposed methodology involves parallel extraction of behavioral features from airplanes and situational context from their surrounding objects. This approach enables estimation of the movement attributes of the aircraft in relation to other objects on the ramp. The effectiveness of this algorithm was validated through a comprehensive dataset collected from the Cincinnati and Northern Kentucky Airport using the proposed platform. Accordingly, the performance improved by 15.76% with the proposed modality through the use of the thermal camera and additionally by 7.29% through utilization of the proposed network.
ER  - 


TY  - CONF
TI  - Incremental Learning of Remote Sensing Target Classification with Class Hierarchy
T2  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
SP  - 6254
EP  - 6257
AU  - Y. Chu
AU  - P. Wang
AU  - Y. Qian
PY  - 2023
KW  - Training
KW  - Learning systems
KW  - Task analysis
KW  - Marine vehicles
KW  - Remote sensing
KW  - Image classification
KW  - Incremental Learning
KW  - Learning without Forgetting
KW  - Hierarchical classification
KW  - Deep learning
DO  - 10.1109/IGARSS52108.2023.10283281
JO  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 16-21 July 2023
AB  - Incremental learning can continuously learn to address new tasks from new data while preserving knowledge learned from previously learned tasks. Incremental remote sensing target classification aims to accurately classify newly added classes while maintaining the classification performance of the old classes. In this paper, we propose a new incremental learning method using the information of class hierarchy (CH) and the strategy of Learning without Forgetting (LWF), named CH-LWF. As we know, the target classes in remote sensing images always have hierarchical relationships and are organized as hierarchical tree structures, and CH has been proven to benefit target classification. Our work in this paper may be the first to introduce CH into incremental learning of remote sensing images. In addition, LWF can learn new classes only with new training samples, and the training samples for old classes are not required to be reused, which is convenient in real applications.
ER  - 


TY  - CONF
TI  - Using VGG16 to Military Target Classification on MSTAR Dataset
T2  - 2021 2nd China International SAR Symposium (CISS)
SP  - 1
EP  - 3
AU  - Y. Gu
AU  - J. Tao
AU  - L. Feng
AU  - H. Wang
PY  - 2021
KW  - Image resolution
KW  - Target recognition
KW  - Neural networks
KW  - Reconnaissance
KW  - Military aircraft
KW  - Feature extraction
KW  - Radar polarimetry
KW  - SAR
KW  - MSTAR dataset
KW  - VGG16
KW  - target classification
DO  - 10.23919/CISS51089.2021.9652365
JO  - 2021 2nd China International SAR Symposium (CISS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 2nd China International SAR Symposium (CISS)
Y1  - 3-5 Nov. 2021
AB  - Synthetic aperture radar has the characteristics of all-weather, all-weather, long range, high resolution, etc., and has played an important role in the fields of battlefield reconnaissance, detection and guidance. Target recognition technology based on SAR images, especially ground military target recognition technology, has received widespread attention. The MSTAR dataset is composed of SAR images of ground stationary targets provided by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL), including civilian and military targets. The convolutional neural network consists of a series of convolutional layers, pooling layers and fully connected layers. It can obtain effective feature representation from big data and recognize it through automatic learning, eliminating the complicated feature extraction algorithm and feature matching process. Now it has been widely used in the field of target interpretation. Experiments show that using the existing neural network VGG16 to classify military targets on the MSTAR data set can obtain good classification accuracy.
ER  - 


TY  - CONF
TI  - A High-Efficiency Aircraft Detection Approach Utilizing Auxiliary Information in Sar Images
T2  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1700
EP  - 1703
AU  - X. Xiao
AU  - X. Yu
AU  - H. Wang
PY  - 2022
KW  - Limiting
KW  - Costs
KW  - Costing
KW  - Geoscience and remote sensing
KW  - Airports
KW  - Radar polarimetry
KW  - Classification algorithms
KW  - Synthetic Aperture Radar
KW  - Aircraft Detection
KW  - Geographical information coordinates
KW  - Subscene Classification
DO  - 10.1109/IGARSS46834.2022.9884883
JO  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 17-22 July 2022
AB  - Aircraft detection in synthetic aperture radar (SAR) image is a special case because all the targets are located in the airport. Comparing with the whole scene SAR image, the area of an airport is relatively small, therefore, this information can be utilized to speed up the algorithm. This paper proposes a centroid network detection method based on the combination of geographic coordinate information and subscene classification. Firstly, the airport area is detected based on the priori geographic coordinate information. Secondly, to further narrow down the scope of detection and extract the regions containing valid targets, the subscene is fed into the ResNet50 network which incorporates Squeeze and Excitation (SE) to separate the aircraft area from the background area. The method is validated in ablation experiments on the GaoFen-3(GF3) datasets to reveal the impact of each factor. The results show that the proposed method can achieve a reduction in false alarm rate around 6% and time cost around 30%.
ER  - 


TY  - CONF
TI  - Multistatic passive radar imaging using the smoothed pseudo Wigner-Ville distribution
T2  - Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)
SP  - 604
EP  - 607 vol.3
AU  - Yong Wu
AU  - D. C. Munson
PY  - 2001
KW  - Passive radar
KW  - Aircraft
KW  - TV
KW  - Radar polarimetry
KW  - Image resolution
KW  - Image analysis
KW  - Frequency
KW  - Radio transmitters
KW  - Receivers
KW  - Image reconstruction
DO  - 10.1109/ICIP.2001.958191
JO  - Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)
Y1  - 7-10 Oct. 2001
AB  - We investigate passive radar imaging of aircraft using reflected TV signals. We apply a smoothed pseudo Wigner-Ville distribution (SPWVD) based SAR imaging algorithm to two different scenarios. In the first simulation, a multistatic VHF-band dataset generated by fast Illinois solver code (FISC) is used. In the second simulation, a more realistic simulated passive radar dataset is used. A set of instantaneous images is produced by our algorithm. They have higher resolution and show more detail and features of the aircraft than can be obtained by direct Fourier reconstruction (DFR). The set of images provides more visual information about the target and helps to estimate its shape and features. This study suggests that the SPWVD-based imaging might be useful in passive radar imaging and target classification.
ER  - 


TY  - CONF
TI  - Probability-Based Binary Attribute Weighted Prediction Network for SAR Image Classification
T2  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
SP  - 7038
EP  - 7041
AU  - X. Xiao
AU  - Z. Ye
AU  - Q. Liu
AU  - H. Wang
PY  - 2023
KW  - Limiting
KW  - Semantics
KW  - Geoscience and remote sensing
KW  - Apertures
KW  - Radar polarimetry
KW  - Task analysis
KW  - Aircraft
KW  - Synthetic Aperture Radar
KW  - Attribute Embedding
KW  - Deep Learning
KW  - Image Classification
DO  - 10.1109/IGARSS52108.2023.10281623
JO  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 16-21 July 2023
AB  - The problem of insufficient samples has been limiting the performance of intelligent interpretation in Synthetic Aperture Radar (SAR) images. Humans have the ability to recognize new instances with only a few samples, indicating that attributes play a crucial role in recognition. Attributes can be shared across categories and provide a distinctive representation. Motivated by this fact, this paper proposes an attribute-guided network consisting of a base classifier (BC) and an attribute classifier (AC). Firstly, we design binary attributes for SAR objects to enable more distinct feature representations. Secondly, the images are mapped into a semantic embedding space by embedding the attribute vectors. Finally, the performance of few-shot classification in SAR images is improved by jointly optimizing the loss in both attribute space and deep feature space. Our attribute-based framework is validated through ablation experiments on the MSTAR dataset and a self-built SAR aircraft dataset.
ER  - 


TY  - JOUR
TI  - High-resolution radar imagery of the Mirage III aircraft
T2  - IEEE Transactions on Antennas and Propagation
SP  - 1356
EP  - 1360
AU  - A. Zyweck
AU  - R. E. Bogner
PY  - 1994
KW  - Airborne radar
KW  - Radar imaging
KW  - Radar scattering
KW  - Aircraft propulsion
KW  - Backscatter
KW  - Military aircraft
KW  - Diffraction
KW  - Engines
KW  - Inverse synthetic aperture radar
KW  - Synthetic aperture radar
DO  - 10.1109/8.318658
JO  - IEEE Transactions on Antennas and Propagation
IS  - 9
SN  - 1558-2221
VO  - 42
VL  - 42
JA  - IEEE Transactions on Antennas and Propagation
Y1  - Sept. 1994
AB  - High-resolution radar imagery has attracted increasing interest in recent years. As more radars are endowed with a high-resolution capability, target classification will become a regular system function. In order to classify an aircraft using radar, one must have an understanding of how the radar imagery relates to the physical aircraft. This paper illustrates the more important radar backscattering features on a typical fighter aircraft. Radar backscatter from an aircraft can occur through a variety of mechanisms. Although direct specular and diffractive mechanisms usually account for the majority of the scattering, indirect phenomena such as cavity scattering and creeping wave scattering are significant. This investigation finds that scattering from engine cavities is a particularly important radar backscatter mechanism for a fighter aircraft. Radar data of an actual Mirage aircraft is collected from a target turntable facility. This data is processed to obtain high-resolution range profiles (HRRP) and inverse synthetic aperture radar (ISAR) images, which indicate the prominent radar scatterers on the aircraft. The imagery is qualitatively examined, and its suitability for target classification is discussed.<>
ER  - 


TY  - CONF
TI  - Real-Time Image Geometric Rectification for Scene Matching Based on Aircraft Attitude
T2  - 2008 International Symposium on Computer Science and Computational Technology
SP  - 805
EP  - 808
AU  - X. Yang
AU  - F. Meng
AU  - L. Hu
AU  - J. Li
PY  - 2008
KW  - Layout
KW  - Aircraft navigation
KW  - Image analysis
KW  - Cameras
KW  - Solid modeling
KW  - Real time systems
KW  - Mathematics
KW  - Mathematical model
KW  - Computer science
KW  - Cities and towns
KW  - scene matching
KW  - real-time image
KW  - mathematic model
KW  - geometric rectification
DO  - 10.1109/ISCSCT.2008.350
JO  - 2008 International Symposium on Computer Science and Computational Technology
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2008 International Symposium on Computer Science and Computational Technology
Y1  - 20-22 Dec. 2008
AB  - A novel approach for scene matching real-time image geometric rectification is proposed. Based on the analysis of the reason caused geometric distortion in the process of real-time image capturing, two coordinates system are given, the real-time image distortion classification are analyzed, and the distortion mathematic models are established. Then, the geometric rectification of the real-time image is realized according to the aircraft camera¿s height and attitude angle information, including method for calculating the offset and determining the size of the new real-time image. Simulation experimental results proved the validity and the efficiency of the approach.
ER  - 


TY  - CONF
TI  - Ice Classification Algorithm Development and Verification for the Alaska Sar Facility Using Aircraft Imagery
T2  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
SP  - 751
EP  - 754
AU  - B. Holt
AU  - R. Kwok
AU  - E. Rignot
PY  - 1989
KW  - Ice
KW  - Classification algorithms
KW  - Image resolution
KW  - Aircraft propulsion
KW  - Oceans
KW  - Radiometry
KW  - Signal to noise ratio
KW  - NASA
KW  - Process design
KW  - Arctic
DO  - 10.1109/IGARSS.1989.578987
JO  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
Y1  - 10-14 July 1989
AB  - 
ER  - 


TY  - CONF
TI  - A system for aircraft recognition in perspective aerial images
T2  - Proceedings of 1994 IEEE Workshop on Applications of Computer Vision
SP  - 168
EP  - 175
AU  - S. Das
AU  - B. Bhanu
AU  - Xing Wu
AU  - R. N. Braithwaite
PY  - 1994
KW  - Image recognition
KW  - Feature extraction
KW  - Aircraft propulsion
KW  - Pattern recognition
KW  - Degradation
KW  - Image databases
KW  - Spatial databases
KW  - Object recognition
KW  - Shape
KW  - Educational institutions
DO  - 10.1109/ACV.1994.341305
JO  - Proceedings of 1994 IEEE Workshop on Applications of Computer Vision
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of 1994 IEEE Workshop on Applications of Computer Vision
Y1  - 5-7 Dec. 1994
AB  - Recognition of aircraft in complex, perspective aerial imagery has to be accomplished in presence of clutter, occlusion, shadow, and various forms of image degradation. This paper presents a system for aircraft recognition under real-world conditions that is based on the use of a hierarchical database of object models. The particular approach involves three key processes: (a) The qualitative object recognition process performs model-based symbolic feature extraction and generic object recognition; (b) The refocused matching and evaluation process refines the extracted features for more specific classification with input from (a); and (c) The primitive feature extraction process regulates the extracted features based on their saliency and interacts with (a) and (b). Experimental results showing the qualitative recognition of aircraft in perspective, aerial images are presented.<>
ER  - 


TY  - CONF
TI  - ELM-based hyperspectral imagery processor for onboard real-time classification
T2  - 2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)
SP  - 43
EP  - 50
AU  - K. Basterretxea
AU  - U. Martinez-Corral
AU  - R. Finker
AU  - I. del Campo
PY  - 2016
KW  - Hyperspectral imaging
KW  - Training
KW  - Image classification
KW  - Field programmable gate arrays
KW  - Real-time systems
KW  - Neurons
KW  - Hyperspectral imaging
KW  - real-time systems
KW  - classification algorithms
KW  - learning systems
KW  - field programmable gate arrays
DO  - 10.1109/DASIP.2016.7853795
JO  - 2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)
Y1  - 12-14 Oct. 2016
AB  - Hyperspectral imagery is being widely used for accurate object detection and terrain feature classification. Modern imaging spectrometers produce huge amounts of data that are compressed onboard and downloaded to ground stations to be processed. Increasing spectral resolution and data acquisition rates demand more efficient compression techniques to meet downlink bandwidth restrictions. A different approach to reducing data-transfer bottlenecks consists of processing hyperspectral imagery information onboard. Real-time onboard processing would, at the same time, broaden the scope of missions that spacecrafts and aircrafts carrying hyperspectral cameras could fulfill by providing them with immediate decision-making capacity in critical circumstances. This paper investigates the use of Extreme Learning Machines (ELMs) for the classification of high dimensional data, and how specialized hardware and application-specific processor design can help to produce high performance, lightweight, and reduced power consumption systems for onboard hyperspectral imagery processing.
ER  - 


TY  - CONF
TI  - Random Part Localization Model for Fine Grained Image Classification
T2  - 2019 IEEE International Conference on Image Processing (ICIP)
SP  - 420
EP  - 424
AU  - Q. Xin
AU  - T. Lv
AU  - H. Gao
PY  - 2019
KW  - Feature extraction
KW  - Automobiles
KW  - Convolutional neural networks
KW  - Encoding
KW  - Aircraft
KW  - Image recognition
KW  - Semantics
KW  - fine-grained
KW  - convolutional neural network
KW  - random part localization
DO  - 10.1109/ICIP.2019.8802935
JO  - 2019 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Image Processing (ICIP)
Y1  - 22-25 Sept. 2019
AB  - Fine-grained recognition is challenging due to its subtle local inter-class differences versus large intra-class variations. Finding those subtle traits that fully characterize the object is not straightforward. In this paper, we present a novel random part localization model, which first extracts the foreground object using the saliency map, and then localizes the discriminative parts through a set of potential regions in a random way based on their contribution to classification. We train three convolutional neural networks to capture the features that belong to different levels and average their classification results as our final prediction score. Experiments show that our approach achieves competitive performance compared with state-of-the-art methods on three publicly available fine-grained recognition datasets (CUB200-2011, Stanford Cars and FGVC-Aircraft).
ER  - 


TY  - CONF
TI  - Geological Segmentation on UAV Aerial Image Using Shape-Based LSM with Dominant Color
T2  - 2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)
SP  - 928
EP  - 933
AU  - C. -L. Huang
AU  - J. -J. Chen
AU  - C. -J. Chen
AU  - Y. -G. Wu
PY  - 2016
KW  - Image color analysis
KW  - Image segmentation
KW  - Geology
KW  - Shape
KW  - Level set
KW  - Rivers
KW  - Aircraft
KW  - geological segmentation
KW  - aerial image
KW  - UAV
KW  - dominant color
KW  - LSM
DO  - 10.1109/WAINA.2016.82
JO  - 2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)
Y1  - 23-25 March 2016
AB  - This paper proposed a geological segmentation algorithm to segment different geology, such as rivers, woodlands, and soil, etc. according on differences image feature of geology for aerial photography. In this paper, aerial image capture from low-flying quadcopter unmanned aircraft vehicle (UAV). Using Fuzzy-C-Mean method for aerial image color classification, and estimate some dominant colors of aerial image. And then estimates each dominant color similar distribution map. Finally, shape-based level set method (LSM) is employed to segment the distribution of geological that the dominant color represents. The experimental results show that the proposed algorithm is efficacious and reliable in geological segmentation.
ER  - 


TY  - CONF
TI  - Image segmentation for automated taxiing of Unmanned Aircraft
T2  - 2015 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1
EP  - 8
AU  - W. Eaton
AU  - W. -H. Chen
PY  - 2015
KW  - Image segmentation
KW  - Image color analysis
KW  - Unmanned aerial vehicles
KW  - Semantics
KW  - Aircraft
KW  - Image edge detection
KW  - Cameras
DO  - 10.1109/ICUAS.2015.7152268
JO  - 2015 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 9-12 June 2015
AB  - This paper details a method of detecting collision risks for Unmanned Aircraft during taxiing. Using images captured from an on-board camera, semantic segmentation can be used to identify surface types and detect potential collisions. A review of classifier lead segmentation concludes that texture feature descriptors lack the pixel level accuracy required for collision avoidance. Instead, segmentation prior to classification is suggested as a better method for accurate region border extraction. This is achieved through an initial over-segmentation using the established SLIC superpixel technique with further untrained clustering using DBSCAN algorithm. Known classes are used to train a classifier through construction of a texton dictionary and models of texton content typical to each class. The paper demonstrates the application of said system to real world images, and shows good automated segment identification. Remaining issues are identified and contextual information is suggested as a method of resolving them going forward.
ER  - 


TY  - JOUR
TI  - Generative adversarial network-based electromagnetic signal classification: A semi-supervised learning framework
T2  - China Communications
SP  - 157
EP  - 169
AU  - H. Zhou
AU  - L. Jiao
AU  - S. Zheng
AU  - L. Yang
AU  - W. Shen
AU  - X. Yang
PY  - 2020
KW  - Gallium nitride
KW  - Electromagnetics
KW  - Generators
KW  - Generative adversarial networks
KW  - Deep learning
KW  - Training
KW  - Pattern classification
KW  - generative adversarial network
KW  - semi-supervised learning
KW  - electromagnetic signal classification
KW  - end-to-end classification
KW  - weighted loss function
DO  - 10.23919/JCC.2020.10.011
JO  - China Communications
IS  - 10
SN  - 1673-5447
VO  - 17
VL  - 17
JA  - China Communications
Y1  - Oct. 2020
AB  - Generative adversarial network (GAN) has achieved great success in many fields such as computer vision, speech processing, and natural language processing, because of its powerful capabilities for generating realistic samples. In this paper, we introduce GAN into the field of electromagnetic signal classification (ESC). ESC plays an important role in both military and civilian domains. However, in many specific scenarios, we can't obtain enough labeled data, which cause failure of deep learning methods because they are easy to fall into over-fitting. Fortunately, semi-supervised learning (SSL) can leverage the large amount of unlabeled data to enhance the classification performance of classifiers, especially in scenarios with limited amount of labeled data. We present an SSL framework by incorporating GAN, which can directly process the raw in-phase and quadrature (IQ) signal data. According to the characteristics of the electromagnetic signal, we propose a weighted loss function, leading to an effective classifier to realize the end-to-end classification of the electromagnetic signal. We validate the proposed method on both public RML2016.04c dataset and real-world Aircraft Communications Addressing and Reporting System (ACARS) signal dataset. Extensive experimental results show that the proposed framework obtains a significant increase in classification accuracy compared with the state-of-the-art studies.
ER  - 


TY  - CONF
TI  - Multiclass 3-D aircraft identification and orientation estimation using multilayer feedforward neural network
T2  - [Proceedings] 1991 IEEE International Joint Conference on Neural Networks
SP  - 758
EP  - 764 vol.1
AU  - Dae-Young Kim
AU  - Sung-Il Chien
AU  - Hyun Son
PY  - 1991
KW  - Nonhomogeneous media
KW  - Neural networks
KW  - Multi-layer neural network
KW  - Testing
KW  - Aircraft manufacture
KW  - Aerospace electronics
KW  - Distortion measurement
KW  - Military aircraft
KW  - Performance evaluation
KW  - Azimuth
DO  - 10.1109/IJCNN.1991.170491
JO  - [Proceedings] 1991 IEEE International Joint Conference on Neural Networks
IS  - 
SN  - 
VO  - 
VL  - 
JA  - [Proceedings] 1991 IEEE International Joint Conference on Neural Networks
Y1  - 18-21 Nov. 1991
AB  - Multilayer neural networks using the modified backpropagation learning algorithm are applied to achieve identification and orientation estimation of different classes of aircraft in a variety of 3-D orientations. 2-D distortion-invariant (L, Phi ) feature space was introduced for describing an aircraft image and used as the input of the neural network classifier. The optimum structure of the neural network was studied to obtain a high-performance classifier, and the reliability measure of the designed neural network classifier is introduced.<>
ER  - 


TY  - CONF
TI  - An Novel Interpretable Fine-grained Image Classification Model Based on Improved Neural Prototype Tree
T2  - 2023 IEEE International Symposium on Circuits and Systems (ISCAS)
SP  - 1
EP  - 5
AU  - J. Cui
AU  - J. Gong
AU  - G. Wang
AU  - J. Li
AU  - X. Liu
AU  - S. Liu
PY  - 2023
KW  - Deep learning
KW  - Computer vision
KW  - Computational modeling
KW  - Prototypes
KW  - Feature extraction
KW  - Decision trees
KW  - Integrated circuit modeling
KW  - Fine-grained Image Classification
KW  - Interpretable Models
KW  - Multi-grained Feature Extraction Network
KW  - Neural De-cision Tree
DO  - 10.1109/ISCAS46773.2023.10181728
JO  - 2023 IEEE International Symposium on Circuits and Systems (ISCAS)
IS  - 
SN  - 2158-1525
VO  - 
VL  - 
JA  - 2023 IEEE International Symposium on Circuits and Systems (ISCAS)
Y1  - 21-25 May 2023
AB  - The fine-grained image classification task is a major task in computer vision. Although many deep learning inter-pretable models have been proposed for this task, the accuracy and interpretability of these models need to be improved. We propose an interpretable fine-grained image classification model based on an improved neural prototype tree. In our model, we design the new multi-grained feature extraction network with three new backbone networks to extract features of fine-grained and multi-grained images more effectively. Furthermore, we design a new background prototype removing mechanism in the soft neural binary decision tree layer to optimize prototype path decision. Afterwards, we design a new loss function with both a leaf node loss function and a fully connected layer loss function to improve the generalization ability. Finally, we evaluate our model on three public datasets CUB-200-2011, FGVC-Aircraft, and Chest X-ray to compare with other baseline models.
ER  - 


TY  - CONF
TI  - Hyperspectral image classification using Support Vector Neural Network algorithm
T2  - 2015 7th International Conference on Recent Advances in Space Technologies (RAST)
SP  - 239
EP  - 243
AU  - G. Lokman
AU  - G. Yılmaz
PY  - 2015
KW  - Training
KW  - Hyperspectral imaging
KW  - Artificial neural networks
KW  - Support vector machines
KW  - Classification algorithms
KW  - Training data
KW  - Hyperspectral images
KW  - Target detection
KW  - Support Vector Neural Networks
DO  - 10.1109/RAST.2015.7208348
JO  - 2015 7th International Conference on Recent Advances in Space Technologies (RAST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 7th International Conference on Recent Advances in Space Technologies (RAST)
Y1  - 16-19 June 2015
AB  - With the developing technology, Hyperspectral images can be obtained with the satellites, aircraft and even unmanned aerial vehicles. Therefore, the classification applications made on the HSI are becoming increasingly important. In particular, fast and reliable classification algorithms are needed. The basic principle in classification algorithms is using characteristics of the data to find classification function that separate the data from each other. Neural Networks are among the non-linear classification method that can perform with high success. But, syntactic classifier has some problems that occur during training. One of this problems is called over-fitting. In many cases, especially in hyperspectral images, regularization is required for preventing the learning algorithm from over fitting the training data. In this study, a regularization scheme that named eigenvalue decay is used to make to this regularization in the training phase of networks. A training method that uses such a regularization scheme provides a margin maximization as in SVM for NNs. The two well-known data sets that are AVIRIS image of the Salinas Valley in California and image of Okavango Delta in Botswana acquired by The Hyperion sensor on NASA EO-1 satellite are used to test this classifier. The effectiveness of this algorithm on the HSI is evaluated using a series of experiments.
ER  - 


TY  - CONF
TI  - Aircraft recognition based on convex-concave analysis
T2  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
SP  - 1391
EP  - 1395
AU  - X. Chao
AU  - Y. Li
AU  - K. Zhang
PY  - 2012
KW  - Shape
KW  - Aircraft
KW  - Image recognition
KW  - Histograms
KW  - Bayesian methods
KW  - Pattern analysis
KW  - Target recognition
KW  - Mathematical morphological algorithm
KW  - Radon transform
KW  - Pose estimation
DO  - 10.1109/FSKD.2012.6233975
JO  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
Y1  - 29-31 May 2012
AB  - The problem of aircraft recognition in a single image is analyzed. A novel method combined convex-concave feature and hierarchical analysis is introduced to do pattern recognition for aircraft using the feature of shapes and regions. Silhouettes obtained by image segmentation are described by chain code. Then convex-concave feature is computed from chaincode. Aircraft features are represented hierarchically with a sequence of convex-concave curves. Fast matching is performed with fundamental convex-concave feature, after which useful information (such as pose of the aircraft) can be obtained. The refined convex-concave feature is used to get detailed classification with additional information. Experimental results show the effectiveness of the algorithm for aircraft pose estimation in a gray level image.
ER  - 


TY  - CONF
TI  - Region-based convolutional neural networks for object detection in very high resolution remote sensing images
T2  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
SP  - 548
EP  - 554
AU  - Y. Cao
AU  - X. Niu
AU  - Y. Dou
PY  - 2016
KW  - Proposals
KW  - Aircraft
KW  - Remote sensing
KW  - Object detection
KW  - Feature extraction
KW  - Image edge detection
KW  - Airports
KW  - aircraft detection
KW  - deep learning
KW  - convolutional neural network
KW  - feature learning
DO  - 10.1109/FSKD.2016.7603232
JO  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)
Y1  - 13-15 Aug. 2016
AB  - Recently, the automatic object detection in high-resolution remote sensing images has become the key point in the application of remote sensing technology. The traditional methods, such as bag-of-visual-words (BOVW), could perform well in simple scenes, but when it used in complex scenes, the performance drops quickly. This paper we first try to use the current hot deep learning technology: Region-based convolutional neural networks (R-CNN), to detect aircrafts under the complex environments in high-resolution remote sensing images. This method has been proved to be very efficiency when using in object detection in natural images. Here, we tried to introduce this method into the field of the remote sensing. During our experiments, we also compared the impact of different proposal generate methods on the final detection results. And we also proposed some practical tips to accelerate the detection speed. After detection, we proposed to use a novel algorithm which we called box-fusion, to eliminate the redundant and repetitive boxes that covering the same object. As experiments and results shows, the R-CNN method is much more effective and robust than the traditional BOVW method when dealing with aircrafts detection under complex scenes in high-resolution remote sensing images.
ER  - 


TY  - CONF
TI  - Label Relationship Graph-Enhanced Class Hierarchy for Incremental Classification of Remote Sensing Images
T2  - ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
SP  - 1
EP  - 5
AU  - Y. Chu
AU  - Y. Qian
PY  - 2025
KW  - Training
KW  - Incremental learning
KW  - Accuracy
KW  - Computational modeling
KW  - Surveillance
KW  - Self-supervised learning
KW  - Signal processing
KW  - Speech processing
KW  - Remote sensing
KW  - Testing
KW  - incremental learning
KW  - learning without forgetting
KW  - hierarchical classification
KW  - feature pyramid networks
KW  - label relation graph
DO  - 10.1109/ICASSP49660.2025.10889458
JO  - ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
IS  - 
SN  - 2379-190X
VO  - 
VL  - 
JA  - ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
Y1  - 6-11 April 2025
AB  - Incremental learning is a strategy that continuously incorporates new data to tackle emerging tasks without the need for retraining the model. While effective, it encounters the challenge of catastrophic forgetting. Hierarchical Classification (HC) enhances classification accuracy and efficiency by assigning objects to multiple labels within a hierarchical structure. This paper introduces LRGIC, a novel approach specifically designed for incremental hierarchical classification of remote sensing images. LRGIC combines class hierarchy (CH), a Feature Pyramid Network (FPN), and a Learning Without Forgetting (LWF) strategy, while also incorporating a HEX graph to constrain labels and encode hierarchical knowledge. These elements, integrated within a hierarchical residual network, significantly boost classification performance. The FPN captures multi-scale features, and the LWF strategy facilitates the learning of new categories without reusing old samples. Experimental results demonstrate that LRGIC effectively classifies new categories and their hierarchical relationships, while preserving the performance of existing categories, underscoring its substantial research and practical value.
ER  - 


TY  - CONF
TI  - Change Detection using Relative Radiometric Correction on Air Field Satellite Imagery
T2  - 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
SP  - 753
EP  - 757
AU  - P. Dayal
AU  - P. Goel
AU  - C. Gupta
AU  - T. K. Patra
PY  - 2020
KW  - Histograms
KW  - Visualization
KW  - Satellite broadcasting
KW  - Aircraft
KW  - Radiometry
KW  - Satellites
KW  - Image color analysis
KW  - Change Detection
KW  - Image Differencing
KW  - Image Ratioing
KW  - Radiometric Correction
KW  - Visual Interpretation
DO  - 10.1109/ICRITO48877.2020.9197883
JO  - 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
Y1  - 4-5 June 2020
AB  - This paper focusses on improving the results of change detection between two satellite images of the same place taken at different times. Some of the primary challenges faced in change detection lies in overcoming the unregistered dataset, disturbances introduced due to air borne sensors and diverse atmospheric conditions such as lighting, cloud shadows and atmosphere haziness. To overcome the above challenges an additional step of relative radiometric correction has been added, before performing change detection. The results derived have been quantified in the form of percentage of change in image. Finally, the changes have been visually interpreted, for analysing the correctness and accuracy of results.
ER  - 


TY  - CONF
TI  - Automatic Recognition of Ship Types from Infrared Images Using Support Vector Machines
T2  - 2008 International Conference on Computer Science and Software Engineering
SP  - 483
EP  - 486
AU  - H. Li
AU  - X. Wang
PY  - 2008
KW  - Image recognition
KW  - Marine vehicles
KW  - Infrared imaging
KW  - Support vector machines
KW  - Image segmentation
KW  - Computer vision
KW  - Feature extraction
KW  - Solids
KW  - Support vector machine classification
KW  - Infrared image sensors
KW  - salient features
KW  - waterline
KW  - moment functions
DO  - 10.1109/CSSE.2008.1647
JO  - 2008 International Conference on Computer Science and Software Engineering
IS  - 
SN  - 
VO  - 6
VL  - 6
JA  - 2008 International Conference on Computer Science and Software Engineering
Y1  - 12-14 Dec. 2008
AB  - In this paper, we present a system addressing autonomous recognition of ship types in infrared images. Firstly, segmentation is implemented after the target region is automatically found based on detection of salient features of the target. Feature extraction is then accomplished as the moment functions for both the target boundary and the solid silhouette are used as the feature set. Lastly, the classification method based on support vector machines (SVMs) is adopted in the recognition stage, as the training sets are obtained through projections of three-dimensional ship models designed by investigators of Naval Postgraduate School. The system was implemented and experimentally validated using both simulated three-dimensional ship model images and real images derived from video of an AN/AAS-44V forward looking infrared(FLIR) sensor. Moreover, our proposed system is general and can be generalized for other similar pattern recognition applications.
ER  - 


TY  - CONF
TI  - Aircraft Detection in Satellite Images
T2  - 2022 International Conference on Futuristic Technologies (INCOFT)
SP  - 1
EP  - 5
AU  - A. Singh
AU  - M. J. Nene
PY  - 2022
KW  - Computer science
KW  - Satellites
KW  - Object detection
KW  - Aircraft
KW  - Object Detection
KW  - YOLO
KW  - TP
KW  - Deep Learning
KW  - Machine Learning
KW  - mAP
KW  - PR AUC
KW  - F1 AUC
DO  - 10.1109/INCOFT55651.2022.10094468
JO  - 2022 International Conference on Futuristic Technologies (INCOFT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Futuristic Technologies (INCOFT)
Y1  - 25-27 Nov. 2022
AB  - Object detection is one of the key areas for all the researchers in the field of computer science. The research is to find the types of objects in the image and provide their temporal and spatial characteristics. In the recent times there have been a lot of improvements in the field of satellite image detection with varying data sets being available and has left high impact on the performance of such analysis. A number of algorithms have evolved over a period of time in object detection and analysis namely different versions of YOLO, CNN, DETR etc. There is a need to deploy a study which enables the performance comparison of different versions of these algorithms on specific data set to understand their efficacy. The study in the paper contributes to understanding, evaluating and analyzing the performance characteristics of YOLO v7 algorithm with varying parameters.
ER  - 


TY  - CONF
TI  - Cross-Granularity Fusion Network for Fine-Grained Image Classification
T2  - 2023 4th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
SP  - 617
EP  - 622
AU  - W. Pang
AU  - W. Song
PY  - 2023
KW  - Seminars
KW  - Visualization
KW  - Image recognition
KW  - Fuses
KW  - Semantics
KW  - Excavation
KW  - Automobiles
KW  - Fine-grained image classification
KW  - coarse-to-fine
KW  - consistency selection loss
KW  - feature fusion
DO  - 10.1109/AINIT59027.2023.10212436
JO  - 2023 4th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 4th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
Y1  - 16-18 June 2023
AB  - Fine-grained image classification (FGIC) aims to identify subtle visual differences among subcategories, which is challenging due to the small inter-class variances. Existing methods recognize subcategories mainly by locating discriminative parts which exists in the regions with high responses in deep feature maps. However, the regions with high responses in deep feature maps correspond to large receptive fields in the input image, leading to the result that subtle visual differences among subcategories cannot be captured precisely. In this paper we propose a novel Cross-Granularity Fusion Network (CGFN), which excavates subtle yet discriminative granularity features within each part and captures potential interactions among granularity features to build powerful part feature representations. The CGFN consists of two modules: First, the Multi-Granularity Proposal (MGP) module locates diverse and discriminative parts and focuses context-complementary granularities across different hierarchies within each part. Second, a Cross-Granularity Fusion (CGF) module is developed by fusing granularity features to acquire robust part features for the final classification. We conduct a series of experiments on publicly available datasets i.e., CUB-200-2011, Stanford Cars and FGVC-Aircraft datasets and experimental results demonstrate that the CGFN achieves state-of-the-art performance.
ER  - 


TY  - CONF
TI  - Recognition and classification of military aircraft in images
T2  - 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)
SP  - 1
EP  - 4
AU  - V. Khavalko
AU  - N. Kalapun
AU  - B. Dokhnyak
PY  - 2023
KW  - Deep learning
KW  - Training
KW  - Image recognition
KW  - Satellites
KW  - Computational modeling
KW  - Surveillance
KW  - Neural networks
KW  - computer vision
KW  - convolutional neural network
KW  - object recognition in images
KW  - object classification
KW  - image analysis
DO  - 10.1109/CSIT61576.2023.10324210
JO  - 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)
IS  - 
SN  - 2766-3639
VO  - 
VL  - 
JA  - 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)
Y1  - 19-21 Oct. 2023
AB  - Various approaches to the recognition and classification of military equipment are explored, including traditional computer vision methods and state-of-the-art approaches based on deep learning and artificial intelligence. The challenges faced by researchers and developers in this field are also examined, along with the potential benefits and applications of automated recognition systems for military equipment. The findings suggest that the use of automated recognition systems can greatly enhance the speed and accuracy of identifying military equipment. By leveraging neural networks and classification algorithms, it becomes possible to automatically determine the type, class, and condition of military equipment with high accuracy.
ER  - 


TY  - CONF
TI  - Detection and Recognition of Manufacturing Defects of Rivet Joints by their Video Images Using Deep Neural Networks
T2  - 2019 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)
SP  - 1
EP  - 4
AU  - O. S. Amosov
AU  - S. G. Amosova
AU  - I. O. Iochkov
PY  - 2019
KW  - Neural networks
KW  - Head
KW  - Inspection
KW  - Aircraft
KW  - Aircraft manufacture
KW  - Training
KW  - Three-dimensional displays
KW  - riveted joint
KW  - defect
KW  - deep neural network
KW  - pattern recognition
KW  - classification
KW  - video image
DO  - 10.1109/FarEastCon.2019.8934095
JO  - 2019 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)
Y1  - 1-4 Oct. 2019
AB  - The issue of detecting and recognizing manufacturing defects in riveted joints using video content is presented. For the detection and classification of defects in riveted joints of aircraft, a computational method using deep neural networks has been developed. An illustrative example is given.
ER  - 


TY  - CONF
TI  - Nighttime Vehicle Classification based on Thermal Images
T2  - 2023 IEEE/ACIS 21st International Conference on Software Engineering Research, Management and Applications (SERA)
SP  - 229
EP  - 234
AU  - X. Qu
AU  - N. Huynh
AU  - R. Mullen
AU  - J. R. Rose
PY  - 2023
KW  - System performance
KW  - Roads
KW  - Thermal engineering
KW  - Transportation
KW  - Lighting
KW  - Data augmentation
KW  - Thermal management
KW  - Vehicle classification
KW  - Thermal images
KW  - Data augmentation
DO  - 10.1109/SERA57763.2023.10197792
JO  - 2023 IEEE/ACIS 21st International Conference on Software Engineering Research, Management and Applications (SERA)
IS  - 
SN  - 2770-8209
VO  - 
VL  - 
JA  - 2023 IEEE/ACIS 21st International Conference on Software Engineering Research, Management and Applications (SERA)
Y1  - 23-25 May 2023
AB  - Each Department of Transportation in the United States must provide to the Federal Highway Administration on annual basis the number and types of vehicles traveled on its state-maintained roads. These data are fed into the Highway Performance Monitoring System used to assess the nation’s highway system performance. Classifying vehicles (i.e., identifying their types, e.g., passenger cars, trucks, etc.) during nighttime is quite challenging due to limited lighting. This study designed and evaluated three Convolutional Neural Network (CNN) models to classify vehicles using their thermal images. These three models have architectures that differ in the number of layers and, in the case of the third model, the addition of an inception layer. Of these, the second model achieves the best performance, achieving mean accuracy scores of greater than 97% for each of the three vehicle classes and f1 scores of greater than 98%. We proposed two training-test methods based on data augmentation to avoid over-fitting and to improve performance. The experimental results demonstrated that a data augmentation training-test method improves model performance further with regard to both accuracy and f1-score.
ER  - 


TY  - CONF
TI  - Cross-Layer Feature based Multi-Granularity Visual Classification
T2  - 2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)
SP  - 1
EP  - 5
AU  - J. Chen
AU  - D. Chang
AU  - J. Xie
AU  - R. Du
AU  - Z. Ma
PY  - 2022
KW  - Visualization
KW  - Cross layer design
KW  - Codes
KW  - Visual communication
KW  - Image processing
KW  - Fish
KW  - Birds
KW  - multi granularity visual classification
KW  - feature pyramid structure
KW  - disentanglement
KW  - reinforcement
DO  - 10.1109/VCIP56404.2022.10008879
JO  - 2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)
IS  - 
SN  - 2642-9357
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Visual Communications and Image Processing (VCIP)
Y1  - 13-16 Dec. 2022
AB  - In contrast to traditional fine-grained visual clas-sification, multi-granularity visual classification is no longer limited to identifying the different sub-classes belonging to the same super-class (e.g., bird species, cars, and aircraft models). Instead, it gives a sequence of labels from coarse to fine (e.g., Passeriformes → Corvidae → Fish Crow), which is more convenient in practice. The key to solving this task is how to use the relationships between the different levels of labels to learn feature representations that contain different levels of granularity. Interestingly, the feature pyramid structure naturally implies different granularity of feature representation, with the shallow layers representing coarse-grained features and the deep layers representing fine-grained features. Therefore, in this paper, we exploit this property of the feature pyramid structure to decouple features and obtain feature representations corre-sponding to different granularities. Specifically, we use shallow features for coarse-grained classification and deep features for fine-grained classification. In addition, to enable fine-grained features to enhance the coarse-grained classification, we propose a feature reinforcement module based on the feature pyramid structure, where deep features are first upsampled and then combined with shallow features to make decisions. Experimental results on three widely used fine-grained image classification datasets such as CUB-200-2011, Stanford Cars, and FGVC-Aircraft validate the method's effectiveness. Code available at https://github.com/PRIS-CV/CGVC.
ER  - 


TY  - CONF
TI  - Automatic installation system of soft rubber caps based on multi-rivet aircraft structural parts
T2  - 2022 9th International Forum on Electrical Engineering and Automation (IFEEA)
SP  - 585
EP  - 589
AU  - Q. Zhang
AU  - J. Mu
AU  - B. Xu
AU  - C. Yu
AU  - X. Tong
AU  - W. Wang
PY  - 2022
KW  - Sealing materials
KW  - Visualization
KW  - Semiconductor lasers
KW  - Production
KW  - Fasteners
KW  - Rubber
KW  - Aircraft
KW  - line laser
KW  - robot
KW  - coordinate transformation
KW  - rivets or screws
KW  - soft rubber cap
KW  - micro-curved
DO  - 10.1109/IFEEA57288.2022.10037939
JO  - 2022 9th International Forum on Electrical Engineering and Automation (IFEEA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 9th International Forum on Electrical Engineering and Automation (IFEEA)
Y1  - 4-6 Nov. 2022
AB  - Many structural parts are connected and fixed with rivets or screws in aircraft assembly. Due to the requirements of sealing, anticorrosion or fastening, soft rubber caps coated with sealant need to be bonded. The current production mode is mostly manual operation, there are a lot of problems, such as heavy workload, high hollow rate of sealing cap, low production efficiency, not environmental-friendly and so on. In this paper, a multi-rivet automatic identification and rubber cap installation system of micro-curved structural parts based on robotic linear laser scanning is designed by cooperative-robot to carry a linear laser camera and a soft rubber cap pickup fixture. The system can realize the automatic scanning, identification, classification and positioning of rivets or screws, automatically pick up different types of rubber caps according to the classification, and link with the automatic glue injection system to complete the quantitative glue injection of rubber caps, and automatically complete the installation of rubber caps according to the positioning information.
ER  - 


TY  - CONF
TI  - Small aircraft target detection using cascade FP-CNN in remote sensing images
T2  - IET International Radar Conference (IET IRC 2020)
SP  - 609
EP  - 614
AU  - J. Yang
AU  - J. Zhi
AU  - Y. Zhang
AU  - J. Wu
AU  - Y. Zhou
AU  - B. Zuo
PY  - 2020
DO  - 10.1049/icp.2021.0797
JO  - IET International Radar Conference (IET IRC 2020)
IS  - 
SN  - 
VO  - 2020
VL  - 2020
JA  - IET International Radar Conference (IET IRC 2020)
Y1  - 4-6 Nov. 2020
AB  - Aiming at the problem that the remote sensing images of small-scale aircraft targets are poorly detected in deep learning networks, the paper proposes a method of remote sensing image aircraft target detection based on Cascade FP-CNN network. The Cascade FP-CNN network uses feature pyramids to achieve feature fusion of feature maps on different scales, which enhances the target features learned by the network, thereby obtaining high-resolution, high-receptive-field feature maps. In addition, referring to the method for determining the crossover threshold of the Cascade network to set the crossover threshold of the Cascade FP-CNN network, a detection algorithm suitable for small-scale aircraft targets is finally constructed, thereby realizing the accurate recognition of small aircraft targets in remote sensing images. The average accuracy of the algorithm on the UCAS-AOD dataset reaches 96.01%, which is about 2% higher than the existed Faster R-CNN and Cascade R-CNN methods, and the detection capability has been significantly improved.
ER  - 


TY  - CONF
TI  - Deep Learning for Recognizing Mobile Targets in Satellite Imagery
T2  - 2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
SP  - 1
EP  - 7
AU  - M. Pritt
PY  - 2018
KW  - Classification algorithms
KW  - Satellites
KW  - Aircraft
KW  - Target recognition
KW  - Measurement
KW  - Windows
KW  - Image recognition
KW  - ATR
KW  - target recognition
KW  - artificial intelligence
KW  - AI
KW  - deep learning
KW  - CNN
KW  - neural networks
KW  - machine learning
KW  - image understanding
KW  - recognition
KW  - classification
KW  - satellite imagery
DO  - 10.1109/AIPR.2018.8707415
JO  - 2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
IS  - 
SN  - 2332-5615
VO  - 
VL  - 
JA  - 2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
Y1  - 9-11 Oct. 2018
AB  - There is an increasing demand for software that automatically detects and classifies mobile targets such as airplanes, cars, and ships in satellite imagery. Applications of such automated target recognition (ATR) software include economic forecasting, traffic planning, maritime law enforcement, and disaster response. This paper describes the extension of a convolutional neural network (CNN) for classification to a sliding window algorithm for detection. It is evaluated on mobile targets of the xView dataset, on which it achieves detection and classification accuracies higher than 95%.
ER  - 


TY  - JOUR
TI  - FRORS: An Effective Fine-Grained Retrieval Framework for Optical Remote Sensing Images
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 7406
EP  - 7419
AU  - Y. -Q. Mao
AU  - Z. Jiang
AU  - Y. Liu
AU  - Y. Zhang
AU  - K. Qi
AU  - H. Bi
AU  - Y. He
PY  - 2025
KW  - Remote sensing
KW  - Feature extraction
KW  - Image retrieval
KW  - Optical imaging
KW  - Prototypes
KW  - Optical reflection
KW  - Aircraft
KW  - Vectors
KW  - Convolutional neural networks
KW  - Aircraft manufacture
KW  - Fine-grained
KW  - image retrieval
KW  - optical images
KW  - remote sensing
DO  - 10.1109/JSTARS.2025.3545828
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 18
VL  - 18
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2025
AB  - Fine-grained retrieval of remote sensing images is an image interpretation task that is still in its infancy. With the rapid development of convolutional neural networks (CNN) in the field of remote sensing, it has become possible for remote sensing image retrieval tasks to move toward more fine-grained classes. However, since current methods focus on how to construct similarity metrics between sample pairs, the model ignores the learning of fine-grained intraclass heterogeneity and interclass commonality features, which poses a huge challenge to fine-grained retrieval. To solve this problem, we propose a novel fine-grained retrieval framework of optical remote sensing (FRORS) images, which aims to improve fine-grained retrieval capabilities by constructing interaction and matching between intraclass heterogeneity features, interclass commonality features, and image features. Specifically, we first construct a fine-grained prototype memory (FPM) module, and continuously update the local prototype storage unit through a lightweight CNN to achieve a refined representation of fine-grained heterogeneity features. Furthermore, to learn interclass commonality, we propose a gram learning (GraL) strategy, which is achieved by learning the correlation between feature dimensions. On this basis, we introduce a gram-based metric match (GMM) mechanism, which fuses the prototype features representing intraclass heterogeneity and the gram vector representing interclass commonality through an embedding manner, thereby achieving the purpose of fully interactive matching between image features and fine-grained class features. With FPM, GraL, and GMM, our FRORS can better learn deep features representing fine-grained classes and promote the improvement of the network's fine-grained retrieval ability. Extensive experiments conducted on a self-constructed THUFG-OPT dataset prove that the proposed FRORS achieves state-of-the-art fine-grained retrieval performance, which is 5.75% higher than the baseline method on $\mathrm{mAP@10}$.
ER  - 


TY  - CONF
TI  - Generalized hough transform for object classification in the maritime domain
T2  - 2016 11th System of Systems Engineering Conference (SoSE)
SP  - 1
EP  - 6
AU  - P. Rerkngamsanga
AU  - M. Tummala
AU  - J. Scrofani
AU  - J. McEachen
PY  - 2016
KW  - Feature extraction
KW  - Shape
KW  - Image edge detection
KW  - Discrete cosine transforms
KW  - Noise reduction
KW  - Biological neural networks
KW  - generalized Hough transform
KW  - neural networks
KW  - image classification
KW  - maritime domain
KW  - feature selection
DO  - 10.1109/SYSOSE.2016.7542914
JO  - 2016 11th System of Systems Engineering Conference (SoSE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 11th System of Systems Engineering Conference (SoSE)
Y1  - 12-16 June 2016
AB  - A generalized Hough transform-based classification scheme for an object-of-interest in maritime-domain images is proposed in this paper. The scheme explores the use of Hough features and neural networks to classify large sets of image objects collected in the maritime domain environment. The object edge points are extracted and used to generate the generalized Hough coordinate tables. The Hough coordinates are in turn reformatted to form Hough features maps. The coordinates of dominant peaks called Hough features are extracted and fed into a feed-forward, back-propagation neural network for classification. In this research, the scheme is tested using perfect geometric shapes as well as maritime-domain images of ships, aircraft, and clouds, and the classification results obtained are reported.
ER  - 


TY  - JOUR
TI  - ARNet: Prior Knowledge Reasoning Network for Aircraft Detection in Remote-Sensing Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 14
AU  - Y. Qian
AU  - X. Pu
AU  - H. Jia
AU  - H. Wang
AU  - F. Xu
PY  - 2024
KW  - Aircraft
KW  - Cognition
KW  - Feature extraction
KW  - Visualization
KW  - Object detection
KW  - Target recognition
KW  - Knowledge engineering
KW  - Aircraft component
KW  - aircraft detection and fine-grained recognition
KW  - prior knowledge in knowledge graph (KG)
KW  - remote-sensing images (RSIs)
DO  - 10.1109/TGRS.2024.3359764
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 62
VL  - 62
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2024
AB  - Amidst the landscape of contemporary remote-sensing (RS) technology, the endeavor to detect and recognize aircraft within RS images (RSIs) assumes pivotal strategic and practical significance. The complex nature of fine-grained aircraft recognition is a result of the intricate interplay between aircraft and their background environments, alongside category imbalance, which collectively leads to the emergence of a long-tail distribution within the dataset. However, experts proficient in RSI interpretation can effectively address these challenges through the application of prior knowledge. This article introduces the aircraft reasoning network (ARNet), a framework tailored for aircraft detection and fine-grained recognition in RSIs, building upon prior knowledge employed in expert interpretation. Specifically, the knowledge reasoning module (KRM) introduces a knowledge graph (KG) that incorporates both common and expert knowledge into the end-to-end network. Additionally, the network encompasses a spatial context module (SCM) and an airport facility relationship module (AFRM). These components facilitate highly accurate detection and recognition of fine-grained aircraft in diverse environmental contexts by employing adaptive prior knowledge reasoning and optimizing target spatial location. Furthermore, an independent aircraft component discrimination module (ACDM) distinguishes aircraft based on their predominant component features, contributing to improved classification performance in both the few-shot and easily confused categories. Moreover, this article introduces the AR-RSI dataset, a compilation of RSIs capturing fine-grained aircraft targets from diverse locations. The effectiveness and superiority of ARNet are exemplified by AR-RSI, achieving a minimum of 3.7% higher mean average precision (mAP) than the mainstream aircraft detection framework.
ER  - 


TY  - JOUR
TI  - A Fine-Grained Image Classification Model Based on Hybrid Attention and Pyramidal Convolution
T2  - Tsinghua Science and Technology
SP  - 1283
EP  - 1293
AU  - S. Wang
AU  - S. Li
AU  - A. Li
AU  - Z. Dong
AU  - G. Li
AU  - C. Yan
PY  - 2025
KW  - Visualization
KW  - Attention mechanisms
KW  - Convolution
KW  - Computational modeling
KW  - Interference
KW  - Feature extraction
KW  - Data mining
KW  - Convolutional neural networks
KW  - Unsupervised learning
KW  - Image classification
KW  - fine-grained image classification
KW  - pyramidal convolution
KW  - hybrid attention
DO  - 10.26599/TST.2024.9010025
JO  - Tsinghua Science and Technology
IS  - 3
SN  - 1007-0214
VO  - 30
VL  - 30
JA  - Tsinghua Science and Technology
Y1  - June 2025
AB  - Finding more specific subcategories within a larger category is the goal of fine-grained image classification (FGIC), and the key is to find local discriminative regions of visual features. Most existing methods use traditional convolutional operations to achieve fine-grained image classification. However, traditional convolution cannot extract multi-scale features of an image and existing methods are susceptible to interference from image background information. Therefore, to address the above problems, this paper proposes an FGIC model (Attention-PCNN) based on hybrid attention mechanism and pyramidal convolution. The model feeds the multi-scale features extracted by the pyramidal convolutional neural network into two branches capturing global and local information respectively. In particular, a hybrid attention mechanism is added to the branch capturing global information in order to reduce the interference of image background information and make the model pay more attention to the target region with fine-grained features. In addition, the mutual-channel loss (MC-LOSS) is introduced in the local information branch to capture fine-grained features. We evaluated the model on three publicly available datasets CUB-200-2011, Stanford Cars, FGVC-Aircraft, etc. Compared to the state-of-the-art methods, the results show that Attention-PCNN performs better.
ER  - 


TY  - JOUR
TI  - Classification Matters More: Global Instance Contrast for Fine-Grained SAR Aircraft Detection
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 15
AU  - D. Zhao
AU  - Z. Chen
AU  - Y. Gao
AU  - Z. Shi
PY  - 2023
KW  - Aircraft
KW  - Feature extraction
KW  - Detectors
KW  - Task analysis
KW  - Location awareness
KW  - Synthetic aperture radar
KW  - Training
KW  - Edge-aware box refinement
KW  - fine-grained detection
KW  - global instance contrast (GIC)
KW  - quality-aware focal loss (QAFL)
KW  - synthetic aperture radar (SAR)
DO  - 10.1109/TGRS.2023.3250507
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 61
VL  - 61
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2023
AB  - Since significant intraclass differences and inconspicuous interclass variations, fine-grained aircraft detection in synthetic aperture radar (SAR) images is challenging. Also, the inherent lack of detailed features and severe noise interference in SAR images make it difficult to learn class-specific feature representations. Current detection approaches focus more on localization accuracy and ignore classification performance, which is more critical in fine-grained detection. To address the above challenges, we present GICNet: global instance contrast (GIC) for fine-grained SAR aircraft detection a global instance-level contrast module is proposed to improve interclass divergences and intraclass compactness. With a specially constructed global instance set, GICNet can contrast a large number of different aircraft targets while keeping a small batch size. Furthermore, we design a novel quality-aware focal loss (QAFL) to facilitate the accurate classification of well-localized aircraft targets. Meanwhile, to maintain localization performance, we develop a new edge-aware bounding-box refinement (EABR) module to refine predicted coarse bounding boxes. Experimental results show that our GICNet outperforms current advanced detectors and achieves a new state-of-the-art performance on the GaoFen-3 SAR aircraft detection dataset. In particular, GICNet also has advantages in reducing misclassification and recognizing well-located targets.
ER  - 


TY  - CONF
TI  - A Neural Network Development for Multispectral Images Recognition
T2  - 2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)
SP  - 278
EP  - 284
AU  - S. Tchynetskyi
AU  - R. Peleshchak
AU  - I. Peleshchak
AU  - V. Vysotska
PY  - 2021
KW  - Image recognition
KW  - Conferences
KW  - Computational modeling
KW  - Neural networks
KW  - Libraries
KW  - Aircraft manufacture
KW  - Information technology
KW  - Neural network
KW  - three-layer neural network
KW  - multispectral images
KW  - activator
KW  - optimiser
KW  - TensorFlow
KW  - Keras
KW  - Python neural network
KW  - machine learning
KW  - hierarchical classification
KW  - neural network model
DO  - 10.1109/CSIT52700.2021.9648735
JO  - 2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)
IS  - 
SN  - 2766-3639
VO  - 2
VL  - 2
JA  - 2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)
Y1  - 22-25 Sept. 2021
AB  - A three-layer neural network has been developed to recognise multispectral images using Python and the TensorFlow library. The constructed neural network will provide the class of the aircraft object according to the input data. Also, in this scientific article, the analytical construction of a neural network and a choice of the best activators and optimisers are considered. Finally, experiments with the developed neural network based on various input data are carried out.
ER  - 


TY  - CONF
TI  - Geospatial 2D and 3D object-based classification and 3D reconstruction of ISO-containers depicted in a LiDAR data set and aerial imagery of a harbor
T2  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
SP  - 4181
EP  - 4184
AU  - D. Tiede
AU  - S. d'Oleire-Oltmanns
AU  - A. Baraldi
PY  - 2015
KW  - Containers
KW  - Three-dimensional displays
KW  - Image color analysis
KW  - Laser radar
KW  - Image segmentation
KW  - Geospatial analysis
KW  - Data integration
KW  - Color naming
KW  - object-based image analysis (OBIA)
KW  - hybrid inference
KW  - inductive data learning
KW  - prior knowledge
DO  - 10.1109/IGARSS.2015.7326747
JO  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
Y1  - 26-31 July 2015
AB  - Within the 2015 IEEE GRSS Data Fusion Contest, an extremely high-resolution 3D LiDAR point cloud of a harbor test site must be “fused” with a 2D multi-spectral aerial image, featuring no radiometric calibration metadata file, of the same surface area. In this scenario we propose an innovative geospatial 2D and 3D object-based classification system, capable of counting instances of two populations of ISO-containers, whose standard dimensions are known a priori based on the ISO 668 - Series 1 freight containers documentation, detected in the 2D and 3D datasets at hand. The degree of novelty of the proposed classification system is twofold. First, it combines inductive (bottom-up, data-driven) and deductive (top-down, prior knowledge-based) inference mechanisms, where the latter initializes the former in a hybrid inference framework. Second, it is provided with feedback loops, which increase its robustness to changes in input data and augment its degree of automation. The geospatial outcome consists of tangible vector objects, which allow estimation of statistics per container together with a detailed reconstruction of the 3D scene in a geographic information system.
ER  - 


TY  - CONF
TI  - Research on image processing technology for online oil monitoring system
T2  - 2014 IEEE International Conference on Imaging Systems and Techniques (IST) Proceedings
SP  - 222
EP  - 225
AU  - M. Ma
AU  - L. Zhao
PY  - 2014
KW  - Monitoring
KW  - Image segmentation
KW  - Educational institutions
KW  - Image recognition
KW  - Image restoration
KW  - Wiener filters
KW  - Image color analysis
KW  - on-line monitoring
KW  - motion-blur
KW  - image restoration
KW  - image segmentation
KW  - image recognition
DO  - 10.1109/IST.2014.6958477
JO  - 2014 IEEE International Conference on Imaging Systems and Techniques (IST) Proceedings
IS  - 
SN  - 1558-2809
VO  - 
VL  - 
JA  - 2014 IEEE International Conference on Imaging Systems and Techniques (IST) Proceedings
Y1  - 14-17 Oct. 2014
AB  - Accurate wear debris classifying using on-line oil monitoring system plays an important role in aero-engine condition monitoring and fault diagnostics. However, there still exist imperfections in on-line oil monitoring system such as: motion blur of abrasive particles in the acquired images, low efficiency of the traditional image segmentation and recognition methods. Aiming to improve this situation, improved image restoration and segmentation algorithms are developed and different kinds of classifiers are designed in our work. Practical experimental results indicate that better recognition results of the abrasive particle images can be achieved via the improved methods, which offer an analysis basis for the fault diagnosis and detection of aircraft engines.
ER  - 


TY  - CONF
TI  - Remote Sensing object recognition based on transfer learning
T2  - 2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
SP  - 930
EP  - 934
AU  - Zhiping Dan
AU  - Nong Sang
AU  - Yanfei Chen
AU  - Xi Chen
PY  - 2013
KW  - Object recognition
KW  - Training data
KW  - Aircraft
KW  - Classification algorithms
KW  - Feature extraction
KW  - Accuracy
KW  - Learning systems
KW  - object recognition
KW  - remote sensing
KW  - image processing
KW  - machine learning
DO  - 10.1109/FSKD.2013.6816328
JO  - 2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
Y1  - 23-25 July 2013
AB  - The deviation of an object's real data distribution from the known training data distribution would lead to low reliability of object recognition. To tackle this problem for Remote Sensing (RS) images, we propose a novel object recognition method based on transfer learning. The feature vectors of an object are first extracted by a joint Local Binary Pattern (LBP). The transfer learning is then employed to find the common parameter set among feature spaces of the object under different distributions. Through extensive experiments, it has been shown that a significant improvement on the accuracy is has been brought by the proposed novel method.
ER  - 


TY  - CONF
TI  - A Deep Learning Approach for Drone Detection and Classification Using Radar and Camera Sensor Fusion
T2  - 2023 IEEE Sensors Applications Symposium (SAS)
SP  - 01
EP  - 06
AU  - V. Mehta
AU  - F. Dadboud
AU  - M. Bolic
AU  - I. Mantegh
PY  - 2023
KW  - Radar detection
KW  - Radar
KW  - Radar imaging
KW  - Feature extraction
KW  - Cameras
KW  - Radar tracking
KW  - Birds
KW  - UAS
KW  - Detection
KW  - Classification
KW  - Radar
KW  - PTZ
KW  - Fusion
DO  - 10.1109/SAS58821.2023.10254123
JO  - 2023 IEEE Sensors Applications Symposium (SAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE Sensors Applications Symposium (SAS)
Y1  - 18-20 July 2023
AB  - With the growth of Unmanned Aircraft Systems (UAS) technology and the increasing misuse of small UAS (sUAS), the importance of a reliable method for detecting and classifying aircraft from other flying objects has become apparent. The current approaches for detecting and classifying aircraft and other flying objects are primarily based on solutions that rely on a single sensor, either visual data features or micro-Doppler extraction from radar data. However, these methods may have limitations when it comes to detecting objects at greater distances or in challenging weather conditions. To address the problem, the paper proposes a joint classification network based on radar and camera fusion. The radar network extracts the Spatio temporal features from the radar track and the camera network extracts the deep, complex features from the image. A synchronized radar and camera data is established using multiple field trials during different times of the year. The radar classification using a combination of IMM filters and RNN, the camera detection and classification using YOLOv5, and the combined joint classification network are evaluated on the field dataset. The experimental results greatly increase the classification performance for drones and birds, respectively, to 98% and 94%. This is especially true in situations when a single sensor would struggle to offer reliable classification. The system can accurately classify drones while reducing false alarms caused by other objects, such as birds.
ER  - 


TY  - CONF
TI  - Optical and Polarimetric SAR Data Fusion Terrain Classification Using Probabilistic Feature Fusion
T2  - IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium
SP  - 2097
EP  - 2100
AU  - R. D. West
AU  - D. A. Yocky
AU  - B. J. Redman
AU  - J. D. Van Der Laan
AU  - D. Z. Anderson
PY  - 2020
KW  - Optical imaging
KW  - Optical sensors
KW  - Adaptive optics
KW  - Training data
KW  - Synthetic aperture radar
KW  - Optical scattering
KW  - Laser radar
KW  - Polarimetric SAR (PolSAR)
KW  - Optical
KW  - Data Fusion
KW  - Probabilistic Feature Fusion
KW  - Terrain Classification
DO  - 10.1109/IGARSS39084.2020.9324022
JO  - IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 26 Sept.-2 Oct. 2020
AB  - Deciding on an imaging modality for terrain classification can be a challenging problem. For some terrain classes a given sensing modality may discriminate well, but may not have the same performance on other classes that a different sensor may be able to easily separate. The most effective terrain classification will utilize the abilities of multiple sensing modalities. The challenge of utilizing multiple sensing modalities is then determining how to combine the information in a meaningful and useful way. In this paper, we introduce a framework for effectively combining data from optical and polarimetric synthetic aperture radar sensing modalities. We demonstrate the fusion framework for two vegetation classes and two ground classes and show that fusing data from both imaging modalities has the potential to improve terrain classification from either modality, alone.
ER  - 


TY  - JOUR
TI  - CMSEA: Compound Model Scaling With Efficient Attention for Fine-Grained Image Classification
T2  - IEEE Access
SP  - 18222
EP  - 18232
AU  - J. Guang
AU  - J. Liang
PY  - 2022
KW  - Image resolution
KW  - Compounds
KW  - Birds
KW  - Convolution
KW  - Complexity theory
KW  - Training
KW  - Annotations
KW  - Fine-grained image classification
KW  - EfficientNet
KW  - image recognition
KW  - channel attention
KW  - convolutional neural networks
DO  - 10.1109/ACCESS.2022.3150320
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - The purpose of fine-grained image classification is to distinguish subcategories belonging to the same basic-level category, for example, two hundred subcategories belonging to birds. It has been a challenging topic in the field of computer vision in recent years due to the small inter-class variance among different subcategories (e.g., color and texture) and the large intra-class variance in the same subcategory (e.g., pose and viewpoint). In this paper, we propose a Compound Model Scaling with Efficient Attention (CMSEA) for fine-grained image classification, which carefully balances the various dimensions of width, depth, and image resolution in model scaling. Furthermore, the proposed method utilizes an additional computational low attention module to efficiently learn subtler features from discriminative regions. In addition, regularization and data augmentation were employed to improve accuracy in the training. Extensive experiments demonstrate that CMSEA achieves 90.63%, 94.51%, and 95.19% accuracy on CUB-200-2011, FGVC-Aircraft, and Stanford Cars datasets, respectively. In particular, CMSEA on CUB-200-2011 obtains 2.3% higher accuracy with 18% fewer network parameters than the original approach. Consequently, our method has better accuracy and parameter efficiency compared to most existing methods.
ER  - 


TY  - CONF
TI  - Aircraft discrimination in high resolution SAR images based on texture analysis
T2  - 2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)
SP  - 118
EP  - 121
AU  - Liping Zhang
AU  - Chao Wang
AU  - Hong Zhang
AU  - Bo Zhang
PY  - 2010
KW  - Aircraft
KW  - Image resolution
KW  - Image analysis
KW  - Image texture analysis
KW  - Backscatter
KW  - Synthetic aperture radar
KW  - Geoscience
KW  - Robotics and automation
KW  - Object detection
KW  - Pixel
KW  - synthetic aperture radar
KW  - target discrimination
KW  - texture feature
KW  - J-M Distance
DO  - 10.1109/CAR.2010.5456757
JO  - 2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)
IS  - 
SN  - 1948-3422
VO  - 2
VL  - 2
JA  - 2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)
Y1  - 6-7 March 2010
AB  - Target discrimination is the key step of automatic target detection in synthetic aperture radar (SAR) images. Aiming at the issue of aircraft discrimination in high resolution SAR images, a novel discrimination method is proposed with using texture features. First of all the method of gray level co-occurrence matrix is used to generate eight discrimination texture features: mean, variance, deficit moment, inertia moment, entropy, angular second moment, relevance and non-similarity and then forming a feature vector. Differing with the common method of extracting the holistic texture features of image to represent the target, the texture features of each pixel are extracted and the feature vectors of all pixels are used to represent the target. Then J-M distance is used to measure the different targets, and supervised training method is applied to achieve the parameters of discrimination rule. Finally, suspected targets are discriminated to different classes by the trained discrimination rule and large numbers of false alarms are eliminated efficiently. The experiments show that the aircraft has small distance to other aircrafts while large difference to false alarms, so this discrimination method has high accuracy with excellent applicability.
ER  - 


TY  - CONF
TI  - A Dataset for Autonomous Aircraft Refueling on the Ground (AGR)
T2  - 2023 28th International Conference on Automation and Computing (ICAC)
SP  - 1
EP  - 6
AU  - B. Kuang
AU  - S. Barnes
AU  - G. Tang
AU  - K. Jenkins
PY  - 2023
KW  - Visualization
KW  - Automation
KW  - Annotations
KW  - Pipelines
KW  - Lighting
KW  - Software
KW  - Safety
KW  - image crawling
KW  - image augmentation
KW  - benchmark
KW  - classification
KW  - scene detection
DO  - 10.1109/ICAC57885.2023.10275212
JO  - 2023 28th International Conference on Automation and Computing (ICAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 28th International Conference on Automation and Computing (ICAC)
Y1  - 30 Aug.-1 Sept. 2023
AB  - Automatic aircraft ground refueling (AAGR) can improve the safety, efficiency, and cost-effectiveness of aircraft ground refueling (AGR), a critical and frequent operation on almost all aircraft. Recent AAGR relies on machine vision, artificial intelligence, and robotics to implement automation. An essential step for automation is AGR scene recognition, which can support further component detection, tracking, process monitoring, and environmental awareness. As in many practical and commercial applications, aircraft refueling data is usually confidential, and no standardized workflow or definition is available. These are the prerequisites and critical challenges to deploying and benefitting advanced data-driven AGR. This study presents a dataset (the AGR Dataset) for AGR scene recognition using image crawling, augmentation, and classification, which has been made available to the community. The AGR dataset crawled over 3k images from 13 databases (over 26k images after augmentation), and different aircraft, illumination, and environmental conditions were included. The ground-truth labeling is conducted manually using a proposed tree-formed decision workflow and six specific AGR tags. Various professionals have independently reviewed the AGR dataset to keep it no-bias. This study proposes the first aircraft refueling image dataset, and an image labeling software with a UI to automate the labeling workflow.
ER  - 


TY  - CONF
TI  - UAV-Assisted Logo Inspection: Deep Learning Techniques for Real- Time Detection and Classification of Distorted Logos
T2  - 2024 8th International Conference on Robotics, Control and Automation (ICRCA)
SP  - 428
EP  - 432
AU  - M. Mohiuddin
AU  - O. A. Hay
AU  - A. Abubakar
AU  - M. Yakubu
AU  - N. Werghi
PY  - 2024
KW  - Deep learning
KW  - Robot vision systems
KW  - Inspection
KW  - Military aircraft
KW  - Distortion
KW  - Prediction algorithms
KW  - Cameras
KW  - Video object detection
KW  - object classification
KW  - automatic logo inspection
KW  - drone-acquired imagery
KW  - distortion assessment
KW  - text orientation
KW  - logo overlap
KW  - placement constraints
DO  - 10.1109/ICRCA60878.2024.10649231
JO  - 2024 8th International Conference on Robotics, Control and Automation (ICRCA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 8th International Conference on Robotics, Control and Automation (ICRCA)
Y1  - 12-14 Jan. 2024
AB  - Ensuring the integrity of safety logos on aircraft is crucial for aviation personnel and overall safety. Presently, human operators perform inspections, which are susceptible to human errors. To address this, we propose an autonomous approach using drone-acquired photographic imagery for detecting and inspecting safety logos on fighter aircraft. Our methodology involves multiple stages: logo detection, distortion assessment, text orientation computation, and checking for logo overlap. We also calculate placement constraints for accurate logo positioning. We rigorously tested our approach on a local dataset, achieving an impressive precision of 92.3 % and a recall of 91.1 % for logo detection. We estimated computed text orientation in degrees and determined the distance between logos in pixels. This research presents a significant advancement in automatic logo inspection for aircraft safety. By leveraging drones and comprehensive detection techniques, our approach reduces human errors and enhances inspection efficiency. The potential impact includes improved safety standards in aviation and the foundation for future advancements in autonomous inspection systems.
ER  - 


TY  - JOUR
TI  - Hierarchical Multilabel Ship Classification in Remote Sensing Images Using Label Relation Graphs
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 13
AU  - J. Chen
AU  - Y. Qian
PY  - 2022
KW  - Marine vehicles
KW  - Remote sensing
KW  - Feature extraction
KW  - Probabilistic logic
KW  - Semantics
KW  - Taxonomy
KW  - Task analysis
KW  - Deep learning
KW  - hierarchical multilabel classification (HMC)
KW  - label relation graph
KW  - ship classification
DO  - 10.1109/TGRS.2021.3111117
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - Hierarchical multilabel classification (HMC) assigns multiple labels to each instance with the labels organized under hierarchical relations. In ship classification in remote sensing images, depending on the expert knowledge and image quality, the same type of ships in different remote sensing images may be annotated with different class labels from coarse to fine levels such as merchant ship (MS) or container ship (CTS). In this article, we propose a novel deep network with two output channels and their associated loss functions to learn an HMC classifier using samples labeled at different levels in the hierarchy. In the proposed network, a hierarchy and exclusion (HEX) graph is introduced to model the label hierarchy, which satisfies hierarchical constraints by encoding semantic relations between any two labels. The output nodes of the first channel are organized according to the HEX graph, and its corresponding probabilistic classification loss is built to reflect the hierarchical structure of the HEX graph. On the other hand, the output nodes of the second channel only represent the finest grained (last level in the hierarchy) classes, and its multiclass cross-entropy loss is designed to enhance the discriminative power of the HMC classifier on the last level labels, which is also compatible with constraints in the HEX graph. The combination of these two losses from two output channels can effectively transfer the hierarchical information of ship taxonomy during network training. Experimental results on two commonly used ship datasets demonstrate that the proposed method outperforms the state-of-the-art HMC approaches, and is especially advantageous when trained with fewer fine-grained samples.
ER  - 


TY  - JOUR
TI  - LN-SCNet: A Lightweight Convolutional Neural Network for SAR Ship Classification
T2  - IEEE Access
SP  - 39394
EP  - 39404
AU  - S. Zhao
AU  - W. Li
AU  - F. Shen
AU  - M. You
PY  - 2025
KW  - Marine vehicles
KW  - Feature extraction
KW  - Radar polarimetry
KW  - Convolution
KW  - Scattering
KW  - Convolutional neural networks
KW  - Synthetic aperture radar
KW  - Accuracy
KW  - Speckle
KW  - Noise
KW  - Synthetic aperture radar
KW  - ship classification
KW  - lightweight convolutional neural network
KW  - high-resolution
KW  - feature extraction
DO  - 10.1109/ACCESS.2025.3546764
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - Ship classification is a critical challenge in the domain of synthetic aperture radar (SAR) automatic target recognition, primarily due to the large number of pixels occupied by ships in high-resolution SAR images. This results in substantial input sizes, which lead to increased computational demands and a large number of parameters in the classification network. In this paper, a new SAR ship classification method based on a lightweight convolutional neural network (CNN) is proposed. The method aims to achieve competitive accuracy and efficiency in classifying ships within SAR images. To evaluate the effectiveness of the proposed method, extensive experiments were conducted on the publicly available FUSAR-Ship dataset. The experimental results demonstrate that the proposed method achieves a classification accuracy of 76.12%and an F1 score of 56.67%. Furthermore, comparisons with other state-of-the-art lightweight CNN methods indicate that the proposed method outperforms existing methods in terms of classification performance.
ER  - 


TY  - CONF
TI  - Development of Image Processing Techniques in Crack Detection and Analysis
T2  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
SP  - 1
EP  - 7
AU  - L. S. Al Dhafari
AU  - A. Afzal
AU  - R. I. Al Bahrani
AU  - Y. Al Busaidi
AU  - A. Sheikh-Akbari
AU  - M. S. Hossain
PY  - 2023
KW  - Visualization
KW  - Ultrasonic imaging
KW  - Image processing
KW  - Digital images
KW  - Inspection
KW  - Autonomous aerial vehicles
KW  - Skin
KW  - Crack detection
KW  - unmanned aerial vehicle
KW  - thermal imaging
KW  - aircraft outer body inspection
DO  - 10.1109/ICECCME57830.2023.10252690
JO  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
Y1  - 19-21 July 2023
AB  - Inspection of aircraft skin is required as per the Corrosion Prevention and Control Program (CPCP) to ensure aircraft structural integrity. Human visual inspection is the most widely used technique in aircraft surface inspection, according to the CPCP. Scheduled inspections and regular maintenance of an aircraft through conventional methods constitute tedious and lengthy procedures. Often the visual inspections lead to subjective judgement and do not constitute repeatability. Many automated vision-based aircraft skin inspection systems have been designed to provide a safe, quick, and accurate visual assessment over the past years. This paper presents a section of research investigating defect detecting and accurately locating the outer body of an aircraft using an Unmanned Aerial Vehicle (UAV) to capture images and digital image processing techniques to locate possible cracks. The inspection system is used to initially detect locations of cracks (defects) on an aircraft’s outer skin and the detected crack is further investigated using thermal and ultrasound imaging methods. The scope of this paper includes a review of the design and development of a series of advanced dedicated image processing algorithms suitable for applying digital image processing on images captured from the outer surface of a typical aircraft fuselage.
ER  - 


TY  - CONF
TI  - Model-Based Aircraft Recognition
T2  - 2006 International Radar Symposium
SP  - 1
EP  - 4
AU  - R. J. Miller
AU  - D. J. Shephard
PY  - 2006
KW  - Backscatter
KW  - Aircraft manufacture
KW  - Radar measurements
KW  - Databases
KW  - Aircraft propulsion
KW  - Assembly
KW  - Radar theory
KW  - Inspection
KW  - Radar scattering
KW  - Radar imaging
DO  - 10.1109/IRS.2006.4338138
JO  - 2006 International Radar Symposium
IS  - 
SN  - 2155-5753
VO  - 
VL  - 
JA  - 2006 International Radar Symposium
Y1  - 24-26 May 2006
AB  - This paper gives a brief account of a way of recognising aircraft using radar range profiles which does not need large numbers of radar measurements of the aircraft of interest beforehand The method is based on constructing backscatter models of the aircraft by identifying regions of high backscatter using drawings, photographs and commercially available scale models of the aircraft. Methods and results are briefly discussed.
ER  - 


TY  - CONF
TI  - Classifying natural aerial scenery for autonomous aircraft emergency landing
T2  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1236
EP  - 1242
AU  - L. Mejias
PY  - 2014
KW  - Support vector machines
KW  - Image color analysis
KW  - Training
KW  - Aircraft
KW  - Testing
KW  - Accuracy
KW  - Kernel
DO  - 10.1109/ICUAS.2014.6842380
JO  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 27-30 May 2014
AB  - In this paper, we present an approach for image-based surface classification using multi-class Support Vector Machine (SVM). Classifying surfaces in aerial images is an important step towards an increased aircraft autonomy in emergency landing situations. We design a one-vs-all SVM classifier and conduct experiments on five data sets. Results demonstrate consistent overall performance figures over 88% and approximately 8% more accurate to those published on multi-class SVM on the KTH TIPS data set. We also show per-class performance values by using normalised confusion matrices. Our approach is designed to be executed online using a minimum set of feature attributes representing a feasible and ready-to-deploy system for onboard execution.
ER  - 


TY  - CONF
TI  - Determining the effect of spatial resolution in land use classification using optical aerial imagery
T2  - 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
SP  - 2272
EP  - 2275
AU  - H. Jiang
AU  - A. Smith
AU  - Z. Zhong
AU  - J. Li
PY  - 2016
KW  - Spatial resolution
KW  - Remote sensing
KW  - Urban areas
KW  - Optical imaging
KW  - Decision trees
KW  - Unmanned aerial vehicles
KW  - Land use
KW  - Optical aerial imagery
KW  - Parcel map
KW  - UAS
DO  - 10.1109/IGARSS.2016.7729587
JO  - 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
Y1  - 10-15 July 2016
AB  - Unmanned Aircraft System (UAS), a low cost and time efficient remote sensing platform, can help public agencies obtain useful urban aerial imagery of cities and update land use information frequently, especially in natural colour bands. In this paper, a decision tree based land use classification approach using optical aerial imagery is proposed. First, land cover information is extracted through the Maximum Likelihood Classifier and tabulated with an Ownership parcel map. Second, a decision tree is generated to establish the relationship between land cover and land use. Taking advantage of the geometric characteristics of parcels, an organized land use parcel map is produced. Afterwards, by resampling the aerial imagery from 20 cm, 50 cm and 100 cm resolution, effects of spatial resolution in this classification approach are discussed and determined. This land use classification method is flexible and can be widely used in urban planning and landscape monitoring.
ER  - 


TY  - JOUR
TI  - Pattern Recognition Experiments in the Mandala/Cosine Domain
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 512
EP  - 520
AU  - Y. S. Hsu
AU  - S. Prum
AU  - J. H. Kagel
AU  - H. C. Andrews
PY  - 1983
KW  - Pattern recognition
KW  - Target recognition
KW  - Image recognition
KW  - Image coding
KW  - Bandwidth
KW  - Data compression
KW  - Image reconstruction
KW  - Image classification
KW  - Humans
KW  - Sorting
KW  - Bandwidth compression
KW  - Bhattacharyya features
KW  - cosine transform
KW  - image coding
KW  - Mandala transform
KW  - object classification
KW  - pattern recognition
KW  - target recognition
DO  - 10.1109/TPAMI.1983.4767430
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 5
SN  - 1939-3539
VO  - PAMI-5
VL  - PAMI-5
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - Sept. 1983
AB  - The problem of recognition of objects in images is investigated from the simultaneous viewpoints of image bandwidth compression and automatic target recognition. A scenario is suggested in which recognition is implemented on features in the block cosine transform domain which is useful for data compression as well. While most image frames would be processed by the automatic recognition algorithms in the compressed domain without need for image reconstruction, this still allows for visual image classification of targets with poor recognition rates (by human viewing at the receiving terminal). It has been found that the Mandala sorting of the block cosine domain results in a more effective domain for selecting target identification parameters. Useful features from this Mandala/cosine domain are developed based upon correlation parameters and homogeneity measures which appear to successfully discriminate between natural and man-made objects. The Bhattacharyya feature discriminator is used to provide a 10:1 compression of the feature space for implementation of simple statistical decision surfaces (Gaussian and minimum distance classification). Imagery sensed in the visible spectra with a resolution of approximately 5-10 ft is used to illustrate the success of the technique on targets such as ships to be separated from clouds. A data set of 38 images is used for experimental verification with typical classification results ranging from the high 80's to low 90 percentile regions depending on the options choosen.
ER  - 


TY  - JOUR
TI  - A Parametric Study of Magneto-Optic Imaging Using Finite-Element Analysis Applied to Aircraft Rivet Site Inspection
T2  - IEEE Transactions on Magnetics
SP  - 3737
EP  - 3744
AU  - Z. Zeng
AU  - X. Liu
AU  - Y. Deng
AU  - L. Udpa
AU  - L. Xuan
AU  - W. C. L. Shih
AU  - G. L. Fitzpatrick
PY  - 2006
KW  - Parametric study
KW  - Finite element methods
KW  - Image analysis
KW  - Magnetic analysis
KW  - Aircraft
KW  - Inspection
KW  - Magnetooptic effects
KW  - Eddy currents
KW  - Aluminum
KW  - Numerical simulation
KW  - Eddy-current techniques
KW  - finite-element analysis
KW  - magneto-optic imaging
DO  - 10.1109/TMAG.2006.880997
JO  - IEEE Transactions on Magnetics
IS  - 11
SN  - 1941-0069
VO  - 42
VL  - 42
JA  - IEEE Transactions on Magnetics
Y1  - Nov. 2006
AB  - Magneto-optic/eddy current imaging (MOI) has become increasingly popular for inspecting aging aluminum airframes for cracks and corrosion due to its accuracy, reliability, and ease of use. As inspection requirements change, modifications to the MOI system must be made to improve sensitivity and resolution to reliably detect smaller and/or deeper defects in the aircraft structure. Incorporating such improvements by “cut and try” methods is time-consuming and expensive. Therefore, a numerical simulation model that produces quantitative values of the magnetic fields associated with induced eddy currents interacting with structural defects is an essential complement to the instrument development process. Such a model provides a convenient tool for parametrically evaluating the effectiveness of the MOI for detecting various structural defects. This paper presents a three-dimensional finite-element model of Maxwell's equations, utilizing the$mbi A$-$V$formulation for numerical simulation of the MOI operation. The model is used to predict quantitative values of field distributions that produce the binary magneto-optic images of subsurface fatigue cracks at rivet sites in an aluminum airframe structure. A parametric study is performed to determine the effects of MOI operational parameters on the binary images. A skewness parameter based on the binary images is established to provide a measure of defect size. This parameter will prove useful for automatic detection and classification of defects. The model-generated images show good agreement with experimentally derived MOI images.
ER  - 


TY  - CONF
TI  - Aircraft Detection Approach Based on YOLOv9 for High-Resolution Remote Sensing
T2  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
SP  - 455
EP  - 459
AU  - I. Saetchnikov
AU  - V. Skakun
AU  - E. Tcherniavskaia
PY  - 2024
KW  - Accuracy
KW  - Supply chains
KW  - Threat assessment
KW  - Trajectory
KW  - Air traffic control
KW  - Aircraft
KW  - Task analysis
KW  - neural network
KW  - deep learning
KW  - aircraft detection
KW  - object detection
KW  - YOLO (you only looks once)
KW  - remote sensing
KW  - aerospace image analysis
DO  - 10.1109/MetroAeroSpace61015.2024.10591528
JO  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
IS  - 
SN  - 2575-7490
VO  - 
VL  - 
JA  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
Y1  - 3-5 June 2024
AB  - The rapid advancements in Cube and Nano satellites have made high-quality remote sensing images easily accessible, leading to crucial object recognition tasks with extensive applications range. The particular task of aircraft detection is placed as one of the most promising applications of computer vision systems for aerospace data processing, primarily due to its broad application prospects starting from aircraft traffic planning and controlling for potential threats detection, environmental monitoring to managing CO2 emissions all the way to logistics supply chain optimization based on spatial finance insights. However, analyzing high-quality remote sensing data presents a significant challenge, particularly in the detection tasks of densely located aircrafts, aircrafts with various size ranges, etc. In this regard, this article focuses on addressing these challenges through a deep learning-based approach based on an optimized version of YOLOv9 for aircraft detection on high-resolution remote sensing data. The proposed approach has been trained and evaluated based on the images from Airbus dataset and demonstrates 3 % increase in accuracy detection compared to existing state-of-the-art approaches, achieving 0.987 in average precision (AP) and 0.746 in mean average precision (mAP).
ER  - 


TY  - CONF
TI  - Neural classification of high resolution remote sensing imagery for power transmission lines surveillance
T2  - IEEE International Geoscience and Remote Sensing Symposium
SP  - 500
EP  - 502 vol.1
AU  - E. Binaghi
AU  - I. Gallo
AU  - M. Pepe
AU  - P. A. Brivio
AU  - S. Musazzi
AU  - A. Bassini
PY  - 2002
KW  - Image resolution
KW  - Remote sensing
KW  - Power transmission lines
KW  - Surveillance
KW  - Space technology
KW  - Availability
KW  - Aircraft
KW  - Image sensors
KW  - Image processing
KW  - Object recognition
DO  - 10.1109/IGARSS.2002.1025086
JO  - IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - IEEE International Geoscience and Remote Sensing Symposium
Y1  - 24-28 June 2002
AB  - The larger availability of high resolution remotely sensed data, provided by novel aircraft and space sensors, offers new perspective to image processing techniques, but it introduces also the need for operational classification tools in order to completely exploit the potentialities of these data In many applicative contexts, in particular for technological network surveillance tasks, which involve specific requirements, such as (1) high resolution and accuracy in object recognition and positioning; (2) straightforward update and change detection; (3) geographic generalisation. The application presented deals with the recognition of features of interest for the surveillance of power transmission lines using IKONOS imagery. We proposed a methodology in which multi-scale and neural techniques are synergically combined to identify features at different scales and to fuse them for class discrimination. The results obtained on a pilot area in Northern Italy proved that the combination of multi-window feature extraction and neural soft classification produced an agile and flexible model that can act as a classifier of objects that vary in shape, size and structure.
ER  - 


TY  - CONF
TI  - Land Use/Cover Classification by Using Digital Camera Imagery
T2  - 2009 Sixth International Conference on Computer Graphics, Imaging and Visualization
SP  - 540
EP  - 546
AU  - H. S. Lim
AU  - M. Z. MatJafri
AU  - K. Abdullah
AU  - C. J. Wong
PY  - 2009
KW  - Digital cameras
KW  - Digital images
KW  - Environmental economics
KW  - Layout
KW  - Image sensors
KW  - Image analysis
KW  - Industrial economics
KW  - Aircraft
KW  - Satellites
KW  - Polynomials
KW  - Digital Camera
KW  - Land use/cover
KW  - Supervised Classification
DO  - 10.1109/CGIV.2009.38
JO  - 2009 Sixth International Conference on Computer Graphics, Imaging and Visualization
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2009 Sixth International Conference on Computer Graphics, Imaging and Visualization
Y1  - 11-14 Aug. 2009
AB  - This paper present an economical analysis of using a digital camera imagery data to classify land use/cover in the Prai Industrial area, Penang, located in Peninsular Malaysia. The data were captured using a digital camera, Kodak DC 290 from a small light aircraft at 8000 feet altitude. This overcomes the problem of difficulty in obtaining cloud-free satellite images especially in the equatorial region where experience showed that just two or three cloud-free scenes per year could be obtained. The use of digital camera as a sensor to capture digital images is cheaper and economical compared to the use of other airborne sensor. The images consisted of the three visible bands-red, green and blue. Three supervised classifications techniques (maximum likelihood, minimum distance-to-mean and parallelepiped) were performed to the digital image. The training sites were established using polygons within each scene and four land cover classes were assigned to each classifier. The relative performance of the techniques was evaluated. The accuracy of the classified images was validated using a reference data set. The results produced high degree of accuracy. Finally, geometric correction was performed to the digital image using the nearest neighborhood method with second order polynomial to produce geocoded map. The final results showed that the digital camera can be used as a tool for providing useful data for land cover classification. The classified images provided useful information for planning and development of a small area of coverage.
ER  - 


TY  - CONF
TI  - Hyperspectral Image Classification for Remote Sensing Using Low-Power Neuromorphic Hardware
T2  - 2019 International Joint Conference on Neural Networks (IJCNN)
SP  - 1
EP  - 7
AU  - V. Parmar
AU  - J. -H. Ahn
AU  - M. Suri
PY  - 2019
KW  - Pipelines
KW  - Neurons
KW  - Hardware
KW  - Training
KW  - Principal component analysis
KW  - Hyperspectral imaging
KW  - HSI
KW  - RBF
KW  - kNN
KW  - Neuromorphic computing
KW  - Remote Sensing
DO  - 10.1109/IJCNN.2019.8852001
JO  - 2019 International Joint Conference on Neural Networks (IJCNN)
IS  - 
SN  - 2161-4407
VO  - 
VL  - 
JA  - 2019 International Joint Conference on Neural Networks (IJCNN)
Y1  - 14-19 July 2019
AB  - In this paper, we present a novel feature extraction algorithm based approach for performing Hyperspectral Image Classification using a low-power Neuromorphic hardware. The application of interest for this study is HSI image classification for remote sensing. We demonstrate energy-efficient data processing pipeline optimized to use with on-edge neuromorphic hardware. The dataset used for the study is Salinas-A. We use the Brilliant USB stick with 4 NM500 chips for prototyping the application. Achieved recognition time is 18.4 μs and energy consumption is ~10 μJ with an accuracy of ~ 97%.
ER  - 


TY  - JOUR
TI  - Development of a field-portable imaging system for scene classification using multispectral data fusion algorithms
T2  - IEEE Aerospace and Electronic Systems Magazine
SP  - 13
EP  - 19
AU  - E. Preston
AU  - T. Bergman
AU  - R. Gorenflo
AU  - D. Hermann
AU  - E. Kopala
AU  - T. Kuzma
AU  - L. Lazofson
AU  - R. Orkis
PY  - 1994
KW  - Layout
KW  - Clustering algorithms
KW  - Real time systems
KW  - Sensor fusion
KW  - Pixel
KW  - Classification algorithms
KW  - Multispectral imaging
KW  - Charge-coupled image sensors
KW  - Infrared image sensors
KW  - Artificial neural networks
DO  - 10.1109/62.312974
JO  - IEEE Aerospace and Electronic Systems Magazine
IS  - 9
SN  - 1557-959X
VO  - 9
VL  - 9
JA  - IEEE Aerospace and Electronic Systems Magazine
Y1  - Sept. 1994
AB  - Battelle scientists have assembled a reconfigurable multispectral imaging and classification system which can be taken into the field to support automated real-time target/background discrimination. The system may be used for a variety of applications including environmental remote sensing, industrial inspection and medical imaging. This paper discusses hard tactical target and runway detection applications performed with the multispectral system. The Battelle-developed system consists of a passive, multispectral imaging electro-optical (EO) sensor suite and a real-time digital data collection and data fusion image processor. The EO sensor suite, able to collect imagery in 12 distinct wavebands from the ultraviolet (UV) through the long wave infrared (LWIR), consists of five charge-coupled device (CCD) cameras and two thermal IR imagers integrated on a common portable platform. The data collection and processing system consists of video switchers, recorders and a real-time sensor fusion/classification hardware system which combines any three input wavebands to perform real-lime data fusion by applying "look-up tables", derived from tailored neural network algorithms, to classify the imaged scene pixel by pixel. The result is then visualized in a video format on a full color, 9-inch, active matrix Liquid Crystal Display (LCD). A variety of classification algorithms including artificial neural networks and data clustering techniques were successfully optimized to perform pixel-level classification of imagery in complex scenes comprised of tactical targets, buildings, roads, aircraft runways, and vegetation. Algorithms implemented included unsupervised maximum likelihood, Linde Buzo Gray, and "fuzzy" clustering algorithms along with Multilayer Perceptron and Learning Vector Quantization (LVQ) neural networks. Supervised clustering of the data was also evaluated. To assess classification robustness, algorithms were tested on imagery recorded over broad periods of time throughout the day. Results were excellent, indicating that scene classification is achievable despite. Temporal signature variations. Waveband saliency analyses were performed to determine which spectral bands contained the bulk of the discriminating information for discerning objects in the scenes. Optimized classification algorithms are then used to populate the look-up tables in the sensor fusion board for real-time use in the field.<>
ER  - 


TY  - CONF
TI  - Identification of aircraft on the basis of 2-D radar images
T2  - Proceedings International Radar Conference
SP  - 405
EP  - 409
AU  - K. Rosenbach
AU  - J. Schiller
PY  - 1995
KW  - Aircraft
KW  - Radar imaging
KW  - Airborne radar
KW  - Radar tracking
KW  - Frequency
KW  - Data preprocessing
KW  - Physics
KW  - Data mining
KW  - Signal processing
KW  - Image sensors
DO  - 10.1109/RADAR.1995.522581
JO  - Proceedings International Radar Conference
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings International Radar Conference
Y1  - 8-11 May 1995
AB  - In the past, many methods have been proposed and investigated to identify aircraft non-cooperatively, e.g. by using the infrared-, acoustical-, optical or radar-signatures of the targets. Some of these methods are passive, having the advantage of not alerting the observed object, but with some other shortcomings such as very limited observation ranges or marginal resolutions. The approach presented is based on exploitation of 2-dimensional radar images of an aircraft. Under certain constraints concerning the aspect angle of the target and manoeuvres, these images contain information that may allow for the identification of the aircraft under observation. A processing scheme for an identification process based on 2-D radar images is outlined and the results using both simulated and real data are shown.
ER  - 


TY  - JOUR
TI  - Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems
T2  - IEEE Transactions on Communications
SP  - 5181
EP  - 5192
AU  - X. Kang
AU  - B. Song
AU  - J. Guo
AU  - Z. Qin
AU  - F. R. Yu
PY  - 2022
KW  - Semantics
KW  - Task analysis
KW  - Computational modeling
KW  - Image communication
KW  - Reinforcement learning
KW  - Image coding
KW  - Artificial intelligence
KW  - Aerial image transmission
KW  - task-oriented communication
KW  - compressive sensing
KW  - reinforcement learning (RL)
DO  - 10.1109/TCOMM.2022.3182325
JO  - IEEE Transactions on Communications
IS  - 8
SN  - 1558-0857
VO  - 70
VL  - 70
JA  - IEEE Transactions on Communications
Y1  - Aug. 2022
AB  - The vigorous developments of the Internet of Things make it possible to extend its computing and storage capabilities to computing tasks in the aerial system with the collaboration of cloud and edge, especially for artificial intelligence (AI) tasks based on deep learning (DL). Collecting a large amount of image/video data, unmanned aerial vehicles (UAVs) can only hand over intelligent analysis tasks to the back-end mobile edge computing (MEC) server due to their limited storage and computing capabilities. How to efficiently transmit the most correlated information for the AI model is a challenging topic. Inspired by task-oriented communication in recent years, we propose a new aerial image transmission paradigm for the scene classification task. A lightweight model is developed on the front-end UAV for semantic block transmission with the perception of images and channel states. To achieve the tradeoff between transmission latency and classification accuracy, deep reinforcement learning (DRL) is applied to explore the semantic blocks which have the greatest contribution to the back-end classifier under various channel states. Experimental results show that the proposed method can significantly improve classification accuracy by more than 4% under the same conditions, compared to other semantic saliency learning methods.
ER  - 


TY  - CONF
TI  - Multiscale fully convolutional network with application to industrial inspection
T2  - 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)
SP  - 1
EP  - 8
AU  - X. Bian
AU  - S. N. Lim
AU  - N. Zhou
PY  - 2016
KW  - Image segmentation
KW  - Inspection
KW  - Semantics
KW  - Training
KW  - Blades
KW  - Aircraft propulsion
KW  - Machine learning
DO  - 10.1109/WACV.2016.7477595
JO  - 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)
Y1  - 7-10 March 2016
AB  - In recent years, deep learning, particularly Convolutional Neural Network (CNN), has shown great efficacy for solving various vision tasks. In image segmentation, it has been demonstrated that a CNN can greatly outperform other approaches. However, special attention has to be paid towards setting various parameters in the CNN that affects the scale of the feature map generated at the last convolutional layer, where scale here refers to the ratio of the number of pixels in the original input image that correspond to each pixel in the feature map. Quite often, the optimal settings are tied to the specific problem on hand and can be fairly challenging to determine. To overcome such an issue, this paper proposes a multiscale Fully Convolutional Network (FCN) that combines networks trained at various scales, thereby allowing for conducting segmentation more generically. Moreover, such a multiscale architecture allows for incremental fine-tuning as more training images become available later on and new networks can be trained and added to the combined network. Such flexibility has great utility in applications such as industrial inspection, where training images may not be readily available initially, but yet requires a high level of accuracy. This paper will validate our findings by reporting the results that we have obtained by applying multiscale FCN to the inspection of aircraft engine part.
ER  - 


TY  - CONF
TI  - Spectral And Textural Classification Of GER 63 Channel Data For Vegetation Cover Mapping In Northeastern Minnesota
T2  - 10th Annual International Symposium on Geoscience and Remote Sensing
SP  - 1617
EP  - 1617
AU  - W. H. Aymard
AU  - J. A. MacDonald
PY  - 1990
KW  - Vegetation mapping
KW  - Image classification
KW  - Pixel
KW  - Optical imaging
KW  - Radiometry
KW  - Narrowband
KW  - Spectroscopy
KW  - Aircraft
KW  - Production
KW  - Geology
DO  - 10.1109/IGARSS.1990.688818
JO  - 10th Annual International Symposium on Geoscience and Remote Sensing
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 10th Annual International Symposium on Geoscience and Remote Sensing
Y1  - 20-24 May 1990
AB  - 
ER  - 


TY  - CONF
TI  - Contextual classification of Cropcam UAV high resolution images using frequency-based approach for land use/land cover mapping case study: Penang Island
T2  - 2011 IEEE Symposium on Industrial Electronics and Applications
SP  - 663
EP  - 668
AU  - F. M. Hassan
AU  - M. Z. Mat Jafri
AU  - H. S. Lim
PY  - 2011
KW  - Accuracy
KW  - Spatial resolution
KW  - Software
KW  - Satellites
KW  - Remote sensing
KW  - Digital cameras
KW  - Cropcam UAV
KW  - Digital Camera
KW  - LULC
KW  - Contextual
DO  - 10.1109/ISIEA.2011.6108799
JO  - 2011 IEEE Symposium on Industrial Electronics and Applications
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2011 IEEE Symposium on Industrial Electronics and Applications
Y1  - 25-28 Sept. 2011
AB  - Cropcam UAV provides GPS based digital images on demand and real time data with high temporal resolution throughout the equatorial region where the sky is often covered by clouds. The images obtained by the UAV system in this research were used to overcome the problem of unclear images obtained by the satellite and manned aircraft in our study area. Conventional classification methods commonly cannot handle the complex landscape environment in the image. The result of each image has often a salt and pepper appearances which are the main characteristic of misclassification. The objective of this study is to evaluate the land use/land cover features over Penang Island using contextual classification method based on the frequency-based approach. The technique was applied to the high resolution images in three bands collected from a digital camera equipped with the platform system to extract thematic maps. Contextual classifier that utilized both spectral and spatial information could be reduce the speckle error and improve the classification performance significantly. Four classes could be classified clearly within the study area, and a high accuracy was achieved in the classification process. In order to evaluate the performance of the classifier, nine different window sizes ranging from 3 by 3 to 19 by 19 with an increment are tested. The study revealed that the frequency based-contextual classifier is effective with the images used in this research compare with the satellite images and images collected from conventional manned platforms and could be used for land use/cover mapping for the small area of coverage.
ER  - 


TY  - CONF
TI  - Language-Assisted Siamese Contrastive Framework for Fine-Grained Remote Sensing Ship Image Retrieval
T2  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
SP  - 7133
EP  - 7137
AU  - Y. Zhang
AU  - Z. Jiang
AU  - Y. Liu
AU  - Y. Li
AU  - X. Wang
AU  - Y. Zhang
AU  - C. Yan
PY  - 2024
KW  - Image retrieval
KW  - Semantics
KW  - Contrastive learning
KW  - Feature extraction
KW  - Data models
KW  - Sensors
KW  - Data mining
KW  - fine-grained ship image retrieval
KW  - remote sensing
KW  - language model
KW  - contrastive learning
DO  - 10.1109/IGARSS53475.2024.10640466
JO  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 7-12 July 2024
AB  - As the number of remote sensing (RS) images increases, it is crucial to retrieval ship targets according to specific demands. The existing ship image retrieval methods only extract features from the image modality, which may not fully utilize the rich text information available and ignore the high-level hierarchical relations between ship classes. In this paper, we propose a language-assisted siamese contrastive framework, namely LASCF, for fine-grained ship retrieval in RS images. In the new LASCF, the siamese vision models are employed to measure the similarity between images. Moreover, a label text encoder with a pretrained language model is designed to extract the high-level semantic information from labels, and thus the information of the hierarchical relations between ship classes are fused in LASCF. Finally, the multimodal similarity measurement module based on contrastive learning is proposed to optimize the siamese vision models. The experimental results show that the proposed LASCF outperforms several existing state-of-the-art methods.
ER  - 


TY  - CONF
TI  - Aircraft Detection from Satellite Images Using ATA-Plane Data Set
T2  - 2019 27th Signal Processing and Communications Applications Conference (SIU)
SP  - 1
EP  - 4
AU  - M. Polat
AU  - H. M. A. Mohammed
AU  - E. A. Oral
AU  - I. Y. Ozbek
PY  - 2019
KW  - Aircraft
KW  - Satellites
KW  - Convolutional neural networks
KW  - Object detection
KW  - Mathematical model
KW  - Military aircraft
KW  - Proposals
KW  - Aircraft Detection
KW  - Satellite Image Analysis
KW  - Object Detection
KW  - Faster Region-based Convolutional Neural Network (Faster R-CNN)
KW  - Transfer Learning
KW  - Alexnet
DO  - 10.1109/SIU.2019.8806582
JO  - 2019 27th Signal Processing and Communications Applications Conference (SIU)
IS  - 
SN  - 2165-0608
VO  - 
VL  - 
JA  - 2019 27th Signal Processing and Communications Applications Conference (SIU)
Y1  - 24-26 April 2019
AB  - Satellite image analysis is a field of study in the field of image processing for many civil and military applications. Satellite images have many civilian applications, including recognition, detection and classification of regions, buildings, roads, aircrafts and other man-made objects. Among these, especially aircraft detection is strategically important for military applications and forms the basis of this study. In the first phase of the study, a new dataset composed of airplanes at different airports on Google Earth's satellite images is prepared to eliminate the lack of data set in this field. In the second stage, the detection of aircraft was carried out by using algorithms based on Convolutional Neural Network. Transfer Learning was used to improve the performance of this process. The accuracy rate obtained as a result of Transfer Learning with previously trained network Alexnet, was 91%.
ER  - 


TY  - JOUR
TI  - Target classification using SIFT sequence scale invariants
T2  - Journal of Systems Engineering and Electronics
SP  - 633
EP  - 639
AU  - X. Zhu
AU  - C. Ma
AU  - B. Liu
AU  - X. Cao
PY  - 2012
KW  - Aircraft
KW  - Support vector machines
KW  - Feature extraction
KW  - Aerospace electronics
KW  - Target recognition
KW  - Image recognition
KW  - target classification
KW  - scale invariant feature transform descriptors
KW  - sequence scale
KW  - support vector machine
DO  - 10.1109/JSEE.2012.00079
JO  - Journal of Systems Engineering and Electronics
IS  - 5
SN  - 1004-4132
VO  - 23
VL  - 23
JA  - Journal of Systems Engineering and Electronics
Y1  - Oct. 2012
AB  - On the basis of scale invariant feature transform (SIFT) descriptors, a novel kind of local invariants based on SIFT sequence scale (SIFT-SS) is proposed and applied to target classification. First of all, the merits of using an SIFT algorithm for target classification are discussed. Secondly, the scales of SIFT descriptors are sorted by descending as SIFT-SS, which is sent to a support vector machine (SVM) with radial based function (RBF) kernel in order to train SVM classifier, which will be used for achieving target classification. Experimental results indicate that the SIFT-SS algorithm is efficient for target classification and can obtain a higher recognition rate than affine moment invariants (AMI) and multi-scale auto-convolution (MSA) in some complex situations, such as the situation with the existence of noises and occlusions. Moreover, the computational time of SIFT-SS is shorter than MSA and longer than AMI.
ER  - 


TY  - CONF
TI  - Aircraft Target Recognition Using Copula Joint Statistical Model and Sparse Representation Based Classification
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 3635
EP  - 3638
AU  - A. Karine
AU  - A. Toumi
AU  - A. Khenchaf
AU  - M. E. Hassouni
PY  - 2018
KW  - Wavelet transforms
KW  - Dictionaries
KW  - Training
KW  - Target recognition
KW  - Feature extraction
KW  - Radar imaging
KW  - ATR
KW  - ISAR
KW  - complex wavelet domain
KW  - copula
KW  - sparse classifier
DO  - 10.1109/IGARSS.2018.8518668
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - This paper proposes a new target recognition method for inverse synthetic aperture radar (ISAR) images. This method is based on joint statistical modeling of the complex wavelet coefficients for ISAR image characterization and the sparse representation based classification (SRC) for the recognition. To extract features from an ISAR image, we first transform it in the complex wavelet domain using the dual-tree complex wavelet transform (DT-CWT). Then, we compute magnitude information for each complex subband. After that, we propose a joint statistical model for magnitude distribution, that takes into account the dependences between different orientations and scales. To do so, we adopt the copula as a multivariate model thanks to its suitability to capture jointly the subband marginal distribution and the dependence structure. For the recognition step, we exploit SRC which recovers the test descriptor to classify over a given dictionary composed by the training descriptors. This method classifies the test sample as the class whose training samples can generate the minimum sparse representation error. Experimental results on ISAR images database show that using copula and sparse classifier improve significantly the recognition rates compared to classical models and classifiers.
ER  - 


TY  - CONF
TI  - Automatic aircraft recognition: toward using human similarity measure in a recognition system
T2  - Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)
SP  - 268
EP  - 273 Vol. 1
AU  - B. Kamgar-Parsi
AU  - A. K. Jain
PY  - 1999
KW  - Humans
KW  - Anthropometry
KW  - Testing
KW  - Databases
KW  - Computer vision
KW  - Image recognition
KW  - Infrared imaging
KW  - Shape measurement
KW  - Military aircraft
KW  - Current measurement
DO  - 10.1109/CVPR.1999.786950
JO  - Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)
IS  - 
SN  - 1063-6919
VO  - 1
VL  - 1
JA  - Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)
Y1  - 23-25 June 1999
AB  - The problem of screening images of the skies to determine whether they contain aircraft or not is both of theoretical and practical interest. After the most prominent visual signal in the infrared image of the sky is extracted, the question is whether the signal is a correct match of an aircraft. Common approaches calculate the degree of similarity of the shape of the signal with a model aircraft using a similarity measure such as Euclidean distance, and make a decision based on whether the degree of similarity exceeds a (pre-specified) threshold. Our approach avoids metric similarity measures and the use of thresholds as it attempts to employ similarity measures used by humans. In the absence of sufficient real data, the approach allows to specifically generate an arbitrarily large number of training exemplars projecting near classification boundary. Once trained on such a training set, the performance of the neural network was comparable to that of a human expert, and far better than a network trained only on the available real data. Furthermore, the results were considerably better than those obtained using a Euclidean discriminator.
ER  - 


TY  - CONF
TI  - Application of Capsule Networks to Open-set Target Recognition of ISAR Images of Small Complex Targets
T2  - 2022 International Conference on Electromagnetics in Advanced Applications (ICEAA)
SP  - 149
EP  - 149
AU  - C. D. Stewart-Burger
AU  - D. J. Ludick
AU  - M. Potgieter
PY  - 2022
KW  - Training
KW  - Target recognition
KW  - Machine learning
KW  - Noise measurement
KW  - Security
KW  - Aircraft
KW  - Task analysis
DO  - 10.1109/ICEAA49419.2022.9899926
JO  - 2022 International Conference on Electromagnetics in Advanced Applications (ICEAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Electromagnetics in Advanced Applications (ICEAA)
Y1  - 5-9 Sept. 2022
AB  - The rising popularity of small aircraft such as multi-rotor drones, and the security concerns linked to small aircraft create an increasing demand for systems that are able to detect and recognize such targets. An effective target recognition system should be able to recognize targets in a real-world setting, under conditions where the system receives noisy inputs. In addition to being able to handle noisy inputs, a target recognition system should be able to handle both known targets (targets that it has been trained on) and unknown targets (targets that do not form part of the training set). Open-set classification such as this is non-trivial, as it can be a challenging task to train a machine learning model to recognize inputs that do not belong to any class observed in training.
ER  - 


TY  - CONF
TI  - Automatic Aircraft Shadow Removal from Remote Sensing Images Using Mask-ShadowGAN
T2  - 2021 IEEE 8th International Conference on Industrial Engineering and Applications (ICIEA)
SP  - 517
EP  - 521
AU  - S. Ganyaporngul
AU  - N. Cooharojananone
AU  - P. Kruachottikul
AU  - D. Trakulwaranont
AU  - S. Satoh
PY  - 2021
KW  - Training
KW  - Shape
KW  - Conferences
KW  - Industrial engineering
KW  - Sensors
KW  - Indexes
KW  - Aircraft
KW  - Deep Learning
KW  - Remote Sensing Image
KW  - Mask-ShadowGAN
KW  - Shadow Removal
DO  - 10.1109/ICIEA52957.2021.9436794
JO  - 2021 IEEE 8th International Conference on Industrial Engineering and Applications (ICIEA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 8th International Conference on Industrial Engineering and Applications (ICIEA)
Y1  - 23-26 April 2021
AB  - Objects with shadow may cause a problem for image classification. For example, it can separate one object into many objects. It can also alter the size or shape of the object resulting in misclassification. In this paper, we focus on removing aircraft shadow from remote sensing images where the shadows occur on wings, bodies, and tails. Since it is very difficult to get shadow-free aircraft images and a shadow aircraft image of the same type for the training part, we adopted Mask-ShadowGAN for solving this issue. The benefit of the Mask-ShadowGAN algorithm is that, in the training part, the technique does not require the same images that have both shadow and shadow-free. In the experiment, we evaluated our proposed technique using RMSE and Jaccard similarity index for measurement. The experimental result shows that our technique shows promising results. We present both best and worst result based on sorted similarity index.
ER  - 


TY  - CONF
TI  - Precision Agriculture: Crop Image Segmentation and Loss Evaluation through Drone Surveillance
T2  - 2023 Third International Conference on Secure Cyber Computing and Communication (ICSCCC)
SP  - 495
EP  - 500
AU  - P. K. Patidar
AU  - D. S. Tomar
AU  - R. K. Pateriya
AU  - Y. K. Sharma
PY  - 2023
KW  - Surveys
KW  - Image segmentation
KW  - Surveillance
KW  - Crops
KW  - Production
KW  - Agriculture
KW  - Task analysis
KW  - Drone Surveillance
KW  - Precision agriculture
KW  - Image Segmentation
KW  - Deep Learning
KW  - Threshold
DO  - 10.1109/ICSCCC58608.2023.10176980
JO  - 2023 Third International Conference on Secure Cyber Computing and Communication (ICSCCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Third International Conference on Secure Cyber Computing and Communication (ICSCCC)
Y1  - 26-28 May 2023
AB  - Agriculture is the most important source of livelihood. Crop segmentation has become an important role in precision agriculture which helps farmers to make decisions about crop damage and its production. However, it's a challenging task to achieve precision in the agriculture field. Drone Surveillance helps to achieve that crop yield assessment, crop damage, crop health, and other parameters. This paper focuses on image segmentation of crops, classified into categories like sparse and dense crops with the multitemporal data image taken by Drone. This model proposed and studied shows the loss percentage in crop identification by image segmentation process, it helps farmers to get good compensation for crops to survey through Drone (UAV) techniques. A detailed analysis with outcome of thisis explained further.
ER  - 


TY  - CONF
TI  - Efficient Image Embedding for Fine-Grained Visual Classification
T2  - 2022 14th International Conference on Knowledge and Smart Technology (KST)
SP  - 40
EP  - 45
AU  - S. Payatsuporn
AU  - B. Kijsirikul
PY  - 2022
KW  - Location awareness
KW  - Training
KW  - Knowledge engineering
KW  - Visualization
KW  - Adaptive systems
KW  - Annotations
KW  - Semantics
KW  - Fine-grained visual classification
KW  - deep learning
KW  - convolutional neural network
KW  - localization
KW  - loss function
KW  - embedding
DO  - 10.1109/KST53302.2022.9729062
JO  - 2022 14th International Conference on Knowledge and Smart Technology (KST)
IS  - 
SN  - 2374-314X
VO  - 
VL  - 
JA  - 2022 14th International Conference on Knowledge and Smart Technology (KST)
Y1  - 26-29 Jan. 2022
AB  - Fine-grained visual classification (FGVC) is a task belonging to multiple sub-categories classification. It is a challenging task due to high intraclass variation and inter-class similarity. Most exiting methods pay attention to capture discriminative semantic parts to address those problems. In this paper, we introduce a two-level network which consists of raw-level and object-level networks, and we name it “Efficient Image Embedding”. Its training procedure has two stages which the raw-level is for localization by the aggregation of feature maps, and the last is for classification. The two-level use Adaptive Angular Margin loss (AAM-loss), which improve an intra-class compactness and inter-class variety of image embedding. Our approach is to identify object regions without any hand-crafted bounding-box, and can be trained in an end-to-end manner. It has achieved better accuracy on two datasets compared to the existing work, which are 89.0% for CUB200-2011 and 93.3% for FGVC-Aircraft.
ER  - 


TY  - CONF
TI  - Automatic feature extraction and stereo image processing with genetic algorithms for LiDAR data
T2  - International Conference on Computer Graphics, Imaging and Visualization (CGIV'05)
SP  - 307
EP  - 309
AU  - Teng-To Yu
AU  - Ming Yang
AU  - Chao-Shi Chen
PY  - 2005
KW  - Feature extraction
KW  - Stereo vision
KW  - Genetic algorithms
KW  - Laser radar
KW  - Digital cameras
KW  - Virtual reality
KW  - Image processing
KW  - Pixel
KW  - Global Positioning System
KW  - Aircraft navigation
KW  - GA
KW  - LiDAR
KW  - Feature Extraction
DO  - 10.1109/CGIV.2005.22
JO  - International Conference on Computer Graphics, Imaging and Visualization (CGIV'05)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - International Conference on Computer Graphics, Imaging and Visualization (CGIV'05)
Y1  - 26-29 July 2005
AB  - Aerial photos with LiDAR data were processed with genetic algorithms for not only the feature extraction but also orthographical image. DSM provided by LiDAR reduced the amount of GCPs needed for the regular processing, thus the reason both of the efficiency and accuracy are highly improved.
ER  - 


TY  - JOUR
TI  - Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses
T2  - IEEE Transactions on Neural Networks and Learning Systems
SP  - 683
EP  - 694
AU  - W. Shi
AU  - Y. Gong
AU  - X. Tao
AU  - D. Cheng
AU  - N. Zheng
PY  - 2019
KW  - Training
KW  - Feature extraction
KW  - Data models
KW  - Measurement
KW  - Automobiles
KW  - Learning systems
KW  - Benchmark testing
KW  - Cascaded softmax loss
KW  - deep convolutional neural network (DCNN)
KW  - fine-grained image classification
KW  - generalized large-margin (GLM) loss
KW  - hierarchical label structure
DO  - 10.1109/TNNLS.2018.2852721
JO  - IEEE Transactions on Neural Networks and Learning Systems
IS  - 3
SN  - 2162-2388
VO  - 30
VL  - 30
JA  - IEEE Transactions on Neural Networks and Learning Systems
Y1  - March 2019
AB  - We develop a fine-grained image classifier using a general deep convolutional neural network (DCNN). We improve the fine-grained image classification accuracy of a DCNN model from the following two aspects. First, to better model the h-level hierarchical label structure of the fine-grained image classes contained in the given training data set, we introduce h fully connected (fc) layers to replace the top fc layer of a given DCNN model and train them with the cascaded softmax loss. Second, we propose a novel loss function, namely, generalized large-margin (GLM) loss, to make the given DCNN model explicitly explore the hierarchical label structure and the similarity regularities of the fine-grained image classes. The GLM loss explicitly not only reduces between-class similarity and within-class variance of the learned features by DCNN models but also makes the subclasses belonging to the same coarse class be more similar to each other than those belonging to different coarse classes in the feature space. Moreover, the proposed fine-grained image classification framework is independent and can be applied to any DCNN structures. Comprehensive experimental evaluations of several general DCNN models (AlexNet, GoogLeNet and VGG) using three benchmark data sets (Stanford car, fine-grained visual classification-aircraft and CUB-200-2011) for the fine-grained image classification task demonstrate the effectiveness of our method.
ER  - 


TY  - CONF
TI  - Deep Learning Based Radar Target Classification Using Micro-Doppler Features
T2  - 2021 Seventh International Conference on Aerospace Science and Engineering (ICASE)
SP  - 1
EP  - 6
AU  - A. Hanif
AU  - M. Muaz
PY  - 2021
KW  - Training
KW  - Deep learning
KW  - Target recognition
KW  - Radar
KW  - Radar imaging
KW  - Sensors
KW  - Convolutional neural networks
KW  - Radar
KW  - micro-Doppler
KW  - target recognition
KW  - deep convolutional neural network
KW  - deep learning
KW  - statistical classification
DO  - 10.1109/ICASE54940.2021.9904145
JO  - 2021 Seventh International Conference on Aerospace Science and Engineering (ICASE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 Seventh International Conference on Aerospace Science and Engineering (ICASE)
Y1  - 14-16 Dec. 2021
AB  - Demand for radar automatic target recognition is ever increasing owing to the extensive employment of radar sensors in urban scenarios and a drastic increase in the number of radar targets, especially drones and UAVs. Micro-Doppler signatures, resulting from the micro-motion dynamics of targets, have emerged as a key distinctive feature for radar automatic target recognition. This paper addresses the problem of radar target recognition based on deep learning and micro-Doppler signatures of targets. The choice of MobileNetV2 deep Convolutional Neural Network based classification on spectrogram images of the targets, has made the system more suitable for system implementation on embedded devices such as Raspberry Pi. Second important contribution of this paper is the augmentation of an extensive and diverse training dataset having five classes ultimately, for the testing of radar automatic target recognition, since few such datasets are available in the open literature. The dataset is developed using a W-band Frequency Modulated Continuous Wave radar. After training the model on the diverse training dataset, validation and test accuracies of 98.67% and 99% respectively, are achieved.
ER  - 


TY  - JOUR
TI  - A Cross-Platform HD Dataset and a Two-Step Framework for Robust Aerial Image Matching
T2  - IEEE Access
SP  - 66153
EP  - 66174
AU  - M. Shahid
AU  - A. B.
AU  - S. S. Channappayya
PY  - 2022
KW  - Image matching
KW  - Satellites
KW  - Visualization
KW  - Cameras
KW  - Aircraft
KW  - Image sensors
KW  - Semantics
KW  - Scene recognition
KW  - image matching
KW  - retrieval & classification
KW  - visual place recognition (VPR) and vision based localization (VBL)
DO  - 10.1109/ACCESS.2022.3184328
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - Image matching has been an active research area in the computer vision community over the past decades. Significant advances in image matching algorithms have attracted attention from many emerging applications. However, aerial image matching remains demanding due to the variety of airborne platforms and onboard electro-optic sensors, long operational ranges, limited datasets and resources, and constrained operating environments. We present two contributions in this work to overcome these challenges: a) an upgraded cross-platform image dataset built over images taken from an aircraft and satellite and b) a two-step cross-platform image matching framework. Our dataset considers several practical scenarios in cross-platform matching and semantic segmentation. The first step in our two-step matching framework performs coarse-matching using a lightweight convolutional neural network (CNN) with help from aircraft instantaneous parameters. In the second step, we fine-tune standard off-the-shelf image matching algorithms by exploiting spectral, temporal and flow features followed by cluster analysis. We validate our proposed matching framework over our dataset, two publicly available aerial cross-platform datasets, and a derived dataset using various standard evaluation methodologies. Specifically, we show that both steps in our proposed two-step framework help to improve the matching performance in the cross-platform image matching scenario.
ER  - 


TY  - CONF
TI  - AE and SAE Based Aircraft Image Denoising
T2  - 2018 5th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
SP  - 81
EP  - 85
AU  - M. Sharma
AU  - K. K. Sarma
AU  - N. Mastorakis
PY  - 2018
KW  - Noise reduction
KW  - Training
KW  - Military aircraft
KW  - Signal to noise ratio
KW  - Computer vision
KW  - Reliability
KW  - Testing
KW  - AE
KW  - SAE
KW  - DNN
KW  - Deep
KW  - Learning
KW  - De-noising
DO  - 10.1109/MCSI.2018.00027
JO  - 2018 5th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 5th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
Y1  - 25-27 Aug. 2018
AB  - Images are corrupted during transmission and acquisition. De-noising is an important image restoration operation which determines the accuracy of interpretation and recognition stages. Time and often traditional methods have been used for image de-noising. Lately, there has been considerably interest on learning aided image de-nosing. As deep learning has lately been established as the most efficient learning aided mechanism, it is increasingly being used for a range of image processing and computer vision applications. This paper focuses on the design of Auto-encoder (AE) and Stacked Auto-encoder (SAE) based approaches for de-noising of certain military aircrafts as part of an automatic target recognition (ATR) system. Five image types are taken for the work which are mixed with Gaussian, Poisson, Speckle, Salt and Pepper noise. For each of these image sets signal to noise ratio (SNR) variation between -3 to 10 dB are taken. Experimental results have show that the SAE based approach is more reliable despite showing higher computational latency.
ER  - 


TY  - CONF
TI  - Modeling the pattern spectrum as a Markov process and its use for efficient shape classification
T2  - 2009 16th IEEE International Conference on Image Processing (ICIP)
SP  - 429
EP  - 432
AU  - E. N. Zois
AU  - V. Anastassopoulos
PY  - 2009
KW  - Markov processes
KW  - Airplanes
KW  - Databases
KW  - Pattern recognition
KW  - Shape measurement
KW  - Research and development
KW  - Laboratories
KW  - Educational technology
KW  - Physics
KW  - Data mining
KW  - Pecstrum
KW  - shape classification
KW  - Markov
DO  - 10.1109/ICIP.2009.5414424
JO  - 2009 16th IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2009 16th IEEE International Conference on Image Processing (ICIP)
Y1  - 7-10 Nov. 2009
AB  - In this work the most important morphological granulometry, i.e. the pattern spectrum, is modeled, for the first time in the literature, as a first order Markov process. In addition, each of the terms of the process is shown to be normally distributed. The classification procedure followed for this specific application is based on modeling each separate class as a Markov process and making extensive use of the chain rule. Experimental results support the proposed classification procedure as quite promising, especially when compared to conventional classification techniques.
ER  - 


TY  - JOUR
TI  - The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification
T2  - IEEE Transactions on Image Processing
SP  - 4683
EP  - 4695
AU  - D. Chang
AU  - Y. Ding
AU  - J. Xie
AU  - A. K. Bhunia
AU  - X. Li
AU  - Z. Ma
AU  - M. Wu
AU  - J. Guo
AU  - Y. -Z. Song
PY  - 2020
KW  - Feature extraction
KW  - Training
KW  - Visualization
KW  - Automobiles
KW  - Task analysis
KW  - Data mining
KW  - Manuals
KW  - Fine-grained image classification
KW  - deep learning
KW  - loss function
KW  - mutual channel
DO  - 10.1109/TIP.2020.2973812
JO  - IEEE Transactions on Image Processing
IS  - 
SN  - 1941-0042
VO  - 29
VL  - 29
JA  - IEEE Transactions on Image Processing
Y1  - 2020
AB  - The key to solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show that it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms - a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive across the spatial dimension. The end result is therefore a set of feature channels, each of which reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford Cars). Ablative studies further demonstrate the superiority of the MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Codes are available at: https://github.com/dongliangchang/Mutual-Channel-Loss.
ER  - 


TY  - CONF
TI  - Rotated Hybrid Task Cascade Network for Remote Sensing Aircraft Target Recognition
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 4180
EP  - 4183
AU  - X. Cao
AU  - H. Zou
AU  - F. Cheng
AU  - R. Li
AU  - S. He
AU  - S. Li
PY  - 2021
KW  - Earth
KW  - Image recognition
KW  - Target recognition
KW  - Fuses
KW  - Semantics
KW  - Internet
KW  - Task analysis
KW  - High-resolution remote sensing image
KW  - Aircraft target
KW  - Direction detection
KW  - Fine-grained recognition
DO  - 10.1109/IGARSS47720.2021.9555014
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - Automatic aircraft target recognition, including direction detection and fine-grained classification, is an important but challenging problem. Multi-directional densely arranged targets and the tiny differences between classes cause difficulties in recognition and direction prediction. To overcome the aforementioned problems, a rotated hybrid task cascade (RHTC) network is proposed. Specifically, RHTC cascades the segmentation branch and the bounding-box (bbox) branch to fuse the semantic feature in a coarse- to- fine manner. In addition, a new oriented bounding box regressor (OBBR) is proposed to predict the direction of target, and a new directionalloss function is added to further optimize the regressor. Moreover, we design fine masks in preprocessing to achieve improved recognition performance. The experimental results evaluated on the datasets collected from Google Earth show that RHTC can achieve the state-of-the-art performance on self-defined direction precision (DP) and mean average precision (mAP).
ER  - 


TY  - CONF
TI  - Combining morphological mapping and principal curves for ship classification
T2  - International Symposium on Signals, Circuits and Systems, 2005. ISSCS 2005.
SP  - 605
EP  - 608 Vol. 2
AU  - H. L. Fernandez
AU  - J. M. de Seixas
AU  - S. R. Neves
AU  - J. B. O. Souza Filho
PY  - 2005
KW  - Marine vehicles
KW  - Data mining
KW  - Image segmentation
KW  - Military aircraft
KW  - Surveillance
KW  - Azimuth
KW  - Signal processing
KW  - Laboratories
KW  - Euclidean distance
KW  - Feature extraction
DO  - 10.1109/ISSCS.2005.1511313
JO  - International Symposium on Signals, Circuits and Systems, 2005. ISSCS 2005.
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - International Symposium on Signals, Circuits and Systems, 2005. ISSCS 2005.
Y1  - 14-15 July 2005
AB  - In this work, we develop a ship classifier, which employs principal curves to extract relevant information from segmented images. This classifier is based on the Euclidean distance of the point whose coordinates represent distinguishing features extracted from an incoming ship image to the principal curve assigned to each class. This methodology is attractive, since it has a low computational cost for the operational phase and easily scales up to an arbitrary number of classes. A mean classification efficiency of 97.3% was achieved, which outperforms previous results based on neural network architecture.
ER  - 


TY  - CONF
TI  - Closed planar shape classification using nonlinear alignment
T2  - 2011 IEEE Recent Advances in Intelligent Computational Systems
SP  - 537
EP  - 540
AU  - P. Telagarapu
PY  - 2011
KW  - Prototypes
KW  - Accuracy
KW  - Signal to noise ratio
KW  - Aircraft
KW  - Noise measurement
KW  - Speech recognition
KW  - Mathematical model
KW  - Nonlinear alignment
KW  - Optimal alignment
KW  - Closed planar classification
KW  - Nearest-mean classification
DO  - 10.1109/RAICS.2011.6069370
JO  - 2011 IEEE Recent Advances in Intelligent Computational Systems
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2011 IEEE Recent Advances in Intelligent Computational Systems
Y1  - 22-24 Sept. 2011
AB  - This paper addresses the problem associated with classification of signatures of four different types of aircraft prototypes. In order to classify the signatures, Nonlinear Alignment method is proposed. This procedure is designed to pair wise generate optimally aligned signatures by back tracking along the optimal alignment path. Classification results on these prototype signatures show that this method is quite robust in classifying the signals with unequal duration, compared to nearest mean classifier. Classification results were observed for different MSSNR for both classification methods. This paper also focused on reconstructing signatures based on the alignment path.
ER  - 


TY  - JOUR
TI  - Efficient classification of ISAR images using 2d fourier transform and polar mapping
T2  - IEEE Transactions on Aerospace and Electronic Systems
SP  - 1726
EP  - 1736
AU  - S. -h. Park
AU  - J. -h. Jung
AU  - S. -h. Kim
AU  - K. -t. Kim
PY  - 2015
KW  - Frequency-domain analysis
KW  - Training
KW  - Image coding
KW  - Fourier transforms
KW  - Feature extraction
KW  - Marine vehicles
KW  - Correlation
DO  - 10.1109/TAES.2015.140184
JO  - IEEE Transactions on Aerospace and Electronic Systems
IS  - 3
SN  - 1557-9603
VO  - 51
VL  - 51
JA  - IEEE Transactions on Aerospace and Electronic Systems
Y1  - July 2015
AB  - This paper proposes an efficient method to classify inverse synthetic aperture radar (ISAR) images. The proposed method achieves invariance to translation and rotation of ISAR images by using two-dimensional (2D) Fourier transform (FT) of ISAR images, polar mapping of the 2D FT image, and a simple nearest-neighbor classifier. In simulations using ISAR images measured in a compact range, the proposed method yielded high classification ratios with small-sized data regardless of the location of the rotation center, whereas the existing method was very sensitive to the location of it.
ER  - 


TY  - CONF
TI  - A Decision Fusion Framework for ISAR Images Recognition Based On Scattering Center and Zernike Moment
T2  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
SP  - 6442
EP  - 6445
AU  - P. Tang
AU  - B. Long
AU  - F. Wang
PY  - 2023
KW  - Image recognition
KW  - Target recognition
KW  - Scattering
KW  - Imaging
KW  - Geoscience and remote sensing
KW  - Radar imaging
KW  - Inverse synthetic aperture radar
KW  - ISAR
KW  - scattering center
KW  - Zernike Moment
KW  - decision fusion
KW  - target recognition
DO  - 10.1109/IGARSS52108.2023.10283047
JO  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 16-21 July 2023
AB  - Under the condition of large difference in imaging angle, Inverse Synthetic Aperture Radar (ISAR) images vary greatly and are difficult to identify. To solve this problem, a decision fusion framework based on scattering centers and Zernike Moment is proposed. In this framework, each feature corresponds to a classifier, then these classifiers are combined together by assigning different weights which reflect the ability of the classifiers. Experiments conducted on simulated ISAR images demonstrate that the proposed method can combine the outputs of the classifiers effectively and achieve better performance than single classifier.
ER  - 


TY  - CONF
TI  - Learn More: Sub-Significant Area Learning for Fine-Grained Visual Classification
T2  - 2023 IEEE International Conference on Image Processing (ICIP)
SP  - 485
EP  - 489
AU  - W. Pan
AU  - S. Yang
AU  - X. Qian
AU  - J. Lei
AU  - S. Zhang
PY  - 2023
KW  - Visualization
KW  - Fuses
KW  - Annotations
KW  - Image processing
KW  - Semantics
KW  - Dogs
KW  - Automobiles
KW  - Attention mechanism
KW  - Feature fusion
KW  - Fine-grained visual classification
KW  - Deep learning
DO  - 10.1109/ICIP49359.2023.10222241
JO  - 2023 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Image Processing (ICIP)
Y1  - 8-11 Oct. 2023
AB  - Fine-grained visual classification is more challenging as a subtask of image classification due to the large intra-class and slight inter-class variations. Recent work has focused on localizing discriminative features using attentional mechanisms. However, attention tends to focus on the salient parts of feature maps, ignoring other regions that are not salient but are discriminative for fine-grained classification. In this regard, we propose an efficient method called Discriminative Region Learning Dual-Branch Attention Network (DAL-Net) to address this problem. We propose: (1)Avoid over-focusing on local features by pixel-level attention wipe on salient features. (2)The features are enhanced and suppressed by the channel space enhancement module to mine multiple discriminative regions. (3)To learn complementary semantic information, we fuse cross-regional discriminative features from another branch. Our method can be trained end-to-end without bounding boxes and annotations. We demonstrate that our method can obtain competitive results on the CUB200-2011, FGVC-Aircraft, Stanford Cars, and Stanford Dogs datasets through comprehensive experiments.
ER  - 


TY  - JOUR
TI  - Aircraft Detection and Classification Based on Joint Probability Detector Integrated With Scattering Attention
T2  - IEEE Transactions on Aerospace and Electronic Systems
SP  - 1722
EP  - 1739
AU  - X. Xiao
AU  - H. Jia
AU  - Q. Wang
AU  - P. Xiao
AU  - C. Fan
AU  - Z. Li
AU  - H. Wang
PY  - 2024
KW  - Aircraft
KW  - Radar polarimetry
KW  - Scattering
KW  - Detectors
KW  - Airports
KW  - Feature extraction
KW  - Image recognition
DO  - 10.1109/TAES.2023.3342798
JO  - IEEE Transactions on Aerospace and Electronic Systems
IS  - 2
SN  - 1557-9603
VO  - 60
VL  - 60
JA  - IEEE Transactions on Aerospace and Electronic Systems
Y1  - April 2024
AB  - Synthetic aperture radar (SAR) image intelligence interpretation is always a challenging task due to its special imaging mechanism. This article focuses on the aircraft detection and recognition in large-scale SAR images. The objects in large-size SAR images are usually sparsely distributed and tend to be highly concentrated in certain areas. Therefore, the detection and classification in SAR images are affected easily by the imaging environment, leading to ineffective and instability. It suffers from time-consuming, high false-alarm rate, and low recognition rate. To alleviate these problems, a joint probability detector integrated with scattering attention (SA) is designed for aircraft detection and classification. Based on the fact that all aircraft are parked in the airport area, an airport detection method with mixed strategy is proposed to obtain the valid region (airport), narrowing down the scope of detection area, increasing the detection efficiency, as well as suppressing a lot of false alarms outside the airport. Considering that it is easy to generate false alarms within the airport on account of clutter scattering interference, a two-stage detector is introduced to improve the accuracy by replacing a stronger region proposal network and optimizing a lower bound to a joint probabilistic objective over two stages. Furthermore, the classification encoder module incorporates SA into a convolution neural network to leverage the distribution relation among strong scattered points to enhance the classification performance. Finally, extensive experiments on the Zhong Ke Xing Tu (ZKXT) and GaoFen-3 datasets verify the effectiveness and superiority of the proposed approach, and surpass runner up by 5.3% with the mF1 indicator. On a large GaoFen3 image (16 550 × 11 945 pixels), the proposed method improves the detection speed by 187.52%, without sacrificing the accuracy.
ER  - 


TY  - JOUR
TI  - Advanced Hyperspectral Image Classification via Spectral–Spatial Redundancy Reduction and TokenLearner-Enhanced Transformer
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 12
AU  - J. Wu
AU  - J. Zhao
AU  - H. Long
PY  - 2025
KW  - Feature extraction
KW  - Transformers
KW  - Termination of employment
KW  - Data mining
KW  - Principal component analysis
KW  - Iron
KW  - Logic gates
KW  - Hyperspectral imaging
KW  - Convolutional neural networks
KW  - Robustness
KW  - Adaptive fusion
KW  - feature redundancy
KW  - hyperspectral image (HSI) classification
KW  - self-attention
KW  - TokenLearner
DO  - 10.1109/TGRS.2025.3541879
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 63
VL  - 63
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2025
AB  - Currently, hyperspectral image (HSI) classification methods based on deep learning (DL) are extensively researched. However, the unique imaging characteristics of HSIs introduce a significant amount of spectral-spatial information redundancy, which creates challenges for existing methods in fully processing and utilizing this information. Furthermore, the classical patch-based data augmentation introduces heterogeneous pixels, posing a risk of losing central target information. To tackle these challenges, we propose a novel HSI classification network via spectral-spatial redundancy reduction and TokenLearner-enhanced transformer, named SSRRTL. Leveraging the inherent advantages of convolutional neural networks (CNNs) and transformer architectures, we innovatively introduce three key modules: spectral redundancy reduction and multiscale information extraction (SRRMIE) module, gated spatial feature enhancement (GSFE) module, and TokenLearner module. The SRRMIE module focuses on extracting multiscale features, reducing spectral redundancy, and preserving discriminative features of minor components through separation, feature extraction (FE), and adaptive fusion strategies. To further handle spatial information redundancy, the GSFE module employs a gated mechanism to enhance the extraction of key spatial features and suppress the expression of redundant spatial features. In addition, the TokenLearner method within the transformer architecture integrates feature tokens with central target tokens to enhance semantic feature expression and stabilize central target information, thereby improving the model’s robustness. Experimental results on multiple datasets confirm the effectiveness of SSRRTL and its superiority over other state-of-the-art methods.
ER  - 


TY  - CONF
TI  - Corona Discharge Classification Based on UAV Data Acquisition
T2  - 2017 21st International Conference on Control Systems and Computer Science (CSCS)
SP  - 690
EP  - 695
AU  - C. Vidan
AU  - M. Maracine
PY  - 2017
KW  - Corona
KW  - Streaming media
KW  - Discharges (electric)
KW  - Unmanned aerial vehicles
KW  - Software
KW  - Feature extraction
KW  - Image segmentation
KW  - Corona discharge
KW  - UAV
KW  - data classification
KW  - data acquisition
KW  - image processing
KW  - machine learning
DO  - 10.1109/CSCS.2017.106
JO  - 2017 21st International Conference on Control Systems and Computer Science (CSCS)
IS  - 
SN  - 2379-0482
VO  - 
VL  - 
JA  - 2017 21st International Conference on Control Systems and Computer Science (CSCS)
Y1  - 29-31 May 2017
AB  - The Corona discharge represents one of the main concerns of the century in the field of electricity and powerline design, as it is both harmful for the environment as well as very costly to cover for the voltage losses on the transport. In this paper, we propose the design for a measurement and analysis platform for this phenomenon that can estimate the Corona discharge losses based on a video stream coming from an electro-optical sensor placed on board of an Unmanned Aerial Vehicle (UAV). After experimenting on a dataset generated from Corona discharge videos, our results suggest that of the supervised learning algorithms taken into consideration those based on Radial-Basis approaches are the most effective in classifying the information being fed to it. Before the classification task, we propose a starter set of filtering operation, composed of grayscale reduction, mean blurring and binary transformation.
ER  - 


TY  - JOUR
TI  - Aircraft Identification by Moment Invariants
T2  - IEEE Transactions on Computers
SP  - 39
EP  - 46
AU  - S. A. Dudani
AU  - K. J. Breeding
AU  - R. B. McGhee
PY  - 1977
KW  - Aircraft
KW  - Distance measurement
KW  - Training
KW  - Optical imaging
KW  - Cameras
KW  - Object recognition
KW  - Feature extraction
KW  - Bayes decision rule
KW  - boundary
KW  - moment invariants
KW  - nearest neighbor rule
KW  - pattern recognition
KW  - recognition accuracy
KW  - silhouette
KW  - three-dimensional objects
DO  - 10.1109/TC.1977.5009272
JO  - IEEE Transactions on Computers
IS  - 1
SN  - 1557-9956
VO  - C-26
VL  - C-26
JA  - IEEE Transactions on Computers
Y1  - Jan. 1977
AB  - Although many systems for optical reading of printed matter have been developed and are now in wide use, comparatively little success has been achieved in the automatic interpretation of optical images of three-dimensional scenes. This paper is addressed to the latter problem and is specifically concerned with automatic recognition of aircraft types from optical images. An experimental system is described in which certain features called moment invariants are extracted from binary television images and are then used for automatic classification. This experimental system has exhibited a significantly lower error rate than human observers in a limited laboratory test involving 132 images of six aircraft types. Preliminary indications are that this performance can be extended to a wider class of objects and that identification can be accomplished in one second or less with a small computer.
ER  - 


TY  - JOUR
TI  - Automatic Detection of Geospatial Objects Using Taxonomic Semantics
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 23
EP  - 27
AU  - X. Sun
AU  - H. Wang
AU  - K. Fu
PY  - 2010
KW  - Object detection
KW  - Image segmentation
KW  - Remote sensing
KW  - Robustness
KW  - Sun
KW  - Classification tree analysis
KW  - Image recognition
KW  - Image analysis
KW  - Unsupervised learning
KW  - Layout
KW  - Image analysis
KW  - image segmentation
KW  - object detection
KW  - unsupervised learning
DO  - 10.1109/LGRS.2009.2027139
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 1
SN  - 1558-0571
VO  - 7
VL  - 7
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - Jan. 2010
AB  - In this letter, we propose a novel method to solve the problem of detecting geospatial objects present in high-resolution remote sensing images automatically. Each image is represented as a segmentation tree by applying a multiscale segmentation algorithm at first, and all of the tree nodes are described as coherent groups instead of binary classified values. The trees are matched to select the maximally matched subtrees, denoted as common subcategories. Then, we organize these subcategories to learn the embedded taxonomic semantics of objects categories, which allow categories to be defined recursively, and express both explicit and implicit spatial configuration of categories. Detection, recognition, and segmentation of the geospatial objects in a new image can be simultaneously conducted by using the learned taxonomic semantics. This procedure also provides a meaningful explanation for image understanding. Experiments for complex and compound objects demonstrate the precision, robustness, and effectiveness of the proposed method.
ER  - 


TY  - CONF
TI  - Ship Classification in Remote Sensing Images using FastAI
T2  - 2021 13th International Conference on Knowledge and Systems Engineering (KSE)
SP  - 1
EP  - 6
AU  - C. Roungroongsom
AU  - O. Chitsobhuk
PY  - 2021
KW  - Training
KW  - Transfer learning
KW  - Benchmark testing
KW  - Optical imaging
KW  - Optical sensors
KW  - Convolutional neural networks
KW  - Marine vehicles
KW  - convolutional neural network (CNN)
KW  - fastai
KW  - remote sensing image (RSI)
KW  - ship classification
DO  - 10.1109/KSE53942.2021.9648787
JO  - 2021 13th International Conference on Knowledge and Systems Engineering (KSE)
IS  - 
SN  - 2694-4804
VO  - 
VL  - 
JA  - 2021 13th International Conference on Knowledge and Systems Engineering (KSE)
Y1  - 10-12 Nov. 2021
AB  - Specifying ship categories in waterways plays an important role in the field of marine surveillance, especially when classification is performed from satellite images due to the advancement in remote sensing technologies. In this paper, we presented an approach for ship classification of optical remote sensing images. Our approach was based on two aspects, modifying models and applying additional techniques to improve accuracy of classification. Two pretrained models, MobileNetV2 and DenseNet121, were modified in this work and all techniques were implemented using Fastai library. To illustrate the effectiveness of our approach, we compared the accuracy of the modified models to the original one. A public Dataset for Ship Classification in Remote sensing images (DSCR), containing six military ship types and a civilian ship type, was used for evaluation. The results showed that our modified DenseNet121 achieved the best accuracy at 99.52% and also outperformed the benchmark result of ResNet101 reported from the original dataset.
ER  - 


TY  - CONF
TI  - Aircraft pose estimation based on polar function descriptor
T2  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
SP  - 1702
EP  - 1706
AU  - W. Ling
AU  - C. Xing
AU  - J. Yan
PY  - 2012
KW  - Aircraft
KW  - Estimation
KW  - Cameras
KW  - Solid modeling
KW  - Transforms
KW  - Feature extraction
KW  - Shape
KW  - Mathematical morphological algorithm
KW  - Radon transform
KW  - Pose estimation
DO  - 10.1109/FSKD.2012.6234157
JO  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery
Y1  - 29-31 May 2012
AB  - The problem of aircraft pose estimating in a single image is analyzed. A novel method combined mathematical morphological algorithm and Radon transform is introduced to detect classical aircraft pose using the feature of shapes and regions. Silhouettes obtained by image segmentation are simplified by mathematical morphological algorithm, and Radon transform is used to do line detection for the simplified image. Tow axises of the aircraft are determined by the feature of fuselage and wings, and pose of the aircraft are estimated by the angle of the lines. Experimental results show the effectiveness of the algorithm for aircraft pose estimation in a gray level image.
ER  - 


TY  - CONF
TI  - A Comparison of Forest Classification using Hyperion and AVIRIS Hyperspectral Imagery
T2  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
SP  - 1956
EP  - 1959
AU  - J. Cipar
AU  - T. Cooley
AU  - R. Lockwood
PY  - 2006
KW  - Hyperspectral imaging
KW  - Hyperspectral sensors
KW  - Clustering algorithms
KW  - Testing
KW  - Spatial resolution
KW  - Signal to noise ratio
KW  - Laboratories
KW  - Classification algorithms
KW  - Aircraft
KW  - Vegetation mapping
DO  - 10.1109/IGARSS.2006.506
JO  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
Y1  - 31 July-4 Aug. 2006
AB  - We test how well a cluster-based unsupervised classification algorithm separates forest land covers. Our test data, Hyperion and AVIRIS images taken in northern Virginia during autumn, provide two spectrally distinct land covers: pine forests and senescent deciduous forests. We find that the algorithm successfully separates these land covers for AVIRIS data that has been spatially aggregated to simulate 30-m Hyperion GSD. The algorithm does not successfully separate the land covers for the Hyperion data.
ER  - 


TY  - CONF
TI  - Learning Two-level Features for Fine-grained Image Classification
T2  - 2018 14th IEEE International Conference on Signal Processing (ICSP)
SP  - 544
EP  - 549
AU  - J. Ji
AU  - L. Jiang
AU  - C. Lei
AU  - W. Zhong
AU  - H. Xiong
PY  - 2018
KW  - Task analysis
KW  - Dogs
KW  - Automobiles
KW  - Birds
KW  - Training
KW  - Neurons
KW  - Manuals
KW  - fine-grained
KW  - visual attention
KW  - multi-scale regions
KW  - pre-trained
DO  - 10.1109/ICSP.2018.8652320
JO  - 2018 14th IEEE International Conference on Signal Processing (ICSP)
IS  - 
SN  - 2164-5221
VO  - 
VL  - 
JA  - 2018 14th IEEE International Conference on Signal Processing (ICSP)
Y1  - 12-16 Aug. 2018
AB  - Fine-grained images categorization is a challenging task due to the difficulty of recognizing the subtle difference among the sub-categories. Existing approaches mainly focus on using the manual annotations or the attention algorithm to localize the discriminative regions and have achieved impressive performance. But many methods regard these multi-scale regions equally even they may contain only one small part of the object. This may decrease the performance of the network. To address the problem, we propose the two-level attention network to generate object-level and part-level regions by a CNN model which has been fine-tuned by the full-size image train set of current task. We also train two subnetworks which combine these two-level features together and achieve better performance than two individual networks. The method consists of two parts: the first part is the attention network which localizes the area of the target in the image and its discriminative regions; the second part is the classification network which learns the two-level feature representations. We conduct experiments on the CUB-200-2011, Stanford Dogs, Stanford Cars and FGVC-Aircraft dataset to evaluate the effectiveness of the proposed method.
ER  - 


TY  - CONF
TI  - Aircraft Rotated Boxes Detection Method Based on YOLOv5
T2  - 2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)
SP  - 390
EP  - 394
AU  - X. Baiqi
AU  - J. Gangwu
AU  - L. Jianhui
AU  - W. Xin
AU  - Y. Peidong
PY  - 2021
KW  - Training
KW  - Smoothing methods
KW  - Convolution
KW  - Feature extraction
KW  - Neck
KW  - Pattern recognition
KW  - Aircraft
KW  - remote sensing image
KW  - aircraft target
KW  - rotated boxes detection
KW  - circular smooth label
KW  - involution
KW  - coordinate attention
DO  - 10.1109/PRAI53619.2021.9551072
JO  - 2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)
Y1  - 20-22 Aug. 2021
AB  - Aiming at the problem of arbitrary alignment direction of aircraft targets in remote sensing images, a rotation frame detection method for aircraft targets is proposed based on YOLOv5. First, the circular smoothing label is introduced in the network classification regression to process the target angle information to avoid the abrupt change of angle value; then, the convolution operation in the backbone network is replaced by involution to reduce the number of network parameters; finally, Coordinate Attention is introduced after the Neck part to remove the background information from the features. The improved network is trained and tested on the aircraft target in the DOTA dataset, and the AP value is 90.68%, which is higher than other rotating frame detection algorithms, verifying the effectiveness of the network in this paper.
ER  - 


TY  - CONF
TI  - An Aircraft Landing Gear State Detection and Recognition Method based on YOLO-V4
T2  - 2023 9th International Conference on Computer and Communications (ICCC)
SP  - 2084
EP  - 2088
AU  - M. Wu
AU  - P. Zhu
AU  - K. Wang
AU  - W. Luo
AU  - T. Chen
AU  - K. Xue
PY  - 2023
KW  - Training
KW  - Deep learning
KW  - Image recognition
KW  - Gears
KW  - Streaming media
KW  - Robustness
KW  - Real-time systems
KW  - Landing gear state recognition
KW  - Landing gear detection
KW  - YOLO-V4
KW  - single-stage
DO  - 10.1109/ICCC59590.2023.10507637
JO  - 2023 9th International Conference on Computer and Communications (ICCC)
IS  - 
SN  - 2837-7109
VO  - 
VL  - 
JA  - 2023 9th International Conference on Computer and Communications (ICCC)
Y1  - 8-11 Dec. 2023
AB  - To solve the problems that most landing gear state detection methods based on image recognition or detection technology need to be divided into in multiple stages, which leads to low efficiency and weak image generalization ability for complex background and different acquisition angles, an single-stage aircraft landing gear state detection and recognition method based on YOLO-V4 is proposed. By adding landing gear state labels to aircraft samples, adding landing gear state output channels to the network, and cleverly designing the loss functions for training. The proposed method makes the detection results of visible landing gears in the corresponding viewing angle and the landing gear state of the aircraft object can be obtained at the same time, which has stronger generalization ability for images from different viewing angles and is easier to be applied in engineering. The experiment on self-made aircraft dataset shows that the proposed method is not only slightly higher than the two-stage method in the average accuracy rate of aircraft landing gear state recognition, but also the average accuracy of aircraft and landing gear detection is 98.92% and 76.00% respectively, which meets the requirements of practical application.
ER  - 


TY  - JOUR
TI  - Ideal Regularized Discriminative Multiple Kernel Subspace Alignment for Domain Adaptation in Hyperspectral Image Classification
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 5833
EP  - 5846
AU  - W. Yang
AU  - J. Peng
AU  - W. Sun
PY  - 2020
KW  - Kernel
KW  - Hyperspectral imaging
KW  - Sun
KW  - Task analysis
KW  - Principal component analysis
KW  - Standards
KW  - Domain adaptation (DA)
KW  - hyperspectral image (HSI) classification
KW  - ideal regularization (IR)
KW  - multiple kernel learning (MKL)
KW  - subspace alignment (SA)
DO  - 10.1109/JSTARS.2020.3026316
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 13
VL  - 13
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2020
AB  - This article proposes a novel unsupervised domain adaptation (DA) method called ideal regularized discriminative multiple kernel subspace alignment (IRDMKSA) for hyperspectral image (HSI) classification. The proposed IRDMKSA method includes three main steps: ideal regularization, discriminative multiple kernel learning, and subspace alignment. The ideal regularization strategy exploits label information of source domain to refine the standard source and target kernels and also to build a connection between them. The discriminative multiple kernel learning can learn a composite kernel to describe the nonlinearity of HSI samples by fusing complementary information among different single kernels. Finally, the subspace alignment is used to diminish the difference between source and target composite kernels. The proposed IRDMKSA method exploits both the sample similarity and label similarity and makes the resulting kernel more appropriate for DA tasks. Experimental results on four DA tasks show that the performance of IRDMKSA is better than some classical unsupervised DA methods for the HSI classification.
ER  - 


TY  - JOUR
TI  - Aircraft Wake Recognition and Strength Classification Based on Deep Learning
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 2237
EP  - 2249
AU  - C. Shen
AU  - W. Tang
AU  - H. Gao
AU  - X. Wang
AU  - P. -W. Chan
AU  - K. -K. Hon
AU  - J. Li
PY  - 2023
KW  - Aircraft
KW  - Laser radar
KW  - Classification algorithms
KW  - Deep learning
KW  - Air traffic control
KW  - Doppler effect
KW  - Real-time systems
KW  - Deep learning
KW  - YOLO
KW  - Air traffic management (ATM)
KW  - deep learning
KW  - recognition
KW  - strength classification
KW  - wake vortex
KW  - YOLOv5s network
DO  - 10.1109/JSTARS.2023.3243941
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 16
VL  - 16
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2023
AB  - Aircraft wake is a pair of counter-rotating vortices generated behind the aircraft, which can greatly impact the safety of fast takeoff and landing of aircraft and limit the improvement of airport capacity. The current wake parameter retrieval methods cannot locate the wake vortex's position and estimate its strength level in real time. To deal with this issue, a novel algorithm based on the YOLOv5s deep learning network is proposed. The new algorithm establishes a single vortex locating concept to adapt the wake vortex's evolution at complicate background wind field conditions, and proposes strength-based classification standard which can represent the real-time hazard of wake vortex to shorten the takeoff and landing intervals. Meanwhile, the EIOU loss function is introduced to improve the precision of YOLOv5s network. Compared with the state-of-the-art object detection approaches, such as Cascade R-CNN, FCOS, and YOLOv5l, the superiority of new method is demonstrated in terms of accuracy and robustness by using the field detection data from Hong Kong International Airport.
ER  - 


TY  - JOUR
TI  - CGC-NET: Aircraft Detection in Remote Sensing Images Based on Lightweight Convolutional Neural Network
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 2805
EP  - 2815
AU  - T. Wang
AU  - X. Zeng
AU  - C. Cao
AU  - W. Li
AU  - Z. Feng
AU  - J. Wu
AU  - X. Yan
AU  - Z. Wu
PY  - 2022
KW  - Aircraft
KW  - Feature extraction
KW  - Object detection
KW  - Detection algorithms
KW  - Military aircraft
KW  - Classification algorithms
KW  - Support vector machines
KW  - Circle frequency filter (CFF)
KW  - deep learning framework
KW  - few-shot learning
KW  - lightweight convolutional neural network (CNN)
KW  - small samples
DO  - 10.1109/JSTARS.2022.3159981
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 15
VL  - 15
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2022
AB  - In the past few years, aircraft detection in remote sensing (RS) images has been an important research hotspot, and it is very crucial in plenty of military applications. Based on the high computational cost of the model and numerous parameters, deep convolution neural networks-based algorithms have excellent performance in the aircraft detection task. However, it is still difficult to detect aircraft due to the complex background of RS images, various types of aircraft, and so on. In addition, it is difficult and costly to make labels for satellite-based optical RS images. Consequently, we propose an end-to-end lightweight aircraft detection framework called CGC-NET (a network based on circle grayscale characteristics), which can accurately detect aircraft with a few training samples. There are only a small number of trainable parameters in CGC-NET, which greatly reduces the need for large datasets. Extensive evaluations indicate the excellent performance of CGC-NET, in which the F-score can reach 91.06% and the model size is only 0.88 M. Therefore, CGC-NET can be used to accurately detect aircraft targets simply and effectively.
ER  - 


TY  - CONF
TI  - Electronic Image Stabilization based on Laplacian Bit Plane and Matching Block Classification
T2  - 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)
SP  - 787
EP  - 793
AU  - F. Tian
AU  - H. Li
AU  - Y. Zeng
PY  - 2019
KW  - Laplace equations
KW  - Jitter
KW  - Motion estimation
KW  - Image edge detection
KW  - Optical flow
KW  - Data mining
KW  - Current measurement
KW  - electronic image stabilization
KW  - laplacian
KW  - bit plane
DO  - 10.1109/IAEAC47372.2019.8998003
JO  - 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)
IS  - 
SN  - 2381-0947
VO  - 1
VL  - 1
JA  - 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)
Y1  - 20-22 Dec. 2019
AB  - In this paper, an electronic image stabilization method based on Laplacian bit plane and matching block classification is proposed. The method uses Laplacian to extract the edge information of the image to construct bit plane. Weight is used to measure the feasibility of the vector becoming local motion vector and global motion vector. The matching result of each matching block and the global motion vector of the previous frames are combined to obtain the global motion of the current frame, which is used to compensate for frame motion to obtain a stable image. Finally, it is proved by experiment that the algorithm proposed in this paper requires a small amount of calculation and can eliminate the interference of small objects in the field of view. For CIF images, the algorithm can achieve real-time image processing, removing and reducing the jitter of the image. Compared with traditional bit plane matching electronic image stabilization methods, this algorithm lays stress on the full utilization of gray information and the adaptability of the algorithm when there is a small foreground object in the field of view.
ER  - 


TY  - CONF
TI  - Deep Learning with Semi-Synthetic Training Images for Detection of Non-Cooperative UAVs
T2  - 2019 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 981
EP  - 988
AU  - C. Briese
AU  - L. Guenther
PY  - 2019
KW  - Training
KW  - Aircraft
KW  - Training data
KW  - Unmanned aerial vehicles
KW  - Cameras
KW  - Neural networks
KW  - Task analysis
DO  - 10.1109/ICUAS.2019.8797731
JO  - 2019 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2019 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 11-14 June 2019
AB  - This paper presents a method to generate a dataset for training a deep convolutional network to detect a non cooperative unmanned aerial vehicle in video data. Deep convolutional network have shown a great potential for tasks like object detection and have been continuously improved in the last years. Still, the amount of training data is large and their generation can be complex and time consuming, especially if the appearance of the detected object is not clearly specified. The concept presented here is to train a deep convolutional neural network just with a few two dimensional images of unmanned aerial vehicle to simplify the process of generating training data. Performance of the trained network is evaluated with data from real experimental flights and compared with hand-labeled ground truth data to validate the correctness. To cover situations when the classifier fails at the detection, the output is integrated in a image processing pipeline for object tracking in order to establish a continuous tracking.
ER  - 


TY  - CONF
TI  - Multi-Granularity Feature Distillation Learning Network for Fine-Grained Visual Classification
T2  - 2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)
SP  - 300
EP  - 303
AU  - Y. Cai
AU  - X. Ke
PY  - 2022
KW  - Representation learning
KW  - Visualization
KW  - Cross layer design
KW  - Computer vision
KW  - Image processing
KW  - Semantics
KW  - Birds
KW  - Fine-grained visual classification
KW  - Multi-granularity feature learning
KW  - Knowledge distillation
DO  - 10.1109/ICICML57342.2022.10009666
JO  - 2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)
Y1  - 28-30 Oct. 2022
AB  - Fine-grained visual classification (FGVC) aims to identify objects belonging to multiple sub-categories of the same super-category (such as species of birds, models of cars and aircraft). The key to solving fine-grained classification problems is to learn discriminative visual feature representation with only subtle differences. Although previous work based on refined feature learning has made great progress, however, high-level semantic features often lack key information for fine-grained visual object nuances. How to efficiently integrate semantic information of different granularities from classification networks is a Critical. In this paper, we propose Multi-Granularity Feature Distillation Learning Network (MGFDL-Net). Our solution integrates multi-granularity hierarchical information through a multi-granularity fusion learning strategy to enhance feature representation. In view of the inherent challenges of large intraclass differences in FGVC, a cross-layer self-distillation regularization is proposed to strengthen the connection between high-level semantics and low-level semantics for robust multi-granularity feature learning. Comprehensive experiments show that our method achieves state-of-the-art performance on three challenging fine-grained visual classification FGVC datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft).
ER  - 


TY  - CONF
TI  - The application of one improved segmentation technology in the aero-engine endoscope detection
T2  - 2010 Chinese Control and Decision Conference
SP  - 3886
EP  - 3890
AU  - Yunlin Luo
AU  - Xiaowei Zhang
PY  - 2010
KW  - Endoscopes
KW  - Color
KW  - Image segmentation
KW  - Pixel
KW  - Engines
KW  - Clustering methods
KW  - Concrete
KW  - Aircraft propulsion
KW  - Automation
KW  - Educational institutions
KW  - Pixel Technology
KW  - Image Segmentation
KW  - Rejection Clustering
KW  - Endoscope Detection
DO  - 10.1109/CCDC.2010.5498469
JO  - 2010 Chinese Control and Decision Conference
IS  - 
SN  - 1948-9447
VO  - 
VL  - 
JA  - 2010 Chinese Control and Decision Conference
Y1  - 26-28 May 2010
AB  - This paper is based on the method of clustering technology, proposed a simple and effective method for color image segmentation. Using the color image segmentation techniques and morphological post-processing techniques in the target search and segmentation process. We proposed a rejection clustering method to the extraction of the target. The result of the experiment that this paper proposed has proven its effectiveness, and can be better used in aviation engine's endoscope detection.
ER  - 


TY  - JOUR
TI  - Automatic Perception of Aircraft Taxiing Behavior via Laser Rangefinders and Machine Learning
T2  - IEEE Sensors Journal
SP  - 3964
EP  - 3973
AU  - P. Li
AU  - S. Liu
AU  - Y. Tian
AU  - T. Hou
AU  - J. Ling
PY  - 2025
KW  - Aircraft
KW  - Measurement by laser beam
KW  - Monitoring
KW  - Sensors
KW  - Laser theory
KW  - Distance measurement
KW  - Airports
KW  - Wheels
KW  - Air traffic control
KW  - Machine learning
KW  - Aircraft taxiing behavior
KW  - aircraft type identification
KW  - laser rangefinders
KW  - lateral deviation
KW  - machine learning
DO  - 10.1109/JSEN.2024.3510568
JO  - IEEE Sensors Journal
IS  - 2
SN  - 1558-1748
VO  - 25
VL  - 25
JA  - IEEE Sensors Journal
Y1  - 15 Jan.15, 2025
AB  - Understanding aircraft taxiing behavior, including type, speed, and lateral deviation, is essential for optimizing runway design and enhancing airport management. This study introduces a novel monitoring system deploying three laser rangefinders, coupled with computational models and machine-learning algorithms, to evaluate aircraft taxiing behavior automatically. Field tests at Chengdu Tianfu International Airport demonstrated the system’s effectiveness. Identifying aircraft types based on landing gear wheel span can be challenging due to overlapping measurements between certain classes. To address this, a classification model based on convolutional neural network (CNN) model is developed and validated by onboard radar data, which achieved an 80% of accuracy rate in aircraft type identification. Analysis of taxiing speeds during take-off and landing revealed significant variations influenced by runway direction, with longer acceleration or deceleration distances leading to broader speed distributions. Lateral deviation analysis indicated a positive skew in wheel track distributions, suggesting a tendency for aircraft to drift toward one side of the runway. The proposed techniques provide a simple and reliable way for aircraft taxiing behavior monitoring, and the findings offer valuable insights for improving runway design and airport safety.
ER  - 


TY  - JOUR
TI  - Context-Driven Automatic Target Detection With Cross-Modality Real-Synthetic Image Merging
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 5600
EP  - 5618
AU  - Z. Geng
AU  - S. Zhang
AU  - C. Xu
AU  - H. Zhou
AU  - W. Li
AU  - X. Yu
AU  - D. Zhu
AU  - G. Zhang
PY  - 2025
KW  - Radar polarimetry
KW  - Feature extraction
KW  - Object detection
KW  - Synthetic aperture radar
KW  - Remote sensing
KW  - Target recognition
KW  - Training
KW  - Solid modeling
KW  - Image resolution
KW  - Data integration
KW  - Multimodal data fusion
KW  - target detection
KW  - unmanned aerial vehicles
DO  - 10.1109/JSTARS.2025.3531788
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 18
VL  - 18
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2025
AB  - This article presents pioneer research on joint scene-target analysis and proposes novel cross-modality real-synthetic target feature fusion method. To begin, multisensor remote sensing images are jointly leveraged for geographical region classification. After that, a novel Context-Aware Region Masking and Situation AWareness (CARMSAW) strategy is employed for target classification based on the inherent target properties and capabilities reflected by SAR and infrared (IR) imagery, and the cross-modality Real-synthetic Image Merging (CRIM) strategy is employed for feature enhancement. Specifically, to tackle with the random deviations of the real SAR imagery from the ideal ones, the synthetic SAR signature generated based on the target CAD model is treated as a “skeleton” with known structure for real-sync target feature alignment. To facilitate the recognition of aircrafts, we leverage on the IR images to construct an “exoskeleton” for the target SAR signature, so that the dimension/shape/contour of the target and its electromagnetic features are united. Furthermore, we propose a novel color-guided component-level attention mechanism, in which the SAR image is partitioned into several subregions highlighted or blacked-out adaptively based on their significance level. To demonstrate the effectiveness of the proposed CARMSAW strategy, a series of experiments are carried out based on the SAR-optical image pairs from the SEN1-2 dataset, the SpaceNet6 dataset, and a self-constructed ship detection dataset featuring the Port of Rotterdam. To verify the performance the proposed CRIM method, experiment results based on both the self-constructed SAR-IR dataset and the MSTAR-SAMPLE dataset in the public domain are provided.
ER  - 


TY  - CONF
TI  - Optimal Collection of High Resolution Aerial Imagery with Unmanned Aerial Systems
T2  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 89
EP  - 94
AU  - B. Stark
AU  - Y. Chen
PY  - 2014
KW  - Estimation
KW  - Sun
KW  - Azimuth
KW  - Minimization
KW  - Optimization
KW  - Vegetation mapping
KW  - Image resolution
KW  - Unmanned Aerial System
KW  - remote sensing
KW  - rangeland management
KW  - natural resource management
KW  - imagery optimization
DO  - 10.1109/ICUAS.2014.6842243
JO  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2014 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 27-30 May 2014
AB  - Remote sensing applications are an emerging topic for Unmanned Aerial Systems (UASs). Unlike many remote sensing image collection methods, UASs have several advantages when it comes to on demand data acquisition. Relatively low operating costs, high portability and low flight altitudes make UASs excellent tools for researchers to collect high resolution imagery where satellites or manned aircraft are inefficient. In particular areas, such as in rangelands, the use of UASs to aid in management practices could have significant benefit. However, in these areas, current methodologies of remote sensing utilizing spectral reflectance data for vegetation analysis have performed poorly due to the high spatial and low spectral heterogeneity of the area. One of the root causes of the poor performance can be traced to the negative effect of shadows that are interspersed in the spectral reflectance data. The unique advantage of low infrastructure and minimal downtime for UASs enables researchers to exert greater control over the precise time of data collection. In this paper, it is demonstrated that the time of imagery collection can be optimized with regards to the minimization of shadows found in the imagery. The process described in this paper utilizes a high resolution digital elevation map (DEM) that can be generated through photogrammetry techniques to create an estimate of shadows given a time of day at a known location. Furthermore, the results of estimated shadow map can be utilized for improving classification techniques without additional equipment.
ER  - 


TY  - CONF
TI  - Deep Learning Algorithms in Aircraft Detection and Classification: An Analytical Survey
T2  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
SP  - 568
EP  - 573
AU  - N. Alkharji
AU  - H. Almazrouei
AU  - S. Alzaabi
AU  - A. B. Nassif
AU  - M. Elsalhy
AU  - M. A. Talib
PY  - 2024
KW  - Deep learning
KW  - Measurement
KW  - Surveys
KW  - Industries
KW  - Accuracy
KW  - Feature detection
KW  - Decision making
KW  - Feature extraction
KW  - Aircraft
KW  - Detection algorithms
KW  - Deep learning
KW  - Aircraft Detection
KW  - Feature Extraction
DO  - 10.1109/DeSE63988.2024.10912016
JO  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
Y1  - 6-8 Nov. 2024
AB  - Aircraft detection has become a very crucial aspect in the aviation industry due to its vital role in identifying unauthorized aircraft trespassing into restricted areas. Due to their significant role in decision-making, these detection systems must have a high level of accuracy. In this paper, we address deep learning feature extraction and aircraft detection methods. This study aims to explore deep learning-based feature extraction techniques and airplane identification techniques. The applications examined stop unauthorized aircraft from entering restricted airspace by utilizing a range of deep learning models. The study results and basic information on feature types, image types, and detection techniques are presented in this paper.
ER  - 


TY  - CONF
TI  - Comparative analysis of polarimetric SAR image exploitation algorithms for environmental stress-change monitoring
T2  - IEEE 1999 International Geoscience and Remote Sensing Symposium. IGARSS'99 (Cat. No.99CH36293)
SP  - 2449
EP  - 2451 vol.5
AU  - J. S. Verdi
AU  - R. Lee
AU  - P. R. Kersten
AU  - S. K. Krasznay
AU  - D. R. Statter
AU  - W. . -M. Boerner
PY  - 1999
KW  - Image analysis
KW  - Algorithm design and analysis
KW  - Clustering algorithms
KW  - Covariance matrix
KW  - Aerospace electronics
KW  - Aircraft navigation
KW  - Layout
KW  - Monitoring
KW  - Rivers
KW  - Laboratories
DO  - 10.1109/IGARSS.1999.771539
JO  - IEEE 1999 International Geoscience and Remote Sensing Symposium. IGARSS'99 (Cat. No.99CH36293)
IS  - 
SN  - 
VO  - 5
VL  - 5
JA  - IEEE 1999 International Geoscience and Remote Sensing Symposium. IGARSS'99 (Cat. No.99CH36293)
Y1  - 28 June-2 July 1999
AB  - Theoretical advances in scene classification using single and multi-frequency POL-SAR techniques have provided a wealth of tools for application to important environmental issues involving ground cover classification and the discrimination of change due to natural versus man-made stress. Among these are techniques which have evolved from the Navy International Cooperative Opportunities Program (NICOP), and although initial emphasis within the present discussion has been placed on NICOP results, a broader base of algorithms is currently being evaluated, with partial results included in this paper. A portion of the multi-frequency POL-SAR data being considered is from the Southern Maryland area, and in particular the entropy based scheme of Cloude (1997, 1998) and the fuzzy c-means technique by Kersten (1997) are used as an initial baseline for this work. The three-component scattering mechanism described by Krogager et al. (1997) and other techniques are incorporated as the work continues.
ER  - 


TY  - CONF
TI  - Micro and mini drone classification based on coherent radar imaging
T2  - 2018 IEEE Radio and Wireless Symposium (RWS)
SP  - 259
EP  - 262
AU  - E. Chang
AU  - R. L. Sturdivant
AU  - B. S. Quilici
AU  - E. W. Patigler
PY  - 2018
KW  - Drones
KW  - Doppler effect
KW  - Radar imaging
KW  - Doppler radar
KW  - Radar antennas
KW  - Sensors
KW  - LSS
KW  - UAS
KW  - drone
KW  - feature classification
KW  - radar
KW  - ISAR
KW  - X-band
KW  - motion compensation
KW  - matched filter
DO  - 10.1109/RWS.2018.8305004
JO  - 2018 IEEE Radio and Wireless Symposium (RWS)
IS  - 
SN  - 2164-2974
VO  - 
VL  - 
JA  - 2018 IEEE Radio and Wireless Symposium (RWS)
Y1  - 15-18 Jan. 2018
AB  - Low, slow, and small unmanned aerial system (LSS-UAS) are a growing threat to the civil and military sectors. This paper reports the results of a feasibility study on a radar-based drone feature-classification system. A 9.6 GHz CW radar was used to acquire Doppler data. Coherent integration of the data in conjunction with motion extraction and compensation turned this conventional Doppler radar into a one-dimensional inverse synthetic aperture radar (ISAR), which produced the detailed image of a mini drone. This result paves the way for a fully intelligent classification system with multiple sensors for countering the LSS-UAS threat.
ER  - 


TY  - CONF
TI  - A Microfluidic Oil Particles Monitoring System based on Raspberry Pi
T2  - 2022 Global Reliability and Prognostics and Health Management (PHM-Yantai)
SP  - 1
EP  - 5
AU  - Z. Liu
AU  - Y. Liu
AU  - H. Zuo
AU  - H. Wang
AU  - H. Fei
AU  - Z. Jiang
PY  - 2022
KW  - Oils
KW  - Particle measurements
KW  - Cameras
KW  - Hardware
KW  - Surface texture
KW  - Aircraft propulsion
KW  - Monitoring
KW  - Raspberry Pi
KW  - microfluidic
KW  - particles
KW  - target detection
DO  - 10.1109/PHM-Yantai55411.2022.9941791
JO  - 2022 Global Reliability and Prognostics and Health Management (PHM-Yantai)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 Global Reliability and Prognostics and Health Management (PHM-Yantai)
Y1  - 13-16 Oct. 2022
AB  - The health status of an aero-engine provides the basic guarantee for the safe flight of an aircraft, and the oil monitoring technology based on oil wear particles analysis is a standard method in the field of aero-engine condition monitoring. This paper aims to design miniaturization, intelligence, and real-time monitoring equipment. Firstly, an experimental monitoring platform is built based on Raspberry Pi. Then images of moving particles flowing through the microfluidic chip are collected by image acquisition software. Finally, the target particles are accurately extracted, and parameters are calculated using significance analysis. The experimental results show that the system is easy to use and provides an efficient and accurate contour identification method, which can be used for intelligent industrial applications in aero-engines and large rotating machines.
ER  - 


TY  - CONF
TI  - Prediction of paddy field area base on aerial photography using multispectral camera
T2  - 2017 International Conference on Sustainable Information Engineering and Technology (SIET)
SP  - 425
EP  - 429
AU  - P. E. Broto
AU  - A. H. Saputro
AU  - D. Kushardono
PY  - 2017
KW  - Data acquisition
KW  - Size measurement
KW  - Image segmentation
KW  - Image classification
KW  - Remote sensing
KW  - Morphology
KW  - Informatics
KW  - Paddy field
KW  - Multispectral
KW  - Segmentation
DO  - 10.1109/SIET.2017.8304176
JO  - 2017 International Conference on Sustainable Information Engineering and Technology (SIET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 International Conference on Sustainable Information Engineering and Technology (SIET)
Y1  - 24-25 Nov. 2017
AB  - Calculation of paddy field area is necessary to know to predict a number of the paddy production in some region. In this paper, we proposed a system to predict paddy field area automatically based on aerial photography that collected using the multispectral camera. The multispectral camera is mounted on the wing of an integrated aircraft with GPS to provide position information of the resulting image. The multispectral images from the camera are then processed using texture segmentation. The threshold value is obtained by using Otsu method to convert the multispectral image into a binary image. Ground Sampling Distance (GSD) is computed by knowing the height of the aircraft flying and the field of view of the camera. GSD values can be used to calculate the area by multiplying by the number of pixels of each class. The system evaluation is performed by comparing the results of software processing with manual processing and calculated by confusion matrix. The accuracy of the system that has been made is 98%. This research was succeeded to make an automatic prediction system of paddy field area based on aerial remote sensing which the output is information of paddy field area.
ER  - 


TY  - CONF
TI  - FPGA Based Military Vehicle Classification from Drone-Based Video Using Deep Learning
T2  - 2024 9th International Conference on Frontiers of Signal Processing (ICFSP)
SP  - 32
EP  - 37
AU  - S. Vasavi
AU  - D. Sree Sowmya
AU  - C. Aishwarya
AU  - W. F. Fuentes
PY  - 2024
KW  - Array signal processing
KW  - Surveillance
KW  - Military vehicles
KW  - Logic gates
KW  - Real-time systems
KW  - Personnel
KW  - Field programmable gate arrays
KW  - Videos
KW  - Drones
KW  - Remotely piloted aircraft
KW  - Drones
KW  - Field Programmable Gate Array
KW  - Image Processing
KW  - Instance Segmentation
KW  - Military Vehicles
KW  - Object Classification
KW  - Object Detection
DO  - 10.1109/ICFSP62546.2024.10785445
JO  - 2024 9th International Conference on Frontiers of Signal Processing (ICFSP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 9th International Conference on Frontiers of Signal Processing (ICFSP)
Y1  - 12-14 Sept. 2024
AB  - Remotely controlled aerial vehicles such as drones are used for military applications such as surveillance, intelligence and target acquisition. Real-time object detection and classification is one of the most recurrent tasks for drones. The proposed system is used to classify the military vehicles from the drone-based videos. Object segmentation and classification of military vehicles (military tanks and APCs) is done using the Detectron2 and is based on Mask-RCNN benchmark. A custom dataset consisting of 300 images of Military Tanks and Armored Personnel Carriers (APCs) has been created and annotated for the purpose of the project. Furthermore, the Mask RCNN (Resnet50+FPN) model is trained with the custom dataset to classify the vehicles. The trained model is integrated in to Field Programmable Gate Arrays (FPGA), to carry out onboard classification in a real-time manner. Performance measures are used to evaluate the proposed model and overall accuracy is 93% with reduced false positive rate.
ER  - 


TY  - CONF
TI  - Classification Of Puget Sound Nearshori Habitats Using Aircraft Mulfl Spectral Scanner Imagery
T2  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
SP  - 695
EP  - 695
AU  - F. Mynar
AU  - R. D. Weerackoon
AU  - M. V. Olsen
PY  - 1989
KW  - Aircraft
KW  - Laboratories
KW  - Protection
KW  - Remote monitoring
KW  - Protocols
KW  - Information analysis
KW  - Image analysis
KW  - Marine technology
KW  - Testing
KW  - Costs
DO  - 10.1109/IGARSS.1989.578946
JO  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
Y1  - 10-14 July 1989
AB  - 
ER  - 


TY  - JOUR
TI  - Optimum Frequencies for Aircraft Classification
T2  - IEEE Transactions on Aerospace and Electronic Systems
SP  - 656
EP  - 665
AU  - H. Lin
AU  - A. A. Ksienski
PY  - 1981
KW  - Frequency
KW  - Airborne radar
KW  - Polarization
KW  - Radar imaging
KW  - Shape
KW  - Doppler radar
KW  - Electromagnetic scattering
KW  - Aircraft propulsion
KW  - Airplanes
KW  - Laboratories
DO  - 10.1109/TAES.1981.309097
JO  - IEEE Transactions on Aerospace and Electronic Systems
IS  - 5
SN  - 1557-9603
VO  - AES-17
VL  - AES-17
JA  - IEEE Transactions on Aerospace and Electronic Systems
Y1  - Sept. 1981
AB  - The selection of the optimum features for aircraft classification from radar returns was studied. A constraint was imposed that a minimum number of simultaneous frequencies be used. The results indicate that a high reliability of classification is achievable with only two frequencies, and even a single frequency can provide over 95 percent reliability if both phase and amplitude at two orthogonal polarizations are used. The data set tested involved eight military aircraft whose sizes varied over a three-to-one range.
ER  - 


TY  - JOUR
TI  - Effective distributed convolutional neural network architecture for remote sensing images target classification with a pre-training approach
T2  - Journal of Systems Engineering and Electronics
SP  - 238
EP  - 244
AU  - L. Binquan
AU  - H. Xiaohui
PY  - 2019
KW  - Training
KW  - Feature extraction
KW  - Computer architecture
KW  - Bayes methods
KW  - Task analysis
KW  - Convolutional neural networks
KW  - Remote sensing
KW  - convolutional neural network (CNN)
KW  - distributed architecture
KW  - remote sensing images (RSIs)
KW  - target classification
KW  - pre-training.
DO  - 10.21629/JSEE.2019.02.02
JO  - Journal of Systems Engineering and Electronics
IS  - 2
SN  - 1004-4132
VO  - 30
VL  - 30
JA  - Journal of Systems Engineering and Electronics
Y1  - April 2019
AB  - How to recognize targets with similar appearances from remote sensing images (RSIs) effectively and efficiently has become a big challenge. Recently, convolutional neural network (CNN) is preferred in the target classification due to the powerful feature representation ability and better performance. However, the training and testing of CNN mainly rely on single machine. Single machine has its natural limitation and bottleneck in processing RSIs due to limited hardware resources and huge time consuming. Besides, overfitting is a challenge for the CNN model due to the unbalance between RSIs data and the model structure. When a model is complex or the training data is relatively small, overfitting occurs and leads to a poor predictive performance. To address these problems, a distributed CNN architecture for RSIs target classification is proposed, which dramatically increases the training speed of CNN and system scalability. It improves the storage ability and processing efficiency of RSIs. Furthermore, Bayesian regularization approach is utilized in order to initialize the weights of the CNN extractor, which increases the robustness and flexibility of the CNN model. It helps prevent the overfitting and avoid the local optima caused by limited RSI training images or the inappropriate CNN structure. In addition, considering the efficiency of the Näıve Bayes classifier, a distributed Näıve Bayes classifier is designed to reduce the training cost. Compared with other algorithms, the proposed system and method perform the best and increase the recognition accuracy. The results show that the distributed system framework and the proposed algorithms are suitable for RSIs target classification tasks.
ER  - 


TY  - CONF
TI  - Research on Intelligent Recognition System of Crop Growth Monitoring by UAV Remote Sensing Image
T2  - 2023 IEEE 6th International Conference on Information Systems and Computer Aided Education (ICISCAE)
SP  - 696
EP  - 700
AU  - W. Xie
PY  - 2023
KW  - Image recognition
KW  - Correlation
KW  - Solid state drives
KW  - Crops
KW  - Autonomous aerial vehicles
KW  - Classification algorithms
KW  - Sensors
KW  - UAV photography
KW  - remote sensing image
KW  - crop monitoring
KW  - intelligent recognition
DO  - 10.1109/ICISCAE59047.2023.10392986
JO  - 2023 IEEE 6th International Conference on Information Systems and Computer Aided Education (ICISCAE)
IS  - 
SN  - 2770-663X
VO  - 
VL  - 
JA  - 2023 IEEE 6th International Conference on Information Systems and Computer Aided Education (ICISCAE)
Y1  - 23-25 Sept. 2023
AB  - This paper introduces a remote sensing load data storage system of UAV based on SOPC. This paper presents a method of data storage using multiple solid-state drives. The hard disk can be read and written in parallel. Use SDRAM as a buffer for data. In order to realize effective processing of crop data, a superblock file system is adopted. A number of image experiments with overlapping areas were carried out using unmanned aircraft. A new remote sensing image processing algorithm is proposed. The algorithm uses the spectral features, geometric features and texture features of multiple targets for classification. Relief-Pearson features are used to reduce the dimensionality of images, and redundant features with strong correlation and recognition performance are removed. Finally, according to the selected features, a multi-target recognition method is established and compared with the results of manual recognition. Based on the experimental data collected in China's corn production experiment and demonstration base, it is found that the new algorithm can simulate the real shadow well, and the overall accuracy is 0.9473. The F1 value reached 0.9184.
ER  - 


TY  - CONF
TI  - Unsupervised Multi-level Segmentation Framework for PolSAR Data using H-Alpha features and the Combined Edge- Region based segmentation
T2  - 2023 IEEE Aerospace Conference
SP  - 1
EP  - 8
AU  - M. A. Elenean
AU  - A. T. Hafez
AU  - A. K. Helmy
AU  - F. ElTohamy
AU  - A. Azouz
PY  - 2023
KW  - Image segmentation
KW  - Visualization
KW  - Urban areas
KW  - Clustering algorithms
KW  - Time measurement
KW  - Classification algorithms
KW  - Matrix decomposition
DO  - 10.1109/AERO55745.2023.10115863
JO  - 2023 IEEE Aerospace Conference
IS  - 
SN  - 1095-323X
VO  - 
VL  - 
JA  - 2023 IEEE Aerospace Conference
Y1  - 4-11 March 2023
AB  - PolSAR (Polimetric Synthetic Aperture Radar) has been shown to be a powerful source of information. As a result of using up to four measurement channels at the same time, which increases the processing depth, it offers information about the geometrical and physical characteristics of objects. However, operating the PolSAR system to its full imaging potential requires significant computing power. In this study, a framework for fully polarimetric SAR image segmentation is proposed, in which the PolSAR signal is decomposed into four components that represent the eigenvectors of the autocovariance matrix corresponding to signals and clutter. The Unsupervised segmentation framework possesses two main processing levels. First level is the data preprocessing, including mean coherency matrix calculation, speckle reduction and polarimetric feature decomposition. Second level include the initial cluster Centers estimation, and edge-region based algorithm. This is achieved by using the combined H-Alpha and (averaged Intensity) lambda features derived from the target decomposition of the PolSAR data. Finally, k-Means clustering based on the Wishart distribution is used to optimize the iterative clustering by merging the clusters with the minimum Wishart distance. The proposed framework is applied on (Flevoland and San_Francisco Bay). The images are selected to react differently with different polarization. The performance evaluation based on qualitative (Visual) and quantitative assessments. Visual assessment provides an excellent information on clarity and delineation of different classes. It is applicable for applications need an accurate statistical information. Quantitative evaluations provide more accurate results for separating different classes in the images. The proposed algorithm is compared to the traditional Cloude-Pottier classification method. The results demonstrate that the proposed algorithm accuracy reaches (88.6 %) with error (0.114), advances over the traditional Cloude-Pottier method with accuracy (84.6 %) and error (0.154).
ER  - 


TY  - JOUR
TI  - Analysis on Effective UAS Survey Conditions for Classification of Coastal Sediments
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 1163
EP  - 1173
AU  - H. Kim
AU  - J. Yu
AU  - L. Wang
AU  - C. Park
AU  - H. S. Han
AU  - S. -G. Jang
PY  - 2022
KW  - Sediments
KW  - Accuracy
KW  - Support vector machines
KW  - Spatial resolution
KW  - Image classification
KW  - Random forests
KW  - Grain size
KW  - Classification
KW  - coastal sediments
KW  - survey condition
KW  - UAS
DO  - 10.1109/JSTARS.2021.3136228
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 15
VL  - 15
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2022
AB  - This study aims to introduce effective unmanned aerial system (UAS) survey conditions for coastal sediment classification, including muddy sand, sand, gravel, and shells in a tidal flat area. UAS images with resolutions ranging from 2 to 60 mm are used as an implication of survey altitudes. The UAS images are used for sediment classification using random forest (RF) and support vector machine (SVM) methods. The results showed that RF is more effective in sediment classification while the general accuracy pattern was similar. The accuracy decreased with lower spatial resolutions. Notably, there is a significant drop of accuracy with a resolution coarser than 40 mm. Considering the training data selection, classification accuracy, and survey efficiency, it is suggested that 40 mm UAS images would provide optimal condition with acceptable accuracy for coastal sediment classification using RF model. To gain higher accuracy, a lower flight altitude is required, which will elongate the survey time significantly. Given the fact that this study is the first approach to test various UAS survey conditions for coastal sediment classifications in a field condition; the methodology and findings of this study can serve as a guideline framework for future coastal UAS sediment mapping.
ER  - 


TY  - CONF
TI  - Development of an Image Processing Techniques for Vehicle Classification Using OCR and SVM
T2  - 2023 International Conference on Science, Engineering and Business for Sustainable Development Goals (SEB-SDG)
SP  - 1
EP  - 9
AU  - I. O. Joshua
AU  - M. O. Arowolo
AU  - M. O. Adebiyi
AU  - O. R. Oluwaseun
AU  - K. A. Gbolagade
PY  - 2023
KW  - Earth
KW  - Visualization
KW  - Image processing
KW  - Documentation
KW  - Feature extraction
KW  - Classification algorithms
KW  - Character recognition
KW  - Support Vector Machine
KW  - Object Character Recognition
KW  - Image Process Techniques
KW  - Vehicle
DO  - 10.1109/SEB-SDG57117.2023.10124622
JO  - 2023 International Conference on Science, Engineering and Business for Sustainable Development Goals (SEB-SDG)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Science, Engineering and Business for Sustainable Development Goals (SEB-SDG)
Y1  - 5-7 April 2023
AB  - Image processing is a method for enhancing unprocessed images from cameras on aircraft, spacecraft, and satellites as well as images taken regularly for a variety of uses. In general, the following strategies can be used to categorize all image processing operations: Images are represented in several ways, which are referred to as image representation, image preprocessing, image enhancement, image restoration, image analysis, picture reconstruction, and image data compression. The first radiometric normalization, geometric distortion correction, and noise removal of the raw image data been discussed in the past. The goal of the information extraction procedures is to automate the identification of tone in a scene by replacing the visual examination of image data with quantitative techniques. This entails analyzing multispectral image data and establishing the earth cover identification of each pixel in an image using statistically based decision procedures. The goal of the classification procedure is to sort all of the pixels in a digital image into one of several different earth cover classes or themes. The purpose of this study is to examine various image processing approaches and algorithms, many sorts of image processing algorithms: Optical Character Recognition (OCR) and Supporting Vector Machine (SVM) a feature extraction technique, on the vehicle classification dataset and had accurate results of 90% for SVM and 95% for OCR, to further improve the performance of machine algorithms in terms of accuracy for image processing technique using a vehicle. This study can be used for vehicle classification research, it also advances and improves the performance of the system in terms of accurate detection.
ER  - 


TY  - CONF
TI  - Computer vision based surveillance concept for airport ramp operations
T2  - 2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC)
SP  - 3D2-1
EP  - 3D2-13
AU  - S. Vaddi
AU  - H. -L. Lu
AU  - M. Hayashi
PY  - 2013
KW  - Cameras
KW  - Aircraft
KW  - Surveillance
KW  - Airports
KW  - Approximation algorithms
KW  - Aircraft manufacture
KW  - Land vehicles
DO  - 10.1109/DASC.2013.6712568
JO  - 2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC)
IS  - 
SN  - 2155-7209
VO  - 
VL  - 
JA  - 2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC)
Y1  - 5-10 Oct. 2013
AB  - Current research develops a vision-based surveillance system concept suitable for airport ramp area operations. The surveillance approach consists of computer vision algorithms operating on video streams from surveillance cameras for detecting aircraft in images and localizing them. Rough order of magnitude estimates of the number of cameras required to cover the ramp area at a sample airport (Dallas/Fort Worth International Airport) were obtained. Two sets of algorithms with complimentary features were developed to detect an aircraft in a given image. The first set of algorithms was based on background subtraction, a popular computer-vision approach, for change detection in video streams. The second set was a supervised-learning approach based on a model learned from a database of images. The Histogram of Oriented Gradient (HOG) feature was used for classification with Support Vector Machines (SVMs). Then, an algorithm for matching aircraft in two different images was developed based on an approximate aircraft localization algorithm. Finally, stereo-vision algorithms were used for 3D-localization of the aircraft. A 1:400 scale model of a realistic airport consisting of a terminal building, jet bridges, ground marking, aircraft, and ground vehicles was used for testing the various algorithms. Aircraft detection was demonstrated using static and moving aircraft images, single and multiple aircraft images, and occluded aircraft images. Preliminary testing using the in-house setup demonstrated 3D localization accuracy of up to 30 ft.
ER  - 


TY  - JOUR
TI  - Techniques for radar imaging based on MUSIC algorithm
T2  - Journal of Systems Engineering and Electronics
SP  - 31
EP  - 38
AU  - G. Biao
AU  - L. Xiaoying
AU  - C. Zengping
PY  - 1999
KW  - Radar imaging
KW  - Multiple signal classification
KW  - Radar scattering
KW  - Lighting
KW  - Frequency measurement
KW  - MUSIC algorithm
KW  - Radar imaging
KW  - Scatter center
DO  - 
JO  - Journal of Systems Engineering and Electronics
IS  - 1
SN  - 1004-4132
VO  - 10
VL  - 10
JA  - Journal of Systems Engineering and Electronics
Y1  - March 1999
AB  - At first, the radar target scattering centers model and MUSIC algorithm are analyzed in this paper. How to efficiently set the parameters of the MUSIC algorithms is given by a great deal of simulated radar data in experiments. After that, according to measured data from two kinds of plane targets on fully polarized and high range resolution radar system, the author mainly investigated particular utilization of MUSIC algorithm in radar imaging. Ana two-dimensional radar images are generated for two targets measured in compact range. In the end. a conclusion is drew about the relation of radar target scattering properties and imaging results.
ER  - 


TY  - JOUR
TI  - Supervised textured image segmentation using feature smoothing and probabilistic relaxation techniques
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 1279
EP  - 1292
AU  - J. Y. Hsiao
AU  - A. A. Sawchuk
PY  - 1989
KW  - Image segmentation
KW  - Smoothing methods
KW  - Pixel
KW  - Image texture analysis
KW  - Image edge detection
KW  - Aerospace electronics
KW  - Feature extraction
KW  - Filtering
KW  - Labeling
KW  - Degradation
DO  - 10.1109/34.41366
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 12
SN  - 1939-3539
VO  - 11
VL  - 11
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - Dec. 1989
AB  - A description is given of a supervised textured image segmentation algorithm that provides improved segmentation results. An improved method for extracting textured energy features in the feature extraction stage is described. It is based on an adaptive noise smoothing concept that takes the nonstationary nature of the problem into account. Texture energy features are first estimated using a window of small size to reduce the possibility of mixing statistics along region borders. The estimated texture energy feature values are smoothed by a quadrant filtering method to reduce the variability of the estimates while retaining the region border accuracy. The estimated feature values of each pixel are used by a Bayes classifier to make an initial probabilistic labeling. The spatial constraints are enforced through the use of a probabilistic relaxation algorithm. Two probabilistic relaxation algorithms are investigated. Limiting the probability labels by probability threshold is proposed. The tradeoff between efficiency and degradation of performed is studied.<>
ER  - 


TY  - CONF
TI  - Very Low-Level Airspace Assessment and Classification for Unmanned Aircraft Operation
T2  - 2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC)
SP  - 6
EP  - 12
AU  - Q. Wu
AU  - J. Zhang
AU  - Y. Zhong
AU  - W. Liu
AU  - X. Zou
AU  - F. Xie
PY  - 2021
KW  - Computer science
KW  - Safety management
KW  - Process control
KW  - Aircraft
KW  - National security
KW  - Standards
KW  - Intelligent control
KW  - unmanned aircraft
KW  - very low-level airspace
KW  - risk assessment
KW  - airspace classification
KW  - mitigation measures
KW  - safety management
DO  - 10.1109/ISCSIC54682.2021.00013
JO  - 2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC)
Y1  - 12-14 Nov. 2021
AB  - With the increasing activity of unmanned aircraft (UA) in very low-level (VLL) airspace, severe challenges have emerged to personal safety, social order and national security. This paper proposes a method of airspace assessment and classification for UA operation. Both ground and air risks caused by UA are taken into considered to establish a quantifiable VLL airspace assessment indication system. Then, the comprehensive risk value of each grid of rasterization airspace is estimated by a set of comprehensive evaluation method. the concept of additional characteristic value is presented to reduce the threat to high risk level areas caused by surrounding UAs. Finally, the VLL airspace for UAs is organized into three classifications defined in this paper: permitted fly area, limited fly area and forbidden fly area. A case study on the simulated VLL urban airspace illustrates that the proposed method can assess the operational risk of UA and classify VLL airspace to set different access criteria for UA. Based on the three classifications of airspace defined in this paper, the efficiency and risks of UA can be balanced by taking adequate mitigation measures and permit UA to enter expanded airspace.
ER  - 


TY  - JOUR
TI  - Fusion of intelligent agents for the detection of aircraft in SAR images
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 378
EP  - 384
AU  - A. Filippidis
AU  - L. C. Jain
AU  - N. Martin
PY  - 2000
KW  - Intelligent agent
KW  - Aircraft
KW  - Radar detection
KW  - Image analysis
KW  - Target recognition
KW  - Fuzzy reasoning
KW  - Synthetic aperture radar
KW  - Colored noise
KW  - Image sensors
KW  - Fusion power generation
DO  - 10.1109/34.845380
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 4
SN  - 1939-3539
VO  - 22
VL  - 22
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - April 2000
AB  - Receiver operating curves are used in the analysis of 20 images using a novel automatic target recognition (ATR) fusion system. Fuzzy reasoning is used to improve the accuracy of the automatic detection of aircraft in synthetic aperture radar (SAR) images using a priori knowledge derived from color aerial photographs. The images taken by the two different sensors are taken at different times. In summarizing the results of our experiments using real and generated targets with noise for a probability of detection of 91.5 percent using the ATR fusion technique, we have improved our false alarm rates by approximately 17 percent over using texture classification.
ER  - 


TY  - CONF
TI  - Learning-Aided Aircraft Detection for High-Resolution SAR Images
T2  - 2019 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)
SP  - 1
EP  - 6
AU  - X. Wang
AU  - X. Jiang
PY  - 2019
KW  - SAR images
KW  - aircraft detection
KW  - CFAR
KW  - HOG
KW  - SVM
DO  - 10.1109/ICSPCC46631.2019.8960883
JO  - 2019 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)
Y1  - 20-22 Sept. 2019
AB  - The constant false alarm rate (CFAR) detection is one of the most widely utilized detection algorithms in SAR images. However, due to the complexity of the airport scene, using CFAR only cannot achieve satisfying aircraft detection. In this paper, we propose a novel framework of learning-aided aircraft detection by machine learning to overcome the issues. This framework links the classic CFAR with machine learning. While preserving the efficiency of the CFAR, the proposed method introduces machine learning to improve accuracy. Specifically, CFAR and a priori knowledge assist us quickly find the location of potential aircraft targets in the scene, which ensures the fast processing speed of framework. In machine learning, we classify the slices of potential targets and then train a classifier for identifying aircraft. Machine learning algorithms perform well on the classification problem, so the high false alarm caused by the traditional method can be smoothly suppressed. Meanwhile, machine learning algorithms improve the accuracy of the framework. Experimental results verify the significant performance of the proposed aircraft detection framework.
ER  - 


TY  - CONF
TI  - Object detection based on BING in optical remote sensing images
T2  - 2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
SP  - 504
EP  - 509
AU  - J. Zheng
AU  - Y. Xi
AU  - M. Feng
AU  - X. Li
AU  - N. Li
PY  - 2016
KW  - Object detection
KW  - Proposals
KW  - Support vector machines
KW  - Aircraft
KW  - Feature extraction
KW  - Fourier transforms
KW  - Training
KW  - object detection
KW  - remote sensing images
KW  - objectness proposal
KW  - BING
DO  - 10.1109/CISP-BMEI.2016.7852763
JO  - 2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
Y1  - 15-17 Oct. 2016
AB  - High-resolution remote sensing images (RSIs) have been adopted in satellites, RSIs processing in satellites will enable new multimedia applications, such as situational awareness. In this paper, we have developed an object detection framework exploiting objectness measurement in which binarized normed gradient (BING) is used to detect some particular objects in RSIs more efficiently. Specifically, we (a) use BING to encode closed boundaries of objects and then obtain a group of candidate regions. The group can narrow down the search space of object detection, accelerating the traditional process of sliding window; and (b) employ a more robust feature descriptor. The feature is obtained by combining the pyramid of histograms of orientation gradients (PHOG) with Elliptic Fourier Transform so as to detect multiscale objects effectively; and (c) apply an SVM classifier to detect particular objects. Experiments show that the proposed framework has achieved an 85.1% average detection precision, 35.1% higher than the HOG-SVM using the same dataset.
ER  - 


TY  - CONF
TI  - UAV Detection, and Explanation Using a Tuned Transfer Learning Model
T2  - 2022 International Conference on Data Science and Intelligent Computing (ICDSIC)
SP  - 192
EP  - 197
AU  - N. S. Sagheer
AU  - N. A. Rabee
AU  - F. H. A. Lmasoudy
PY  - 2022
KW  - Training
KW  - Privacy
KW  - Military computing
KW  - Transfer learning
KW  - Decision making
KW  - Data science
KW  - Military aircraft
KW  - drones
KW  - detection
KW  - classification
KW  - localization
KW  - deep learning
DO  - 10.1109/ICDSIC56987.2022.10075960
JO  - 2022 International Conference on Data Science and Intelligent Computing (ICDSIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Data Science and Intelligent Computing (ICDSIC)
Y1  - 1-2 Nov. 2022
AB  - Computer vision is one of computer science's most important subfields. The rapid evolution of the camera, video, and image applications led to this significance. In the past years, drones have been rapidly developed. Most of the time, they have been employed for privacy violations, espionage, and military attacks in the past several years. The rotary drone is one of the different Unmanned Aircraft vehicles (UAV) used for various reasons. This paper uses a deep learning-based model to detect and classify rotary drones on two datasets. The first image dataset shows the drones with a white or transparent background, while the remaining images include a different background without a drone gathered from the second dataset. The second dataset consists of images with diverse backgrounds, some of which contain drones and others without being used for testing the system. The suggested system includes three stages. The first is preprocessing, such as Normalized and resized images. The second parts represent the tuned VGG16 model that trained for twenty epochs passed since the beginning of the training process to produce results. Finally, the explanation stage is used to ensure the correctness of decision-making. The results indicate an efficient drone classification with 99% accuracy.
ER  - 


TY  - CONF
TI  - In-situ monitoring of selective laser melting based on heterogeneous integration of acoustic signals and images
T2  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
SP  - 420
EP  - 424
AU  - Y. Dongsen
AU  - Z. Yingjie
PY  - 2021
KW  - Support vector machines
KW  - Metals
KW  - Signal processing
KW  - Feature extraction
KW  - Laser modes
KW  - Acoustics
KW  - Convolutional neural networks
KW  - selective laser melting
KW  - heterogeneous integration
KW  - in-situ monitoring
DO  - 10.1109/CCISP52774.2021.9639261
JO  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
Y1  - 19-21 Nov. 2021
AB  - Monitoring during the selective laser melting (SLM) process is the only way to achieve 100% Non Destructive Testing (NDT) for parts produced. This work provided a heterogeneous integration monitoring with acoustic signals and images. Acoustic signal features and image features could indicate different states of the melting and then their signal features were extracted by deep belief networks (DBN) and convolutional neural networks (CNN) respectively. Heterogeneous features were used to recognize the sates of the melting by Support vector machines (SVM) method.Classifications were carried out with acoustic signal and image features respectively for comparisons. The optimal classification rate came from fused features from acoustic signals and images. The results indicated that the fused features improved the defect diagnosis capabilities for the SLM process. It is effective and promising by recognizing heterogeneous integrated signals for the in-situ monitoring of the SLM process.
ER  - 


TY  - CONF
TI  - Radar imaging of noncooperating maneuvering aircraft
T2  - IEEE International Conference on Radar
SP  - 563
EP  - 568
AU  - G. Corsini
AU  - E. Dalle Mese
AU  - G. Manara
AU  - F. Bessi
AU  - G. Bettini
AU  - F. Zacca
PY  - 1990
KW  - Radar imaging
KW  - Aircraft
KW  - Motion compensation
KW  - Testing
KW  - Electromagnetic scattering
KW  - Radar tracking
KW  - Noise measurement
KW  - Time measurement
KW  - Electromagnetic modeling
KW  - Radar scattering
DO  - 10.1109/RADAR.1990.201090
JO  - IEEE International Conference on Radar
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE International Conference on Radar
Y1  - 7-10 May 1990
AB  - The problem of performing automatic high resolution radar imaging of noncooperating maneuvering aircraft is investigated. In order to define the main features of the radar processor, a numerical model of the system which makes use of an electromagnetic model of the moving aircraft so that the actual, complete operating conditions can be simulated has been developed. The availability of such a numerical tool avoids the need to carry out long and onerous experimental tests. In this framework an automatic high-resolution radar imaging technique has been developed. The imaging algorithm provides parameters useful for target classification.<>
ER  - 


TY  - JOUR
TI  - Coarse-to-Fine Description for Fine-Grained Visual Categorization
T2  - IEEE Transactions on Image Processing
SP  - 4858
EP  - 4872
AU  - H. Yao
AU  - S. Zhang
AU  - Y. Zhang
AU  - J. Li
AU  - Q. Tian
PY  - 2016
KW  - Visualization
KW  - Image segmentation
KW  - Testing
KW  - Object detection
KW  - Electronic mail
KW  - Training
KW  - Neural networks
KW  - Fine-grained visual categorization
KW  - convolutional neural network
KW  - coarse-to-fine description
DO  - 10.1109/TIP.2016.2599102
JO  - IEEE Transactions on Image Processing
IS  - 10
SN  - 1941-0042
VO  - 25
VL  - 25
JA  - IEEE Transactions on Image Processing
Y1  - Oct. 2016
AB  - Recent years have witnessed the significant advance in fine-grained visual categorization, which targets to classify the objects belonging to the same species. To capture enough subtle visual differences and build discriminative visual description, most of the existing methods heavily rely on the artificial part annotations, which are expensive to collect in real applications. Motivated to conquer this issue, this paper proposes a multi-level coarse-to-fine object description. This novel description only requires the original image as input, but could automatically generate visual descriptions discriminative enough for fine-grained visual categorization. This description is extracted from five sources representing coarse-to-fine visual clues: 1) original image is used as the source of global visual clue; 2) object bounding boxes are generated using convolutional neural network (CNN); 3) with the generated bounding box, foreground is segmented using the proposed k nearest neighbour-based co-segmentation algorithm; and 4) two types of part segmentations are generated by dividing the foreground with an unsupervised part learning strategy. The final description is generated by feeding these sources into CNN models and concatenating their outputs. Experiments on two public benchmark data sets show the impressive performance of this coarse-to-fine description, i.e., classification accuracy achieves 82.5% on CUB-200-2011, and 86.9% on fine-grained visual categorization-Aircraft, respectively, which outperform many recent works.
ER  - 


TY  - CONF
TI  - Reclamation Aquaculture Detection Based on VHR UAV Aerial Images
T2  - 2019 9th International Conference on Information Science and Technology (ICIST)
SP  - 316
EP  - 319
AU  - J. Fan
AU  - X. Liu
AU  - J. Zhao
AU  - X. Su
AU  - X. Wang
AU  - X. Wang
PY  - 2019
KW  - Aquaculture
KW  - Remote sensing
KW  - Image resolution
KW  - Monitoring
KW  - Image segmentation
KW  - Semantics
KW  - Hidden Markov models
KW  - Reclamation aquaculture
KW  - UAV
KW  - very high resolution aerial images
KW  - target detection
DO  - 10.1109/ICIST.2019.8836728
JO  - 2019 9th International Conference on Information Science and Technology (ICIST)
IS  - 
SN  - 2573-3311
VO  - 
VL  - 
JA  - 2019 9th International Conference on Information Science and Technology (ICIST)
Y1  - 2-5 Aug. 2019
AB  - Unmanned aircraft vehicle (UAV) has emerged widely at the lower cost of obtaining remote sensing imagery, which can collect very high resolution (VHR) data from multispectral or SAR sensor modalities conveniently. This trend facilitates a transition from large scale land-use segmentation to object-level scene understanding. Reclamation aquaculture is one of the most important types of the coastal zone. It has a tight relationship between the marine ecology and economy. In this paper, some centimeter aerial remote sensing imagery is used to monitor reclamation aquaculture areas with unsupervised approaches. Experiments on actual reclamation aquaculture areas demonstrate effectiveness.
ER  - 


TY  - CONF
TI  - Improving the classification accuracy of the method of the moments using aspect ratio normalization
T2  - [1988] Proceedings. The Twentieth Southeastern Symposium on System Theory
SP  - 349
EP  - 352
AU  - A. R. Rostampour
PY  - 1988
KW  - Shape
KW  - Pattern recognition
KW  - Equations
KW  - Moment methods
KW  - Dynamic range
KW  - Gravity
KW  - Humans
KW  - Aircraft
KW  - Loss measurement
KW  - Pixel
DO  - 10.1109/SSST.1988.17072
JO  - [1988] Proceedings. The Twentieth Southeastern Symposium on System Theory
IS  - 
SN  - 0094-2898
VO  - 
VL  - 
JA  - [1988] Proceedings. The Twentieth Southeastern Symposium on System Theory
Y1  - 20-22 March 1988
AB  - The method of moments has been used in several forms for shape recognition. Due to the dynamic range of the moments, high-order moment elements do not contribute significantly in the classification process and in some cases they reduce the classification accuracy. A normalization procedure, called aspect ratio normalization, which improves the classification accuracy, is discussed. The procedure is applied to a set of data to demonstrate its performance.<>
ER  - 


TY  - CONF
TI  - Efficient automatic target recognition method for aircraft SAR image using supervised SOM clustering
T2  - Conference Proceedings of 2013 Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
SP  - 601
EP  - 604
AU  - S. Ohno
AU  - S. Kidera
AU  - T. Kirimoto
PY  - 2013
KW  - Synthetic aperture radar
KW  - Training
KW  - Training data
KW  - Target recognition
KW  - Robustness
KW  - Aircraft
KW  - Radar imaging
KW  - Automatic target recognition(ATR)
KW  - Supervised Self organizing map(SOM)
KW  - SAR imagery
DO  - 
JO  - Conference Proceedings of 2013 Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Conference Proceedings of 2013 Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
Y1  - 23-27 Sept. 2013
AB  - Synthetic aperture radar (SAR) has significant advantages in providing high resolution target images, even in darkness or adverse weather. Nevertheless, human operators find target images difficult to recognize because SAR images are generated using complex-valued radio signals of around 1.0-m wavelength. To address this issue, various automatic target recognition (ATR) approaches have been developed, such as those based on neural network or SVM(Support Vector Machine). Moreover, we have already proposed the efficient ATR method using a supervised self-organizing map (SOM), where a binarized SAR image is accurately classified by exploiting the unified distance matrix (Umatrix) metric. Although this method significantly enhances the ATR performance even with heavily contaminated SAR images, it still has a significant problem requiring enormous calculational demands under expansions of scale and thus cannot handle the ATR issue using more training data. As a solution for this problem, this paper employs the A-star algorithm to accelerate the classification speed, and then newly introduces the constrained learning process in generating SOM, which enhances the robustness to the angular variation in targets. Experimental results validate the effectiveness of our proposed method.
ER  - 


TY  - CONF
TI  - Fine-Grained Visual Classification using Self Assessment Classifier
T2  - 2024 IEEE Conference on Artificial Intelligence (CAI)
SP  - 597
EP  - 602
AU  - T. Do
AU  - H. Tran
AU  - E. Tjiputra
AU  - Q. D. Tran
AU  - A. Nguyen
PY  - 2024
KW  - Training
KW  - Visualization
KW  - Accuracy
KW  - Image resolution
KW  - Dogs
KW  - Self-supervised learning
KW  - Learning (artificial intelligence)
KW  - fine-grained classification
KW  - self-supervised learning
DO  - 10.1109/CAI59869.2024.00117
JO  - 2024 IEEE Conference on Artificial Intelligence (CAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE Conference on Artificial Intelligence (CAI)
Y1  - 25-27 June 2024
AB  - Extracting discriminative features plays a crucial role in the fine-grained visual classification task. Most of the existing methods focus on developing attention or augmentation mechanisms to achieve this goal. However, addressing the ambiguity in the top-k prediction classes is not fully investigated. In this paper, we introduce a Self Assessment Classifier, which simultaneously leverages the representation of the image and top-k prediction classes to reassess the classification results. Our method is inspired by self-supervised learning with coarse-grained and fine-grained classifiers to increase the discrimination of features in the backbone and produce attention maps of informative areas on the image. In practice, our method works as an auxiliary branch and can be easily integrated into different architectures. We show that by effectively addressing the ambiguity in the top-k prediction classes, our method achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and FGVC Aircraft datasets. Furthermore, our method also consistently improves the accuracy of different existing fine-grained classifiers with a unified setup.
ER  - 


TY  - CONF
TI  - Disbond detection through ultrasonic signal classification using an artificial neural network
T2  - IJCNN-91-Seattle International Joint Conference on Neural Networks
SP  - 906 vol.2
EP  - 
AU  - D. R. Prabhu
AU  - M. N. Abedin
AU  - W. P. Winfree
AU  - E. I. Madaras
PY  - 1991
KW  - Pattern classification
KW  - Artificial neural networks
KW  - Aircraft
KW  - Neural networks
KW  - Bonding
KW  - Feedforward neural networks
KW  - Skin
KW  - Ultrasonic transducers
KW  - Network topology
KW  - Image recognition
DO  - 10.1109/IJCNN.1991.155493
JO  - IJCNN-91-Seattle International Joint Conference on Neural Networks
IS  - 
SN  - 
VO  - ii
VL  - ii
JA  - IJCNN-91-Seattle International Joint Conference on Neural Networks
Y1  - 8-12 July 1991
AB  - Summary form only given. An ultrasound technique for detecting disbonds in aircraft lap joints and in the adhesive joints between aircraft skin and reinforcing doublers was discussed. A high-frequency ultrasonic pulse is transmitted into the aircraft skin by a contacting ultrasonic transducer. This pulse is reflected at the bond interface, and is picked up by the transducer. The output is a time-varying ultrasonic signal that characterizes the bond interface. The use of an artificial neural network for classifying the signals as corresponding to bonded and disbonded regions is discussed. Training and classification performances were obtained for several values of network parameters. A peak classification accuracy of 98.7% was obtained on the test signal set. The advantages of using a neural network are its low noise sensitivity, low classification time, high classification accuracy, and convenient threshold capability of output for disbond detection.<>
ER  - 


TY  - CONF
TI  - Double-Triplet-Pseudo-Siamese Architecture For Remote Sensing Aircraft Target Recognition
T2  - 2021 International Conference on Computer, Blockchain and Financial Development (CBFD)
SP  - 140
EP  - 146
AU  - X. Cao
AU  - H. Zou
AU  - X. Ying
AU  - R. Li
AU  - S. He
AU  - F. Cheng
PY  - 2021
KW  - Training
KW  - Measurement
KW  - Target recognition
KW  - Fuses
KW  - Computer architecture
KW  - Feature extraction
KW  - Sensors
KW  - Fine-grained recognition
KW  - Remote sensing of aircraft targets
KW  - Pseudo-Siamese
KW  - Triplet
KW  - Discriminative fusion
DO  - 10.1109/CBFD52659.2021.00034
JO  - 2021 International Conference on Computer, Blockchain and Financial Development (CBFD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computer, Blockchain and Financial Development (CBFD)
Y1  - 23-25 April 2021
AB  - The key challenge of remote sensing aircraft target (RSAT) recognition is that features generated by similar target are difficult to distinguish. To solve the problem, we present a double-triplet-pseudo-siamese (DTPS) architecture to learn to distinguish the subtle discriminative features between similar targets. Specifically, we first construct image triplet and mask triplet, which are then sent to the convolutional neural networks, fully connected layers and softmax sequentially for classification. Besides the classification predictions, we utilize standard templates for contrastive prediction in the test process and introduce a discriminative fusion method to fuse the multiple prediction. In addition, we utilize classification loss, contrast loss and triplet loss during training, which help the network to distinguish similar targets by metric learning. We conduct extensive experiments on benchmark RSAT datasets to demonstrate the effectiveness of our network and the experimental results show that the performance of the proposed method surpasses other existing methods.
ER  - 


TY  - CONF
TI  - ACO-DD: An improved framework for UAV autonomous landing recognition based on Multiple Instance Learning
T2  - 2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)
SP  - 44
EP  - 48
AU  - An Su-yang
AU  - Zhang Fei-juan
AU  - Li Chu
AU  - Zhang Yu
AU  - Ge Wen-bin
AU  - Bao Yong
PY  - 2016
KW  - Unmanned aerial vehicles
KW  - Feature extraction
KW  - Classification algorithms
KW  - Labeling
KW  - Supervised learning
KW  - Image segmentation
KW  - Mathematical model
DO  - 10.1109/CGNCC.2016.7828756
JO  - 2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)
Y1  - 12-14 Aug. 2016
AB  - UAV flight test for scientific phases has a high accident rate, especially for autonomous landing. UAV ground monitoring system is an essential approach to reducing the risk for UAV autonomous landing, and recognition system is the core of system. This paper discusses recognition system for UAV autonomous landing. However, traditional supervised learning usually suffers from expensive labeling in recognition system. In our study, an improved Multiple Instance Learning framework for UAV autonomous landing recognition is proposed, called ACO-DD. First, a novel bags and instances construction method is presented for UAV autonomous landing images, and then the parameter optimization challenge in Diverse Density (DD) algorithm is solved by Ant Colony Optimization (ACO) algorithm. Finally, the completed framework for UAV autonomous landing recognition is presented. The experimental results show our method reduced manual labeling in training sample; meanwhile the algorithm performance is improved.
ER  - 


TY  - JOUR
TI  - Toward Crop Maturity Assessment via UAS-Based Imaging Spectroscopy—A Snap Bean Pod Size Classification Field Study
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 17
AU  - A. Hassanzadeh
AU  - F. Zhang
AU  - S. P. Murphy
AU  - S. J. Pethybridge
AU  - J. van Aardt
PY  - 2022
KW  - Crops
KW  - Hyperspectral imaging
KW  - Feature extraction
KW  - Indexes
KW  - Vegetation mapping
KW  - Data collection
KW  - Noise reduction
KW  - Classification
KW  - feature selection
KW  - hyperspectral imaging
KW  - machine learning
KW  - maturity assessment
KW  - pod size
KW  - precision agriculture
KW  - snap bean
KW  - unmanned aerial system (UAS)
DO  - 10.1109/TGRS.2021.3134564
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - Timely assessment of crop maturity contributes to optimized harvesting schedules while limiting food loss/waste at the farm level. Maturity assessments are typically performed via costly and time-consuming in situ methods. This study aimed to evaluate pod size crop maturity using imaging spectroscopy via unmanned aerial systems (UASs), as well as identifying discriminating wavelengths, using snap bean as a proxy crop. The research utilized a UAS-mounted hyperspectral imager in the visible-to-near-infrared region. Two years’ worth of data were collected at two different geographical locations for six different snap bean cultivars. Our approach consisted of calibration to reflectance, vegetation detection, noise reduction, creating classification bins, and feature selection. We used our previously published feature selection library, Jostar, and utilized ant colony optimization and simulated annealing to detect five spectral features and Plus-L Minus-R to identify one to ten features. We utilized decision trees and random forest classifiers for the classification task. Our findings show that, given the proper wavelengths, accurate pod maturity assessment is feasible for large-sieve cultivars (F1 score = 0.79–0.91), separating sieve sizes between ready-to-harvest and not ready-to-harvest pods. These spectral features were in the ~450, ~530, ~660, 700–720, ~740, and ~760 nm regions. This bodes well for the potential extension of results to an operational, multispectral sensor, tuned with the identified bands, thereby negating the need for a costly hyperspectral system. However, this proposition mandates further investigation, including data acquisition from geographical locations with variable climates, and quantifying noise for the hyperspectral imager to compare results with noisier datasets.
ER  - 


TY  - CONF
TI  - Typical Target Detection in Satellite Images Based on Convolutional Neural Networks
T2  - 2015 IEEE International Conference on Systems, Man, and Cybernetics
SP  - 2956
EP  - 2961
AU  - H. Wu
AU  - H. Zhang
AU  - J. Zhang
AU  - F. Xu
PY  - 2015
KW  - Image edge detection
KW  - Aircraft
KW  - Object detection
KW  - Fuel storage
KW  - Proposals
KW  - Satellites
KW  - Military aircraft
KW  - Target detection
KW  - EdgeBoxes
KW  - Convolutional Neural Networks
DO  - 10.1109/SMC.2015.514
JO  - 2015 IEEE International Conference on Systems, Man, and Cybernetics
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE International Conference on Systems, Man, and Cybernetics
Y1  - 9-12 Oct. 2015
AB  - With the rapid technological development of various different satellite sensors, a huge volume of high-resolution image data sets can now be obtained and widely used in military and civilian fields. Detecting typical targets in satellite images is a challenging task due to the complicated background. Traditional manually engineered features (i.e. HOG, Gabor feature and Hough transform, etc.) do not work well for massive high-resolution remote sensing image data. Thus, we are expected to find an efficient way to automatically learn the presentations from the massive image data and increase the computational efficiency of target detection. Comparing to the general objects in nature images, the edge information of targets in satellite images shows more distinctive and concise characteristics. This paper proposes a new target detection framework based on Edge Boxes and Convolutional Neural Networks (CNN). CNN can learn rich features automatically and is invariant to small rotation and shifts, has achieved state of-the-art performance in many image classification databases. Edge Boxes can generate a smaller set of object proposals based on the edges of objects. The proposed method can reduce the computational time of the detector. Extensive experiments demonstrate that the proposed framework is effective in typical target detection systems.
ER  - 


TY  - CONF
TI  - Efficient construction of training database for identification of aircraft ISAR images
T2  - Proceedings of 2011 IEEE CIE International Conference on Radar
SP  - 520
EP  - 523
AU  - G. . -G. Choi
AU  - S. . -K. Han
AU  - K. . -T. Kim
PY  - 2011
KW  - Mercury (metals)
KW  - Inverse Synthetic Aperture Radar (ISAR)
KW  - Target Classification
DO  - 10.1109/CIE-Radar.2011.6159593
JO  - Proceedings of 2011 IEEE CIE International Conference on Radar
IS  - 
SN  - 1097-5764
VO  - 1
VL  - 1
JA  - Proceedings of 2011 IEEE CIE International Conference on Radar
Y1  - 24-27 Oct. 2011
AB  - This paper presents a new paradigm for classification of aircraft inverse synthetic aperture radar (ISAR) images. A database for identification is efficiently constructed using various 3-D aircraft flight scenarios. Simulation results demonstrate that this method can reduce the memory space for target identification significantly compared to the conventional methodology while maintaining reliable identification performance.
ER  - 


TY  - CONF
TI  - Classification of Partial Discharge in Electric Aircraft based on Short-Term Behavior of Insulation Systems
T2  - 2021 AIAA/IEEE Electric Aircraft Technologies Symposium (EATS)
SP  - 1
EP  - 10
AU  - M. Borghei
AU  - M. Ghassemi
PY  - 2021
KW  - Partial discharges
KW  - Industries
KW  - Condition monitoring
KW  - Partial discharge measurement
KW  - Insulation
KW  - Electric potential
KW  - Fault location
DO  - 10.23919/EATS52162.2021.9704813
JO  - 2021 AIAA/IEEE Electric Aircraft Technologies Symposium (EATS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 AIAA/IEEE Electric Aircraft Technologies Symposium (EATS)
Y1  - 11-13 Aug. 2021
AB  - warming crisis has started a movement toward the reduction of emissions and fossil-fuel dependency. A major potential resides in the transportation sector which has led to the proliferation of electric vehicles. Similarly, significant progress has been made toward electrification in the aviation industry. Unlike electric cars, electrification in the commercial aircraft industry still looks far. One of the major bottlenecks in this path is the reliability of electrical equipment in aeronautical applications. The harsh environmental conditions along with the higher electric tension imposed by the high-power density equipment (which are necessary for the aviation industry) can accelerate the aging of insulation systems. The insulation system is regarded as the heart of the electrical equipment and its failure results in the breakdown of the system. A major deterioration mechanism in the insulation system is partial discharge (PD). In this study, we develop a framework for the condition monitoring of insulation systems at high altitudes based on deep learning. Different cases of corona discharge are tested based on standard IEC 60270 as potential sources of threat to the electrical systems. The measured signals are the input of the Dielectric Online Condition Monitoring System (DOCMS) that preprocesses the data, converts them into phase-resolved PD (PRPD) images, and classifies them based on their source type using EfficientNet. Then, DOCMS updates the system engineer about the status of electrical equipment in case of unsafe operation. The results demonstrate the high accuracy and fastness of the proposed approach to identify potential threats to the health of the insulation systems.
ER  - 


TY  - CONF
TI  - Pose estimation for ISAR image classification
T2  - 2010 IEEE International Geoscience and Remote Sensing Symposium
SP  - 4620
EP  - 4623
AU  - M. N. Saidi
AU  - A. Toumi
AU  - A. Khenchaf
AU  - B. Hoeltzener
AU  - D. Aboutajdine
PY  - 2010
KW  - Estimation
KW  - Shape
KW  - Transforms
KW  - Target recognition
KW  - Feature extraction
KW  - Radar
KW  - Pixel
KW  - Automatic Target Recognition
KW  - ISAR image
KW  - Pose estimation
KW  - Similarity measure
KW  - Classification
DO  - 10.1109/IGARSS.2010.5651187
JO  - 2010 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2010 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 25-30 July 2010
AB  - This paper presents aircraft target recognition (ATR) system using Inverse Synthetic Aperture Radar (ISAR) images. Knowing the pose of the target can improve the ATR performance (recognition rate and computational complexity). So, we propose in this paper a new pose estimator from ISAR images, based on the axis of symmetry and the similarity measure. The method proposed is compared with several approaches proposed recently in the literature, such as 2-D Continuous wavelet Transform and Hough transform. Once the pose of target is estimated, the classification if finally performed by K-Nearest Angle (KNA) classifier which insert the pose information into image retrieval task.
ER  - 


TY  - JOUR
TI  - Classification of Multispectral Image Data by Extraction and Classification of Homogeneous Objects
T2  - IEEE Transactions on Geoscience Electronics
SP  - 19
EP  - 26
AU  - R. L. Kettig
AU  - D. A. Landgrebe
PY  - 1976
KW  - Multispectral imaging
KW  - Data mining
KW  - Layout
KW  - Laboratories
KW  - Remote sensing
KW  - Decision theory
KW  - Partitioning algorithms
KW  - Geoscience
KW  - Aircraft propulsion
KW  - Satellites
DO  - 10.1109/TGE.1976.294460
JO  - IEEE Transactions on Geoscience Electronics
IS  - 1
SN  - 0018-9413
VO  - 14
VL  - 14
JA  - IEEE Transactions on Geoscience Electronics
Y1  - Jan. 1976
AB  - A method of classification of digitized multispectral image data is described. It is designed to exploit a particular type of dependence between adjacent states of nature that is characteristic of the data. The advantages of this, as opposed to the conventional "per point" approach, are greater accuracy and efficiency, and the results are in a more desirable form for most purposes. Experimental results from both aircraft and satellite data are included.
ER  - 


TY  - JOUR
TI  - Identification of Foliar Disease Regions on Corn Leaves Using SLIC Segmentation and Deep Learning Under Uniform Background and Field Conditions
T2  - IEEE Access
SP  - 111985
EP  - 111995
AU  - H. Phan
AU  - A. Ahmad
AU  - D. Saraswat
PY  - 2022
KW  - Diseases
KW  - Image segmentation
KW  - Deep learning
KW  - Plants
KW  - Testing
KW  - Image color analysis
KW  - Monitoring
KW  - Agriculture
KW  - Vegetation mapping
KW  - SLIC segmentation
KW  - deep learning
KW  - image classification
KW  - corn leaf disease identification
KW  - field conditions
DO  - 10.1109/ACCESS.2022.3215497
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - Plant diseases lead to severe losses in crop yield worldwide. The conventional approach for diagnosing diseases relies on manual scouting. In recent years, advances in convolutional neural networks have motivated the use of deep learning-based computer vision for automatically identifying plant diseases. Although image classification techniques are commonly used for analyzing agricultural data, their use for accurately identifying diseased regions corresponding to different disease types on individual plant leaves is limited. In this study, Simple Linear Iterative Clustering (SLIC) segmentation was used on corn leaf images from the PlantVillage and CD&S datasets to create super-pixels, a cluster of pixels representing a region of interest on a corn leaf. The VGG16, ResNet50, DenseNet121, Xception, and InceptionV3, pre-trained deep learning models were utilized to identify diseased regions corresponding to five super-pixel classes (healthy, northern leaf blight (NLB), gray leaf spot (GLS), common rust, and background) for the PlantVillage dataset and four super-pixel classes (NLB, GLS, northern leaf spot, and background) for the CD&S dataset, on corn leaves. After setting the spatial proximity value (sigma) for SLIC segmentation to five, a total of 100 models were trained by using different numbers of segments per image (five and fifteen) in the SLIC algorithm for both datasets and choosing training: testing split ratios of 90:10, 80:20, 70:30, 60:40, and 50:50 for each of the five models. The highest overall testing accuracy of 97.77% was observed after training the DenseNet121 model to identify super-pixels created from the CD&S dataset, belonging to the four classes created using a sigma value of five, five segments per image, and an 80:20 training: testing split ratio. Web and mobile applications were developed to identify diseased regions on corn leaves using the best deep learning model as the classifier. The results suggest that SLIC segmentation on corn leaf images helps accurately identify diseased regions. This research demonstrates the potential of image-based scouting as an efficient alternative to manual scouting for disease monitoring.
ER  - 


TY  - CONF
TI  - Tomographic Image Reconstruction using Inverse Synthetic Aperture Radar Methods
T2  - 2021 9th International Japan-Africa Conference on Electronics, Communications, and Computations (JAC-ECC)
SP  - 213
EP  - 216
AU  - A. -A. I. Mahmoud Hassanin
AU  - N. Abdel-Salam Bauomy
PY  - 2021
KW  - Shape
KW  - Shape measurement
KW  - Tomography
KW  - Radar imaging
KW  - Programming
KW  - Noise measurement
KW  - Inverse synthetic aperture radar
KW  - Tomographic image reconstruction
KW  - inverse synthetic aperture radar
KW  - flight path
KW  - comprehensive programming package
KW  - and design
DO  - 10.1109/JAC-ECC54461.2021.9691413
JO  - 2021 9th International Japan-Africa Conference on Electronics, Communications, and Computations (JAC-ECC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 9th International Japan-Africa Conference on Electronics, Communications, and Computations (JAC-ECC)
Y1  - 13-14 Dec. 2021
AB  - The Tomographic reconstruction of the image using Inverse Synthetic Aperture Radar Methods aims to increase the resolution over the target area. In this paper, the algorithm, theoretical study, methodology, and techniques of tomographic imaging were originally developed in the context of tomographic imaging synthetic aperture radar methods, and have been used in radar imaging system reconstruction. So, the purpose of this presentation is to explore the application of tomographic imaging techniques to learn about the target or familiarize the flight path of aircraft. Tomography image reconstruction methods have been applied to creating three-dimensional models at the range of microwave to several different applications, air defense, and moving targets. Moreover, the goal of this study is to create a comprehensive programming package to work with Inverse Synthetic Aperture Radar Systems. Design and implementation of the system to use for obtaining the form of the object and analysis have been presented. Also, of interest is the improvement in target classification performance afforded by tomographic imaging. In addition to what was mentioned, the performance and critical factors, and identifying promising areas for future research have been presented and achieved.
ER  - 


TY  - CONF
TI  - Towards an Automatic Aircraft Wreckage Detection Using A Monocular Camera of UAV
T2  - 2019 International Electronics Symposium (IES)
SP  - 501
EP  - 504
AU  - A. Risnumawan
AU  - M. I. Perdana
AU  - A. H. Hidayatulloh
AU  - A. K. Rizal
AU  - I. A. Sulistijono
PY  - 2019
KW  - Wreckage aircraft detection
KW  - UAV image
KW  - a monocular camera
KW  - real-time detector
KW  - cascaded classifiers
DO  - 10.1109/ELECSYM.2019.8901632
JO  - 2019 International Electronics Symposium (IES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 International Electronics Symposium (IES)
Y1  - 27-28 Sept. 2019
AB  - Accidents occur suddenly and cannot be avoided either in the air, land, and water. The first step taken by the rescue team is to determine the location of the accident. Crash debris must be discovered even though it is on challenging terrains, such as a mountain, forest, or sea. In practice, a large number of casualties is caused by the delay in handling victims who are in unknown locations. Exploration is hampered due to the vastness of the search area, lack of technology, and the terrain that is difficult to reach by rescue teams. Therefore, in this paper, a wreckage aircraft detection system will be studied using only visual information from air sensing. A single monocular camera mounted on a UAV is employed in the system. Haar-like features and cascaded classifiers that have been widely used for object detection is studied due to its efficiency and practical implementation for most embedded platform attached in UAVs. We believe that this research will help accelerate finding accident sites so that the process of handling victims becomes more useful for the search and rescue team.
ER  - 


TY  - JOUR
TI  - Improving Few-Shot Remote Sensing Scene Classification With Class Name Semantics
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 12
AU  - J. Chen
AU  - Y. Guo
AU  - J. Zhu
AU  - G. Sun
AU  - D. Qin
AU  - M. Deng
AU  - H. Liu
PY  - 2022
KW  - Semantics
KW  - Prototypes
KW  - Remote sensing
KW  - Task analysis
KW  - Visualization
KW  - Image analysis
KW  - Feature extraction
KW  - Few-shot learning
KW  - remote sensing scene classification (RSSC)
KW  - semantics information
DO  - 10.1109/TGRS.2022.3219726
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - Few-shot remote sensing scene classification (FSRSSC) has been used for new class recognition in the presence of a limited number of labeled samples. The representation vector (prototype) of categories obtained using images only confronts some challenges, such as insufficient generalization when the number of samples is too small. To address this problem, we propose a new FSRSSC method based on prototype networks, named improved prototype network with class name semantics (CNSPN), which combines semantic information of class names (name of the scene categories, such as aircraft, harbor, and bridge). First, CNSPN extracts semantics for class names using a pretrained word-embedding model, which enriches the feature representation ability of the category at the source. Then, an enhanced fusion prototype is generated by fusing the semantic information of text and visual information in the image through a multimodal prototype fusion module (MPFM). Finally, the query image is classified by measuring the distance between the query sample and the visual prototype, and between the query sample and the fusion prototype. Comparative experiments on the Northwestern Polytechnical University (NWPU)-remote sensing image scene classification (RESISC)45 and remote sensing dataset (RSD)46-Wuhan University (WHU) datasets show that the proposed method significantly improves FSRSSC performance. Code is available at https://github.com/RS-CSU/CNSPN.git.
ER  - 


TY  - CONF
TI  - For Safer Navigation: Pedestrian-View Intersection Classification
T2  - 2020 International Conference on Information and Communication Technology Convergence (ICTC)
SP  - 7
EP  - 10
AU  - M. Astrid
AU  - J. -H. Lee
AU  - M. Zaigham Zaheer
AU  - J. -Y. Lee
AU  - S. -I. Lee
PY  - 2020
KW  - Roads
KW  - Aircraft navigation
KW  - Information and communication technology
KW  - Task analysis
KW  - Autonomous vehicles
KW  - Convergence
KW  - Autonomous robots
KW  - intersection classification
KW  - transfer learning
DO  - 10.1109/ICTC49870.2020.9289182
JO  - 2020 International Conference on Information and Communication Technology Convergence (ICTC)
IS  - 
SN  - 2162-1233
VO  - 
VL  - 
JA  - 2020 International Conference on Information and Communication Technology Convergence (ICTC)
Y1  - 21-23 Oct. 2020
AB  - Intersection classification is one of the key components of autonomous navigation. Several related research works have been conducted as a prior task to solve problems such as autonomous driving, aircraft hovering, and navigating in mines. However, to the best of our knowledge, none of these studies support pedestrian-view navigation to guide the small and slow robots for which it is too dangerous to be operated on normal roads along with normal vehicles. To address this problem, we propose: 1) a pedestrian-view-level intersection classification image dataset, 2) ResNet-based architecture fine-tuned on the proposed dataset, and 3) thorough experimentation to explore the capabilities of our proposed architecture. The detailed analysis reported in this paper enabled us to find the network configuration that is neither underfit nor overfit to our data and achieves 80% test accuracy.
ER  - 


TY  - CONF
TI  - Low-cost database-free automatic target classification using 3D-ISAR
T2  - 2022 23rd International Radar Symposium (IRS)
SP  - 178
EP  - 183
AU  - S. Ghio
AU  - E. Giusti
AU  - M. Martorella
PY  - 2022
KW  - Airplanes
KW  - Three-dimensional displays
KW  - Design automation
KW  - Costs
KW  - Databases
KW  - Shape
KW  - Radar
KW  - ATR
KW  - ATC
KW  - radar imaging
KW  - 3D
KW  - interferometry
DO  - 10.23919/IRS54158.2022.9905056
JO  - 2022 23rd International Radar Symposium (IRS)
IS  - 
SN  - 2155-5753
VO  - 
VL  - 
JA  - 2022 23rd International Radar Symposium (IRS)
Y1  - 12-14 Sept. 2022
AB  - Automatic Target Classification / Recognition (ATC/R) algorithms based on ISAR 2D images have proven effective but require complex databases. Moving from 2D to 3D systems, can reduce the database size and the costs since target's Computer-Aided Design (CAD) models can be directly used without introducing variations due to the target aspect angle or projection onto the Image Projection Plane (IPP). In this context, we would like to propose a solution that does not need a complex database and that requires only some easily available a priori knowledge about the targets, i.e. target main sizes and generic 2D shapes that can be representative of a target class. This method does not need a training phase and has very low requirements in terms of system memory, system cost and deployment time. This method is therefore useful in situations where there is the need to quickly build an ATC/R system. In this paper, the proposed method has been tested using simulated 3D InISAR reconstructions of airplanes and ships, but the same approach can be easily extended to other target classes because of its easy set-up.
ER  - 


TY  - CONF
TI  - Construction Methods of Defective datasets for Deep Learning Vision Algorithms
T2  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
SP  - 93
EP  - 102
AU  - M. C. Jiang
AU  - L. Zhong Meng
AU  - G. Yang
AU  - H. Y. Yu
AU  - Y. Shi
AU  - P. Q. Wang
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Embedded systems
KW  - Semantic segmentation
KW  - Object detection
KW  - Inference algorithms
KW  - Classification algorithms
KW  - Object tracking
KW  - Image fusion
KW  - Image classification
KW  - Deep Learning
KW  - Vision Tasks
KW  - Defective datasets
DO  - 10.1109/ICITES62688.2024.10777417
JO  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
Y1  - 20-23 Sept. 2024
AB  - In response to the lack of defective datasets for deep learning vision algorithms, we researched methods for constructing such datasets. We analyzed the reasons behind the failures of deep learning vision algorithms and summarized 18 types of defects that lead to these failures. These defects were categorized into four major types: dataset, algorithm training, algorithm inference, and algorithm deployment. We constructed 266 defective datasets based on six typical vision tasks: image classification, object detection, object tracking, object extraction, semantic segmentation, and image fusion. Through comparative experiments, we verified the impact of these defective datasets on the performance of various vision algorithms.
ER  - 


TY  - CONF
TI  - Target automatic recognition based on ISAR image with wavelet transform and MBLBP
T2  - 2010 International Symposium on Signals, Systems and Electronics
SP  - 1
EP  - 4
AU  - F. Wang
AU  - W. Sheng
AU  - X. Ma
AU  - H. Wang
PY  - 2010
KW  - Feature extraction
KW  - Histograms
KW  - Image recognition
KW  - Wavelet transforms
KW  - Pixel
KW  - Pattern recognition
DO  - 10.1109/ISSSE.2010.5606988
JO  - 2010 International Symposium on Signals, Systems and Electronics
IS  - 
SN  - 2161-0827
VO  - 2
VL  - 2
JA  - 2010 International Symposium on Signals, Systems and Electronics
Y1  - 17-20 Sept. 2010
AB  - A novel feature extraction algorithm of Inverse Synthetic Aperture Radar (ISAR) image based on wavelet transform (WT) and multi-scale block local binary pattern (MB-LBP) is proposed in this paper. Firstly, the mathematical morphology method is adopted to enhance ISAR image. Secondly, 2D wavelet transform is used to get the low frequency sub-band image from the enhancement ISAR image. Then, ISAR image is divided into small regions from which block local binary pattern (BLBP) with different weight histograms are extracted and concatenated into a feature vector to be used as an efficient ISAR descriptor. At last the classification is performed by using a nearest neighbor classifier with Chi square as a dissimilarity measure in the computed feature space. The experimental results on three kinds of aircraft targets show that the method performs very well.
ER  - 


TY  - CONF
TI  - Regional land use/cover classification in Malaysia Based on conventional digital camera imageries
T2  - 2009 IEEE Aerospace conference
SP  - 1
EP  - 7
AU  - H. S. Lim
AU  - M. Z. MatJafri
AU  - K. Abdullah
AU  - C. J. Wong
AU  - N. M. Saleh
PY  - 2009
KW  - Digital cameras
KW  - Remote sensing
KW  - Environmental economics
KW  - Digital images
KW  - Image sensors
KW  - Layout
KW  - Image resolution
KW  - Testing
KW  - Rivers
KW  - Lakes
DO  - 10.1109/AERO.2009.4839458
JO  - 2009 IEEE Aerospace conference
IS  - 
SN  - 1095-323X
VO  - 
VL  - 
JA  - 2009 IEEE Aerospace conference
Y1  - 7-14 March 2009
AB  - This paper presents an economical analysis of land cover in Malaysia. Land cover classification from remotely sensed data is an important topic in remote sensing applications. We attempted to investigate the feasibility of using a conventional digital camera for acquiring high resolution imagery for land use/cover mapping. The objective of this study is to test the high-resolution digital camera imagery for land cover mapping using remote sensing technique. The study area is the Merbok River estuary, Kedah and Timah Tasoh Lake, Perlis, both located in Peninsular Malaysia. The digital images were taken from a low-attitude light aircraft. A Kodak camera, model DC290, was used to capture images from an elevation of 8000 feet on board Cessna 172Q. The use of a digital camera as a sensor to capture digital images is cheaper and more economical compared to the use of other airborne sensors. This technique overcomes the problem of the difficulty in obtaining cloud-free scenes in the Equatorial region from a satellite platform. The images consisted of the three visible bands-red, green, and blue. Supervised classification technique (maximum likelihood, ML, minimum distance-to-mean, MDM, and parallelepiped, P) was applied to the digital camera spectral bands (red, green and blue) to extract the thematic information from the acquired scenes. The accuracy of each classification map produced was validated using the reference data sets consisting of a large number of samples collected per category. The results produced a high degree of accuracy. This study indicates that the use of a conventional digital camera as a sensor in remote sensing studies can provide useful information for planning and development of a small area of coverage.
ER  - 


TY  - CONF
TI  - Deep Learning-Based Methodological Approach for Vineyard Early Disease Detection Using Hyperspectral Data
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 9063
EP  - 9066
AU  - J. Hruška
AU  - T. Adão
AU  - L. Pádua
AU  - P. Marques
AU  - E. Peres
AU  - A. Sousa
AU  - R. Morais
AU  - J. J. Sousa
PY  - 2018
KW  - Hyperspectral imaging
KW  - Diseases
KW  - Image classification
KW  - Data processing
KW  - Feature extraction
KW  - Hyperspectral data
KW  - remote sensing
KW  - agriculture
KW  - forestry
KW  - machine learning
KW  - deep learning
DO  - 10.1109/IGARSS.2018.8519136
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - Machine Learning (ML) progressed significantly in the last decade, evolving the computer-based learning/prediction paradigm to a much more effective class of models known as Deep learning (DL). Since then, hyperspectral data processing relying on DL approaches is getting more popular, competing with the traditional classification techniques. In this paper, a valid ML/DL-based works applied to hyperspectral data processing is reviewed in order to get an insight regarding the approaches available for the effective meaning extraction from this type of data. Next, a general DL-based methodology focusing on hyperspectral data processing to provide farmers and winemakers effective tools for earlier threat detection is proposed.
ER  - 


TY  - CONF
TI  - A subpixel precision image matching-aided navigation method
T2  - Proceedings of the 33rd Chinese Control Conference
SP  - 556
EP  - 561
AU  - B. Wang
AU  - X. Leng
AU  - S. Wu
AU  - X. Mao
PY  - 2014
KW  - Image matching
KW  - Equations
KW  - Mathematical model
KW  - Navigation
KW  - Classification algorithms
KW  - Real-time systems
KW  - Accuracy
KW  - image matching-aided navigation
KW  - four-dimension real matrix
KW  - the least square algorithm
DO  - 10.1109/ChiCC.2014.6896684
JO  - Proceedings of the 33rd Chinese Control Conference
IS  - 
SN  - 1934-1768
VO  - 
VL  - 
JA  - Proceedings of the 33rd Chinese Control Conference
Y1  - 28-30 July 2014
AB  - Image matching-aided navigation system has broad application prospects, according to its prominent advantages in autonomy and high precision. In order to meet the demands of real time and accuracy of image aided navigation system, the paper propose a subpixel precision image matching algorithm by using a method called the four-dimension real matrix. At first this paper converts the transformation of aircraft's position and attitude to transformation of two images. Then an optimal transform matrix which is called a four-dimension real matrix can be obtained by the least square algorithm to fit the two images. At last, the deviations of aircraft's position and attitude are derived according to the proposed model. Experiments have been conducted and the results show that the time consumption is within a second, translation deviations are within a pixel, and angle deviations are within a degree, which illustrates the good performance of the algorithm and the model. As a conclusion, the algorithm satisfies the requirements of real-time situation and accuracy of the image matching-aided navigation system.
ER  - 


TY  - CONF
TI  - A hierarchical approach for visual suspicious behavior detection in aircrafts
T2  - 2009 16th International Conference on Digital Signal Processing
SP  - 1
EP  - 7
AU  - D. Arsic
AU  - B. Hornler
AU  - B. Schuller
AU  - G. Rigoll
PY  - 2009
KW  - Aircraft
KW  - Surveillance
KW  - Transportation
KW  - Robustness
KW  - Skin
KW  - Support vector machines
KW  - Support vector machine classification
KW  - Testing
KW  - Image databases
KW  - Spatial databases
KW  - Behavior Detection
KW  - Low Level Features
KW  - Fusion
KW  - SVM
KW  - Surveillance
DO  - 10.1109/ICDSP.2009.5201258
JO  - 2009 16th International Conference on Digital Signal Processing
IS  - 
SN  - 2165-3577
VO  - 
VL  - 
JA  - 2009 16th International Conference on Digital Signal Processing
Y1  - 5-7 July 2009
AB  - Recently great interest has been shown in the visual surveillance of public transportation systems. The challenge is the automated analysis of passenger's behaviors with a set of visual low-level features, which can be extracted robustly. On a set of global motion features computed in different parts of the image, here the complete image, the face and skin color regions, a classification with support vector machines is performed. Test-runs on a database of aggressive, cheerful, intoxicated, nervous, neutral and tired behavior.
ER  - 


TY  - CONF
TI  - Performances of frequency-based contextual classifier for land use/land cover using Cropcam UAV data
T2  - Proceeding of the 2011 IEEE International Conference on Space Science and Communication (IconSpace)
SP  - 14
EP  - 19
AU  - F. M. Hassan
AU  - M. Z. MatJafri
AU  - H. S. Lim
AU  - M. R. Mustapha
PY  - 2011
KW  - Accuracy
KW  - Spatial resolution
KW  - Software
KW  - Digital images
KW  - Remote sensing
KW  - Digital cameras
KW  - Cropcam UAV
KW  - LULC
KW  - FBC
DO  - 10.1109/IConSpace.2011.6015843
JO  - Proceeding of the 2011 IEEE International Conference on Space Science and Communication (IconSpace)
IS  - 
SN  - 2165-431X
VO  - 
VL  - 
JA  - Proceeding of the 2011 IEEE International Conference on Space Science and Communication (IconSpace)
Y1  - 12-13 July 2011
AB  - Traditional aerial images provided by satellite, manned aircraft or stock photography are often expensive, difficult to obtain or outdated. The Cropcam provides GPS based digital images on demand and real time data with high temporal resolution throughout the equatorial region where the sky is often covered by clouds. The images obtained by the Cropcam will allow producers to detect, locate, and have a better assessment of the actions required to overcome the problem of unclear images obtained by the satellite and manned aircraft in this area. A Pentax digital camera, model Optio A40, was used to capture images from the height of 320 meters on board the Cropcam UAV autopilot. The objective of this study is to evaluate the land use /land cover (LULC) features over Penang Island using the images obtained during the platform flying mission. The study also tests the effectiveness of Frequency-Based contextual classifier instead of conventional methods in a classification process in order to overcome or minimize the difficulty in classification of the mixed pixel area using high resolution images with spatial ground 8 cm. The technique was applied to the digital camera spectral bands (red, green and blue) to extract thematic information from the acquired scene by using PCI Geomatica 10.3 image processing software. Training sites were selected within each scene, and four LULC classes were assigned to each classifier. The accuracy assessment of each classification map produced was validated using the reference data sets consisting of a large number of samples collected per category. The results showed that the Frequency-Based contextual classifier produced superior results and achieved a high degree of accuracy. The study revealed that the Frequency-Based contextual classifier is effective and could be used for LULC classification using high resolution images of a small area of coverage acquired by the CropCam UAV.
ER  - 


TY  - CONF
TI  - A Finite Mixtures Algorithm For Finding Classes In Images
T2  - [Proceedings] IGARSS'91 Remote Sensing: Global Monitoring for Earth Management
SP  - 1835
EP  - 1838
AU  - R. Samadani
AU  - J. F. Vesecky
PY  - 1991
KW  - Pixel
KW  - Sea ice
KW  - Aircraft propulsion
KW  - Laboratories
KW  - Density functional theory
KW  - Maximum likelihood estimation
KW  - Computer errors
KW  - Image segmentation
KW  - L-band
KW  - Polarization
DO  - 10.1109/IGARSS.1991.579602
JO  - [Proceedings] IGARSS'91 Remote Sensing: Global Monitoring for Earth Management
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - [Proceedings] IGARSS'91 Remote Sensing: Global Monitoring for Earth Management
Y1  - 3-6 June 1991
AB  - 
ER  - 


TY  - CONF
TI  - Multi-class classification of vegetation in natural environments using an Unmanned Aerial system
T2  - 2011 IEEE International Conference on Robotics and Automation
SP  - 2953
EP  - 2959
AU  - A. Reid
AU  - F. Ramos
AU  - S. Sukkarieh
PY  - 2011
KW  - Training
KW  - Image color analysis
KW  - Vegetation mapping
KW  - Vegetation
KW  - Probabilistic logic
KW  - Image segmentation
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2011.5980061
JO  - 2011 IEEE International Conference on Robotics and Automation
IS  - 
SN  - 1050-4729
VO  - 
VL  - 
JA  - 2011 IEEE International Conference on Robotics and Automation
Y1  - 9-13 May 2011
AB  - This paper presents an automated approach for the classification of vegetation in natural environments based on high resolution aerial imagery acquired by a low flying Unmanned Aerial Vehicle (UAV). Standard colour and texture descriptors are extracted on a frame by frame basis to build a representation of appearance, which is probabilistically classified by a novel multi-class generalisation of the Gaussian Process (GP) developed for this work. A GP approach was selected for probabilistic outputs, and the ability to automatically determine the relevance of each input dimension to each of the C classes in the problem. When learning hyperparameters from N training examples, the new formulation scales at O(N ), rather than O(CN3) for the standard one-vs-all approach. The novel classification framework is trained and validated on a set of manual labels, and then queried to visualise a map of vegetation type under the UAV flight path. Mapping results are presented for a region of farmland in Northern Queensland, Australia that is infested with two invasive introduced tree species.
ER  - 


TY  - CONF
TI  - Fusion of Global and Local Feature Using KCCA for Automatic Target Recognition
T2  - 2009 Fifth International Conference on Image and Graphics
SP  - 958
EP  - 962
AU  - J. Zhao
AU  - Y. Fan
AU  - W. Fan
PY  - 2009
KW  - Target recognition
KW  - Support vector machines
KW  - Support vector machine classification
KW  - Kernel
KW  - Algorithm design and analysis
KW  - Feature extraction
KW  - Image analysis
KW  - Aircraft
KW  - Image recognition
KW  - Object recognition
KW  - kernel canonical correlation analysis
KW  - feature fusion
KW  - support vector machine
KW  - automatic target recognition
DO  - 10.1109/ICIG.2009.149
JO  - 2009 Fifth International Conference on Image and Graphics
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2009 Fifth International Conference on Image and Graphics
Y1  - 20-23 Sept. 2009
AB  - Based on the ideas of feature fusion and Kernel Canonical Correlation Analysis (KCCA), a novel framework for fusing global and local features on Automatic Target Recognition (ATR) algorithm is proposed. Firstly, the feature fusion method based on KCCA is established, then pseudo Zernike moments and Scale Invariant Feature Transform (SIFT) are extracted as global features and local features. K-means algorithm is applied to normalize the local features to obtain the same form as global features. After the fusion of two features, one-against-all Support Vector Machine (SVM) is employed as classifier for the Multi-class target recognition. Theoretical analysis and experiments on aircraft images results show that KCCA features fusion representations significantly outperform CCA fusion method and single feature approach. Feature fusion of global features and local features based on target image for recognition are proved to be a promising strategy in object recognition field.
ER  - 


TY  - CONF
TI  - Gradient-direction-based Rectangles and Triangles Traffic Signs Detection Algorithm in Natural Scenes
T2  - 2019 Chinese Automation Congress (CAC)
SP  - 1115
EP  - 1120
AU  - Y. Jin
AU  - X. Guan
AU  - H. Zhang
PY  - 2019
KW  - Image segmentation
KW  - Feature extraction
KW  - Image edge detection
KW  - Image color analysis
KW  - Detection algorithms
KW  - Shape
KW  - Gray-scale
KW  - image convolution
KW  - filter design
KW  - image processing
KW  - rectangles detection
KW  - triangles detection
KW  - traffic sign
DO  - 10.1109/CAC48633.2019.8997502
JO  - 2019 Chinese Automation Congress (CAC)
IS  - 
SN  - 2688-0938
VO  - 
VL  - 
JA  - 2019 Chinese Automation Congress (CAC)
Y1  - 22-24 Nov. 2019
AB  - In this paper, we propose a novel method to detect rectangular and triangular traffic signs in natural scenes according to the geometric features of the rectangle and triangle traffic signs. It is a Gradient-direction-based method that mainly includes four stages: gradient information classification, filter templates-based line segments with different orientation extraction, structural corners detection and rectangle/triangle detection. The extracted corners are categorized into different sets according to the orientation of the line segments, by which we can quickly extract rectangle and triangle traffic signs in image. And the experimental results demonstrate under different conditions are shown that the proposed method can detect rectangular and triangular traffic signs in natural scenes efficiently and robustness.
ER  - 


TY  - CONF
TI  - A New Hybrid Approach to Radar Target Classification for the Estimation of Scattering Centers
T2  - 2006 IEEE 14th Signal Processing and Communications Applications
SP  - 1
EP  - 4
AU  - O. Gultekin
AU  - T. Gunel
AU  - I. Erer
PY  - 2006
KW  - Radar scattering
KW  - Radar imaging
KW  - Radar applications
KW  - Scattering parameters
KW  - Spaceborne radar
KW  - Image coding
KW  - Brain modeling
KW  - Multiple signal classification
KW  - Convergence
KW  - Artificial neural networks
DO  - 10.1109/SIU.2006.1659770
JO  - 2006 IEEE 14th Signal Processing and Communications Applications
IS  - 
SN  - 2165-0608
VO  - 
VL  - 
JA  - 2006 IEEE 14th Signal Processing and Communications Applications
Y1  - 17-19 April 2006
AB  - Radar images, range profiles and scattering centers are used as feature parameters in radar target classification applications. Scattering center parameters, when used as feature parameters, enable an efficient compression of feature space compared to classical target classification methods based on radar images and range profiles. A method used for the estimation of scattering centers via cancellation of side lobes is the CLEAN algorithm. In this work, model based Prony, MUSIC, ESPRIT and evolutionary based CLEAN methods are applied for the estimation of scattering centers. A hybrid method is proposed which improves the convergence of evolutionary based CLEAN. Scattering centers which are estimated by aforementioned methods are classified using correlation based matching score method, Bayes classifier and artificial neural networks. Classification is accomplished using simulated data of four different aircraft models created by the point target model at different frequency bands and aspect angles
ER  - 


TY  - CONF
TI  - Research on Recognition and Classification of Active Targets in Airport Flight Area Based on Improved Deep Learning Algorithm
T2  - 2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
SP  - 1517
EP  - 1521
AU  - M. Zhang
AU  - X. Jiang
PY  - 2024
KW  - YOLO
KW  - Deep learning
KW  - Training
KW  - Accuracy
KW  - Target recognition
KW  - Transportation
KW  - Detectors
KW  - YOLOv5
KW  - Deep Learning
KW  - Object Recognition
KW  - Object Classification
DO  - 10.1109/AINIT61980.2024.10581432
JO  - 2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
Y1  - 29-31 March 2024
AB  - Today, in the 21st century, China's national economy continues to maintain stable growth, and the public generally pursues comfort and short-term travel, so the airplane has become the preferred means of transportation for the public. To realize the rapid recognition and classification of aircraft targets in aerial images, an aircraft target recognition method based on YOLOv5 framework was proposed. As a typical representative of fine-grained image classification, the accuracy of aircraft classification is seriously affected by its shape, color, and strong interference factors of arbitrary modification. In order to improve the accuracy of target recognition, overlapping cropping, rotation, scaling, adjusting saturation and brightness, and increasing noise were used to increase the number of sample features: in order to solve the problem of excessive change in the size of the aircraft target, samples of different scales were put into the YOLOv5 framework for training, and the corresponding detectors were obtained, and the prediction results of multiple detectors were synthesized, and the position and category of the target frame were determined by non-maximum suppression. A small sample set containing a variety of aircraft types was selected and annotated from the public remote sensing dataset, and the results were tested. Experimental results show that the deep learning algorithm performs well in the target recognition task of the airport flight area, and can realize the task of classification of active targets in the flight area.
ER  - 


TY  - CONF
TI  - A Fuzzy-Rough-Based Approach for Uncertainty Classification on Hybrid Information System
T2  - 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)
SP  - 791
EP  - 796
AU  - R. He
AU  - C. Xu
AU  - D. Li
AU  - W. Hou
AU  - X. Yu
AU  - H. Zhang
PY  - 2018
KW  - Classification algorithms
KW  - Approximation algorithms
KW  - Prediction algorithms
KW  - Information systems
KW  - Rough sets
KW  - Task analysis
KW  - Uncertainty
KW  - hybrid information system
KW  - heterogeneous distance
KW  - fuzzy-rough set
KW  - FRNN-HIS algorithm
DO  - 10.1109/ICIVC.2018.8492824
JO  - 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)
Y1  - 27-29 June 2018
AB  - Hybrid information system (HIS) contains variety types of data including boolean, categorical, missing, real-valued, and set-based data, which are becoming very important for meaningful information analysis in real applications. The defect of the previous classification algorithms mainly lies in that the feature types of dataset haven't been comprehensively considered. In this paper, the internal relations among objects based on features that can reflect multiple data types have been expressed by heterogeneous distance function. And then, a new fuzzy-rough relation calculation under HIS and a new fuzzy-rough lower and upper approximation have been analyzed. Finally, the fuzzy-rough nearest neighbor classification under HIS (FRNN-HIS) algorithm has been proposed. Eight datasets has been adopted to conduct experiments. The experiments have shown that the proposed FRNN-HIS algorithm can significantly outperform FRNN and VQNN algorithms in terms of predicting classification results.
ER  - 


TY  - CONF
TI  - Small Format Air Photo From Ultralight Aircraft As An Aid For Data Collection Of Agricultural Statistics In Sahelian Countries
T2  - International Geoscience and Remote Sensing Symposium, 'Remote Sensing: Moving Toward the 21st Century'.
SP  - 269
EP  - 270
AU  - E. Bartholome
AU  - J. . -M. Gregoire
AU  - R. Zeyen
PY  - 1988
KW  - Aircraft
KW  - Crops
KW  - Statistics
KW  - Photography
KW  - Cameras
KW  - Production
KW  - Remote sensing
KW  - Remote monitoring
KW  - Data analysis
KW  - Africa
DO  - 10.1109/IGARSS.1988.570112
JO  - International Geoscience and Remote Sensing Symposium, 'Remote Sensing: Moving Toward the 21st Century'.
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - International Geoscience and Remote Sensing Symposium, 'Remote Sensing: Moving Toward the 21st Century'.
Y1  - 12-16 Sept. 1988
AB  - 
ER  - 


TY  - JOUR
TI  - EFM-Net: An Essential Feature Mining Network for Target Fine-Grained Classification in Optical Remote Sensing Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 16
AU  - Y. Yi
AU  - Y. You
AU  - C. Li
AU  - W. Zhou
PY  - 2023
KW  - Feature extraction
KW  - Remote sensing
KW  - Task analysis
KW  - Marine vehicles
KW  - Data mining
KW  - Semantics
KW  - Training
KW  - Attention mechanism
KW  - data augmentation
KW  - deep learning
KW  - essential feature extraction
KW  - fine-grained target classification
DO  - 10.1109/TGRS.2023.3265669
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 61
VL  - 61
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2023
AB  - Target fine-grained classification has been the research hotspot in remote sensing image interpretation, which has received general attention in application fields. One challenge of the fine-grained classification task is to learn the most discriminative feature using the deep convolutional neural network (DCNN). At present, many works of fine-grained image classification obtain target features by optimizing the feature extraction and enhancement, which are not accurate enough in remote sensing images. In this article, we propose an essential feature mining network (EFM-net) based on DCNN to address this issue. Its major motivation is to obtain the essential feature, which is fine enough to distinguish between similar instances. The proposed pipeline includes the Miner for purifying the essential feature and the Refiner for data augmentation. These two modules can work in a mutually reinforcing way and extract the essential feature of targets. We evaluate EFM-Net on two public fine-grained classification datasets in remote sensing, FGSC-23 and FGSCR-42, and our Aircraft-16. The results show that the proposed method consistently outperforms existing alternatives. We have released our source code in GitHub https://github.com/JACYI/EFM-Net-Pytorch.git.
ER  - 


TY  - CONF
TI  - Target Classification for Air Defence Radars
T2  - 2006 IET Seminar on High Resolution Imaging and Target Classification
SP  - 3
EP  - 16
AU  - P. Tait
PY  - 2006
DO  - 
JO  - 2006 IET Seminar on High Resolution Imaging and Target Classification
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2006 IET Seminar on High Resolution Imaging and Target Classification
Y1  - 21-21 Nov. 2006
AB  - Radar is evolving rapidly from being a sensor which detects and tracks targets to a high resolution imaging sensor with target classification capabilities. This paper addresses the techniques being developed for air defence radars. The application of target classification functions in fire control radars which support missile systems and multi-function phased array radars is discussed. The main high resolution techniques employed are range profiling, ISAR, frequency/spectral analysis and range-frequency imaging. Typical aircraft images measured are presented. The level of maturity of the hardware and signal processing technology to support these modes is discussed. The key technical, operational and practical issues which drive the design and application of high resolution modes for these types of radars are presented. Factors which determine the type of target classification mode which can be employed include the: operational frequency of the radar, time available for the application of the high resolution waveform, pulse repetition frequency, radar bandwidth available, basic architecture of the radar, and mission of the radar system. The other critical issue for the practical employment of target classification modes is that of the target signature database. Classifier algorithms are used to provide the match between the signature measured by the radar and reference signatures of targets of interest held in a database to provide an estimate of the target class. The assembly and maintenance of the target signature database is discussed and techniques employed are presented
ER  - 


TY  - JOUR
TI  - Localization-Aware Adaptive Pairwise Margin Loss for Fine-Grained Image Recognition
T2  - IEEE Access
SP  - 8786
EP  - 8796
AU  - T. Kim
AU  - H. Kim
AU  - H. Byun
PY  - 2021
KW  - Image recognition
KW  - Measurement
KW  - Task analysis
KW  - Semantics
KW  - Location awareness
KW  - Training
KW  - Adaptive margin
KW  - deep neural networks
KW  - fine-grained image recognition
KW  - metric learning
KW  - image augmentation
KW  - image generation
DO  - 10.1109/ACCESS.2021.3049305
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - Fine-grained image recognition is a highly challenging problem due to subtle differences between images. There are many attempts to solve fine-grained image recognition problems using data augmentation, jointly optimizing deep metric learning. CutMix is one of the excellent data augmentation strategies which crops and merges to generate new images. However, it sometimes generates meaningless and obscured object images that degrade recognition performance. We propose a novel framework that solves the above problem and expands the CutMix leveraging localizing method. Also, we improve the recognition accuracy to joint optimizing with a pairwise margin loss using generated images from the improved CutMix. There are some images similar to the reference image among the generated images. They are generated by replacing similar parts from the reference image. Those generated images should not be located much farther than the margin value in embedding space because those generated images and a reference image have similar semantic meaning. However, the conventional margin loss can not consider those images which are located much farther than the margin. To solve this problem, we propose an additional margin loss to consider those generated images. The proposed framework consists of two stages: the part localization-aware CutMix and an adaptive pairwise margin loss. The proposed method achieves state-of-the-art performance on the CUB-200-2011, FGVC-Aircraft, Stanford Cars, and DeepFashion datasets. Furthermore, extensive experiments demonstrate that each stage improves the final performance.
ER  - 


TY  - CONF
TI  - SIRT: Machine-Learning-based Selective Intensity Range Thresholding for Aircraft Visual Docking Guidance Refinement and Interpretation
T2  - 2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
SP  - 1
EP  - 6
AU  - D. Pal
AU  - A. Singh
AU  - A. Alladi
PY  - 2022
KW  - Visualization
KW  - Thresholding (Imaging)
KW  - Text recognition
KW  - Image color analysis
KW  - Pipelines
KW  - Lighting
KW  - Light emitting diodes
KW  - Image thresholding
KW  - Visual docking guidance system
KW  - Dot-matrix display
KW  - Advanced parking guidance system
DO  - 10.1109/CONECCT55679.2022.9865711
JO  - 2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
IS  - 
SN  - 2766-2101
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)
Y1  - 8-10 July 2022
AB  - We propose an automated Visual Docking Guidance System (VDGS) message interpretation service towards a smart and safe aviation endeavor. Owing to the priority of aviation safety criticality and the simultaneous need for smart airport, ground marshallers are getting replaced by VDGS to automatically detect obstacles, probable wingtip collisions and provide suitable assisted parking guidance to pilots. Nevertheless, the discrete presence of light-emitting diodes (LED) in VDGS dot-matrix display coupled with adversarial climatic conditions and far, low-light visibility creates the barrier in automated message detection and interpretation services. In this paper, we propose a novel Selective Intensity Range Thresholding (SIRT) for learning degraded LED pixel intensities and generate a refined synthetic display image. Furthermore, we propose an end-to-end pipeline for automatic recognition of VDGS alphanumeric texts and interpretation of symbologies to infer docking messages. The proposed solution aims to provide automated notification to the pilots about interpreted situational awareness messages from VDGS and record VDGS LED malfunction for rectification.
ER  - 


TY  - CONF
TI  - Classification and Recognition of Objects on Radar Portraits Formed by the Equipment of Mobile Small-Size Radar Systems
T2  - 2020 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
SP  - 1
EP  - 4
AU  - A. A. Sentsov
AU  - S. A. Ivanov
AU  - S. A. Nenashev
AU  - E. L. Turnetskaya
PY  - 2020
KW  - Training
KW  - Phased arrays
KW  - Software algorithms
KW  - Neural networks
KW  - Airborne radar
KW  - Radar
KW  - Radar equipment
KW  - radar
KW  - neural networks
KW  - unmanned aircraft vehicle
KW  - object recognition and classification.
DO  - 10.1109/WECONF48837.2020.9131475
JO  - 2020 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 Wave Electronics and its Application in Information and Telecommunication Systems (WECONF)
Y1  - 1-5 June 2020
AB  - The article considers the possibility of using algorithms for classifying and recognizing objects on radar portraits formed by the distributed mobile radar stations. The article covers the issues of detecting small-sized aerial objects using radar stations built using the technology of cascaded active phased waveguide-slot antenna arrays. The result of the work is the creation of a methodology and a software and hardware module based on a small-sized mobile intelligent radar system, an assessment of its implementation on a modern domestic element base and the determination of its characteristics. A method for recognizing small-sized objects based on the use of neural network algorithms is proposed.
ER  - 


TY  - CONF
TI  - A Comparison of Machine Learning Algorithms for Micro-Doppler Based Drone Classification
T2  - 2023 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (USNC-URSI)
SP  - 01
EP  - 02
AU  - M. Rashid
AU  - J. A. Nanzer
PY  - 2023
KW  - Training
KW  - Machine learning algorithms
KW  - Rotors
KW  - Kinematics
KW  - Radar scattering
KW  - Feature extraction
KW  - Classification algorithms
KW  - micro-Doppler signatures
KW  - drones classification
KW  - principal component analysis
KW  - machine learning algorithms
DO  - 10.1109/USNC-URSI52151.2023.10237988
JO  - 2023 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (USNC-URSI)
IS  - 
SN  - 1947-1491
VO  - 
VL  - 
JA  - 2023 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (USNC-URSI)
Y1  - 23-28 July 2023
AB  - We characterize the performance of a set of machine learning algorithms for classifying simulated micro-Doppler responses of a set of rotorcrafts. Scattered radar signals are generated using a kinematic model of aircraft rotors as a function of signal-to-noise ratio (SNR). The training and test datasets are transformed using the principal components analysis method to extract the most salient features. The classification accuracy is investigated by varying the SNR of the signals to determine the minimum SNR needed to obtain a classification performance. Our results demonstrate that an SNR of at least - 5 dB is sufficient to achieve high classification accuracy based on simulated micro-Doppler signatures.
ER  - 


TY  - JOUR
TI  - Radar target classification of commercial aircraft
T2  - IEEE Transactions on Aerospace and Electronic Systems
SP  - 598
EP  - 606
AU  - A. Zyweck
AU  - R. E. Bogner
PY  - 1996
KW  - Airborne radar
KW  - Aircraft
KW  - Radar imaging
KW  - Target recognition
KW  - Radar signal processing
KW  - Australia
KW  - Signal resolution
KW  - Airports
KW  - Optical scattering
KW  - Signal processing
DO  - 10.1109/7.489504
JO  - IEEE Transactions on Aerospace and Electronic Systems
IS  - 2
SN  - 1557-9603
VO  - 32
VL  - 32
JA  - IEEE Transactions on Aerospace and Electronic Systems
Y1  - April 1996
AB  - With the increased availability of coherent wideband radars there has been a renewed interest in radar target recognition. A large bandwidth gives high resolution in range which means target discrimination may be possible. Coherence makes cross-range resolution and radar images possible. Some of the problems of classifying high resolution range profiles (HRRPs) are examined and simple preprocessing techniques which may aid subsequent target classification are investigated. These techniques are applied to HRRP data acquired at a local airport using the Microwave Radar Division (MRD) mobile radar facility It is found that Boeing 727 and Boeing 737 aircraft can be reliably distinguished over a range of aspect angles. This augers well for future target classification studies using HRRPs.
ER  - 


TY  - CONF
TI  - Adversarial Complementary Attention-Enhancement Network For Fine-grained Image Recognition
T2  - 2020 8th International Conference on Digital Home (ICDH)
SP  - 63
EP  - 69
AU  - M. Huang
AU  - X. Sun
AU  - Y. Ji
AU  - X. Chen
AU  - S. Yan
PY  - 2020
KW  - Training
KW  - Visualization
KW  - Image recognition
KW  - Annotations
KW  - Feature extraction
KW  - Data mining
KW  - Automobiles
KW  - Fine-grained
KW  - Image recognition
KW  - Adversarial complementary
DO  - 10.1109/ICDH51081.2020.00019
JO  - 2020 8th International Conference on Digital Home (ICDH)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 8th International Conference on Digital Home (ICDH)
Y1  - 19-20 Sept. 2020
AB  - Due to subtle visual differences among fine-grained subcategories only locate at local regions, how to locate and extract differentiated local features has become the primary contradiction to be solved in this field. In the early stage, many works based on weak supervision are inclined to focus only on the most discriminative feature while ignoring other features that play an equally important role in image recognition. To tackle this problem, this paper proposes an adversarial complementary strategy-based method that can urge the network to find a complementary region by removing the most meaningful parts in the image, which can extract more information with significant features to promote the precision of fine-grained image classification. At the same time, similarity loss was utilized to determine the similarity between the two regions to avoid obtaining similar features as much as possible. In addition, Bilinear Attention Pooling (BAP) is introduced to extract sequential part features, which can effectively merge several different levels of features together. Experiments show that our method achieves the competitive results in various challenging datasets, such as CUB-200-2011, FGVC-Aircraft and Stanford Car.
ER  - 


TY  - JOUR
TI  - Automatic Composite-Modulation Classification Using Ultra Lightweight Deep-Learning Network Based on Cyclic-Paw-Print
T2  - IEEE Transactions on Cognitive Communications and Networking
SP  - 866
EP  - 879
AU  - X. Yan
AU  - P. Yang
AU  - X. Zhong
AU  - Q. Wang
AU  - H. -C. Wu
AU  - L. He
PY  - 2024
KW  - Feature extraction
KW  - Frequency modulation
KW  - Space communications
KW  - Signal to noise ratio
KW  - Phase locked loops
KW  - Telemetry
KW  - Satellite communication
KW  - Automatic composite modulation classification (ACMC)
KW  - cyclic spectrum
KW  - cyclic-paw-print (CPP)
KW  - ultra lightweight network (ULWNet)
KW  - cognitive telemetry
KW  - tracking & command (TT&C)
KW  - cognitive space communications
DO  - 10.1109/TCCN.2024.3357850
JO  - IEEE Transactions on Cognitive Communications and Networking
IS  - 3
SN  - 2332-7731
VO  - 10
VL  - 10
JA  - IEEE Transactions on Cognitive Communications and Networking
Y1  - June 2024
AB  - Automatic composite-modulation classification (ACMC) has been considered as an essential function in the next generation intelligent telemetry, tracking & command (TT&C), cognitive space communications, and space surveillance. This paper introduces a novel ACMC scheme using the cyclic-paw-print extracted from the composite-modulation (CM) signals. In this new framework, the cyclic-spectrum analysis is first invoked to acquire the polyspectra of the received CM signals corrupted by different fading channels. Then, a new feature, namely cyclic-paw-print (CPP), is established upon the image representation of the cyclic spectrum, which can be robust against channel noise. Then, a highly-efficient ultra lightweight deep-learning network (ULWNet), which takes the CPPs as the input features, is designed to identify the composite modulation type. Our proposed new scheme can greatly improve the computational efficiencies incurred by the existing deep-learning networks and capture more reliable features latent in CM signals to result in an excellent classification accuracy. Monte Carlo simulation results demonstrate the effectiveness and the superiority of our proposed new ACMC scheme to the existing deep-learning networks.
ER  - 


TY  - JOUR
TI  - Image segmentation by clustering
T2  - Proceedings of the IEEE
SP  - 773
EP  - 785
AU  - G. B. Coleman
AU  - H. C. Andrews
PY  - 1979
KW  - Image segmentation
KW  - Clustering algorithms
KW  - Image recognition
KW  - Mathematical model
KW  - Prototypes
KW  - Humans
KW  - Image converters
KW  - Scattering parameters
KW  - Performance evaluation
KW  - Psychology
DO  - 10.1109/PROC.1979.11327
JO  - Proceedings of the IEEE
IS  - 5
SN  - 1558-2256
VO  - 67
VL  - 67
JA  - Proceedings of the IEEE
Y1  - May 1979
AB  - This paper describes a procedure for segmenting imagery using digital methods and is based on a mathematical-pattern recognition model. The technique does not require training prototypes but operates in an "unsupervised" mode. The features most useful for the given image to be segmented are retained by the algorithm without human interaction, by rejecting those attributes which do not contribute to homogeneous clustering in N-dimensional vector space. The basic procedure is a K-means clustering algorithm which converges to a local minimum in the average squared intercluster distance for a specified number of clusters. The algorithm iterates on the number of clusters, evaluating the clustering based on a parameter of clustering quality. The parameter proposed is a product of between and within cluster scatter measures, which achieves a maximum value that is postulated to represent an intrinsic number of clusters in the data. At this value, feature rejection is implemented via a Bhattacharyya measure to make the image segments more homogeneous (thereby removing "noisy" features); and reclustering is performed. The resulting parameter of clustering fidelity is maximized with segmented imagery resulting in psychovisually pleasing and culturally logical image segments.
ER  - 


TY  - JOUR
TI  - Using multiple-polarization L-band radar to monitor marsh burn recovery
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 635
EP  - 639
AU  - E. W. Ramsey
AU  - G. A. Nelson
AU  - S. K. Sapkota
AU  - S. C. Laine
AU  - J. Verdi
AU  - S. Krasznay
PY  - 1999
KW  - L-band
KW  - Radar
KW  - Remote monitoring
KW  - Biomass
KW  - Sea measurements
KW  - Optical sensors
KW  - Aircraft
KW  - Polarization
KW  - Wildlife
KW  - Fires
DO  - 10.1109/36.739136
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 1
SN  - 1558-0644
VO  - 37
VL  - 37
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - Jan. 1999
AB  - Aircraft L-band VV-, HH-, and VH-polarizations were examined as tools for monitoring burn recovery in a coastal marsh. Significant relationships were observed between time-since-burn (difference between burn and image collection dates; 550-900 days after burn) and returns related to all polarizations. As marsh burn recovery progressed, VV returns decreased while HH and VH returns increased. Radar returns extracted from control sites adjacent to each burn-simulated nonburn marsh and were not individually or in combination significantly related to the time-since-burn. Normalized by the control data, VH-polarization explained up to 83% of the total variations. Overall, the L-band multipolarization radars estimated time-since-burn within /spl plusmn/59 to /spl plusmn/92 days.
ER  - 


TY  - JOUR
TI  - An Uncertainty Quantification Framework for Counter Unmanned Aircraft Systems Using Deep Ensembles
T2  - IEEE Sensors Journal
SP  - 20896
EP  - 20909
AU  - R. Sahay
AU  - J. J. Stubbs
AU  - C. G. Brinton
AU  - G. C. Birch
PY  - 2022
KW  - Uncertainty
KW  - Sensors
KW  - Deep learning
KW  - Training
KW  - Convolutional neural networks
KW  - Testing
KW  - Predictive models
KW  - Adversarial training
KW  - counter unmanned aircraft system (cUAS)
KW  - deep ensembles (DEs)
KW  - electro-optical (EO) sensor data classification
KW  - uncertainty quantification (UQ)
KW  - unmanned aircraft system (UAS) detection
DO  - 10.1109/JSEN.2022.3208527
JO  - IEEE Sensors Journal
IS  - 21
SN  - 1558-1748
VO  - 22
VL  - 22
JA  - IEEE Sensors Journal
Y1  - 1 Nov.1, 2022
AB  - The reliable detection and neutralization of unmanned aircraft systems (UASs), known as counter UAS (cUAS), is pivotal in restricted air spaces. The application of deep learning (DL) classifiers on electro-optical (EO) sensor data is promising for cUAS, but it introduces three key challenges. Specifically, DL-based cUAS produces point estimates at test time with no associated measure of uncertainty (softmax outputs produced by typical DL models are often overconfident predictions, resulting in unreliable measures of uncertainty), easily triggers false positive detections for birds and other aerial wildlife, and cannot accurately characterize out-of-distribution (OOD) input samples. In this work, we develop an epistemic uncertainty quantification (UQ) framework, which utilizes the advantages of DL while simultaneously producing uncertainty estimates on both in-distribution and OOD input samples. In this context, in-distribution samples refer to testing samples collected according to the same data generation process as the training data, and OOD samples refer to in-distribution samples that are intentionally perturbed in order to shift the distribution of the testing set away from the distribution of the training set. Our framework produces a distributive estimate of each prediction, which accurately expresses UQ, as opposed to a point estimate produced by standard DL. Through evaluation on a custom field-collected dataset consisting of images captured from EO sensors and in comparison to prior cUAS baselines, we show that our framework effectively expresses low and high uncertainty on in-distribution and OOD samples, respectively, while retaining accurate classification performance.
ER  - 


TY  - CONF
TI  - Effect of noise in moment invariant neural network aircraft classification
T2  - Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991
SP  - 743
EP  - 749 vol.2
AU  - A. McAuley
AU  - A. Coker
AU  - K. Saruhan
PY  - 1991
KW  - Intelligent networks
KW  - Neural networks
KW  - Noise level
KW  - Noise robustness
KW  - Computer networks
KW  - Optical noise
KW  - Optical computing
KW  - Aircraft propulsion
KW  - Computer science
KW  - Aerospace engineering
DO  - 10.1109/NAECON.1991.165835
JO  - Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991
Y1  - 20-24 May 1991
AB  - An image may be reduced to a small number of moment invariants such that these are independent of the shift, scale, and rotation of an object in the image. However, noise interferes with the ability to provide invariance. The authors examine the effects of noise on the invariance provided by the moment invariants. They then show that rotation invariance is maintained in low levels of noise. They then show that a neural network may be used to provide robustness against noise. The moment invariants, computed for different levels of noise, are used to train a neural network to identify two aircraft. A split inversion algorithm is used because it is much faster than back propagation. The resulting network provides accurate classification in high levels of noise.<>
ER  - 


TY  - CONF
TI  - Fine-Grained Classification Based on Priority Selection of Local Areas
T2  - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
SP  - 1
EP  - 5
AU  - Y. Xu
AU  - H. Shao
AU  - X. Jiang
AU  - X. Tian
AU  - Y. Ji
PY  - 2019
KW  - local areas
KW  - fine-grained classification
KW  - priority selection
DO  - 10.1109/CISP-BMEI48845.2019.8965853
JO  - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
Y1  - 19-21 Oct. 2019
AB  - Fine-grained classification is the classification of subclasses. Since subclasses usually only have subtle differences, the local areas become more important in fine-grained classification. Aiming at the problem that most existing methods cannot obtain reliable local areas, we propose a priority selection algorithm of local areas. Our method first uses the attention model to obtain candidate local areas. Then, according to the classification ability differences of the candidate areas in different subclasses, the geometric relationship between the areas is determined by removing the unnecessary area and the abnormal position area. Finally, the selected areas are classified using a convolutional neural network (CNN). The algorithm we proposed achieves a correct rate of 87.0% in the CUB-Birds bird dataset and 90.2% in the FGVC-Aircraft aircraft dataset, reaching the current state-of-the-art level.
ER  - 


TY  - JOUR
TI  - Semantic Pyramids for Gender and Action Recognition
T2  - IEEE Transactions on Image Processing
SP  - 3633
EP  - 3645
AU  - F. S. Khan
AU  - J. van de Weijer
AU  - R. M. Anwer
AU  - M. Felsberg
AU  - C. Gatta
PY  - 2014
KW  - Detectors
KW  - Face
KW  - Semantics
KW  - Image recognition
KW  - Feature extraction
KW  - Computer vision
KW  - Face recognition
KW  - Gender recognition
KW  - action recognition
KW  - pyramid representation
KW  - bag-of-words
DO  - 10.1109/TIP.2014.2331759
JO  - IEEE Transactions on Image Processing
IS  - 8
SN  - 1941-0042
VO  - 23
VL  - 23
JA  - IEEE Transactions on Image Processing
Y1  - Aug. 2014
AB  - Person description is a challenging problem in computer vision. We investigated two major aspects of person description: 1) gender and 2) action recognition in still images. Most state-of-the-art approaches for gender and action recognition rely on the description of a single body part, such as face or full-body. However, relying on a single body part is suboptimal due to significant variations in scale, viewpoint, and pose in real-world images. This paper proposes a semantic pyramid approach for pose normalization. Our approach is fully automatic and based on combining information from full-body, upper-body, and face regions for gender and action recognition in still images. The proposed approach does not require any annotations for upper-body and face of a person. Instead, we rely on pretrained state-of-the-art upper-body and face detectors to automatically extract semantic information of a person. Given multiple bounding boxes from each body part detector, we then propose a simple method to select the best candidate bounding box, which is used for feature extraction. Finally, the extracted features from the full-body, upper-body, and face regions are combined into a single representation for classification. To validate the proposed approach for gender recognition, experiments are performed on three large data sets namely: 1) human attribute; 2) head-shoulder; and 3) proxemics. For action recognition, we perform experiments on four data sets most used for benchmarking action recognition in still images: 1) Sports; 2) Willow; 3) PASCAL VOC 2010; and 4) Stanford-40. Our experiments clearly demonstrate that the proposed approach, despite its simplicity, outperforms state-of-the-art methods for gender and action recognition.
ER  - 


TY  - CONF
TI  - Towards a Remote Sensing System for Railroad Bridge Inspections: A Concrete Crack Detection Component
T2  - SoutheastCon 2018
SP  - 1
EP  - 4
AU  - L. Daniel Otero
AU  - M. Moyou
AU  - A. Peter
AU  - C. E. Otero
PY  - 2018
KW  - Bridges
KW  - Inspection
KW  - Image edge detection
KW  - Gray-scale
KW  - Concrete
KW  - Classification algorithms
KW  - Structural inspections
KW  - Bridge inspections
KW  - Concrete crack detection
KW  - Transportation systems engineering
DO  - 10.1109/SECON.2018.8478856
JO  - SoutheastCon 2018
IS  - 
SN  - 1558-058X
VO  - 
VL  - 
JA  - SoutheastCon 2018
Y1  - 19-22 April 2018
AB  - This research paper presents ongoing research work towards developing an effective remote sensing system for bridge inspections. The paper describes the development and evaluation of a five-step prototype image processing algorithm to detect concrete cracks. The algorithm, based on an unsupervised learning approach, extracts pixels that belong to concrete cracks from images. The algorithm was tested using various images collected from field tests. The images included different types of concrete cracks, along with various types of noise factors. The algorithm was successful in detecting the different types of concrete cracks from various images. Contrary to most studies from the literature, these images were not “clean”. That is, the images were collected from field tests and used as inputs to the detection algorithm without any preprocessing activity.
ER  - 


TY  - JOUR
TI  - Oversampling-Based Imbalanced Signal Modulation Classification via Cosine Distance and Distribution
T2  - IEEE Internet of Things Journal
SP  - 33657
EP  - 33670
AU  - J. Bai
AU  - H. Li
AU  - Y. Wang
AU  - Z. Xiao
AU  - H. Zhou
AU  - L. Jiao
PY  - 2024
KW  - Modulation
KW  - Classification algorithms
KW  - Feature extraction
KW  - Internet of Things
KW  - Accuracy
KW  - Security
KW  - Wireless communication
KW  - Data distribution
KW  - multiclass imbalanced signal
KW  - oversampling
KW  - signal modulation classification (SMC)
DO  - 10.1109/JIOT.2024.3432548
JO  - IEEE Internet of Things Journal
IS  - 20
SN  - 2327-4662
VO  - 11
VL  - 11
JA  - IEEE Internet of Things Journal
Y1  - 15 Oct.15, 2024
AB  - Advances in communication technology have enabled signal modulation classification (SMC) to be widely used in noncooperative identification situations, such as spectrum detection, electronic countermeasures, and target identification. In the face of complex electromagnetic environments and various classification tasks, the class imbalance phenomenon in modulated signal data sets has become a problem that cannot be ignored. For the SMC based on machine learning, the unbalanced training data set will cause the actual decision boundary to shift, thereby reducing the prediction accuracy of minority signals. And for SMC based on deep learning, unbalanced data will lead to distortion of the feature space and affect the extraction of discriminative features. However, the existing modulation classification methods cannot effectively deal with the imbalance problem. This study introduces an oversampling method tailored for modulation signals. Our method balances the data set by synthesizing new samples according to the distribution of signal samples and the distance between samples, which will effectively reduce the impact of the imbalance problem on the classifier. For modulated signals, experimental results show that our method performs better than other oversampling methods. In addition to the SMC task, we test the performance of the proposed method for individual identification of radiation sources on the aircraft communications addressing and reporting system data set. Compared with other comparison methods, our method improves the classification performance the most.
ER  - 


TY  - CONF
TI  - Features influence on targets classification performance using the high range resolution profiles (HRR profiles)
T2  - 2007 IET International Conference on Radar Systems
SP  - 1
EP  - 4
AU  - B. Atrouz
AU  - H. Aait Ouazzou
AU  - H. Kimouche
PY  - 2007
KW  - High Range Resolution Profile
KW  - NCTR
KW  - ISAR
KW  - 1 Nearest Neighbour
KW  - PCA
KW  - classification
DO  - 10.1049/cp:20070621
JO  - 2007 IET International Conference on Radar Systems
IS  - 
SN  - 0537-9989
VO  - 
VL  - 
JA  - 2007 IET International Conference on Radar Systems
Y1  - 15-18 Oct. 2007
AB  - In this paper, we propose the aircrafts supervised classification using the high range resolution profiles (HRR Profiles), extracted from ISAR images.
ER  - 


TY  - CONF
TI  - EEN:Edge Enhanced Network for Object Detection from SAR Images
T2  - 2024 17th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
SP  - 1
EP  - 6
AU  - H. Zhang
AU  - L. Dong
AU  - Y. Zhang
AU  - J. Zang
AU  - M. Han
AU  - C. Xiao
AU  - T. Xu
PY  - 2024
KW  - Image edge detection
KW  - Biological system modeling
KW  - Noise
KW  - Signal processing algorithms
KW  - Object detection
KW  - Speckle
KW  - Feature extraction
KW  - Radar polarimetry
KW  - Synthetic aperture radar
KW  - Context modeling
KW  - Object detection
KW  - Edge enhancement
KW  - Context fusion
KW  - SAR images
DO  - 10.1109/CISP-BMEI64163.2024.10906166
JO  - 2024 17th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 17th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
Y1  - 26-28 Oct. 2024
AB  - Object detection in Synthetic Aperture Radar(SAR) imagery is becoming increasingly important due to its ability to function effectively in conditions where visible remote sensing is limited by weather factors. Despite these advantages, SAR images are frequently marred by coherent speckle noise and distinct edge features, which pose considerable challenges for object detection algorithms. We present an Edge Enhanced Network (EEN) designed to enhance the performance of SAR image object detection algorithms. The proposed network comprises two integral components: the Edge Enhancement Module (EEM) and the Context Fusion Module (CFM). The EEM applies specialized edge convolution to elements within SAR images, effectively amplifying edge features while reducing the influence of coherent speckle noise. The CFM primarily integrates contextual features, thereby enhancing multi-scale feature representation and bolstering the model's generalization capability. To validate the effectiveness of the EEN, this study conducts experiments using mainstream object detection networks, including YOLOX and RtmDet. Comparative and ablation experiments are carried out on the SSDD, RSDD-SAR, and SAR-AIRcraft datasets. The experimental results confirm the effectiveness of the EEN.
ER  - 


TY  - JOUR
TI  - Audio Signal Feature Extraction and Classification Using Local Discriminant Bases
T2  - IEEE Transactions on Audio, Speech, and Language Processing
SP  - 1236
EP  - 1246
AU  - K. Umapathy
AU  - S. Krishnan
AU  - R. K. Rao
PY  - 2007
KW  - Feature extraction
KW  - Instruments
KW  - Automobiles
KW  - Humans
KW  - Aircraft
KW  - Helicopters
KW  - Animals
KW  - Birds
KW  - Insects
KW  - Image analysis
KW  - Audio classification
KW  - dissimilarity measures
KW  - feature extraction
KW  - linear discriminant analysis (LDA)
KW  - local discriminant bases (LDB)
KW  - wavelet packets
DO  - 10.1109/TASL.2006.885921
JO  - IEEE Transactions on Audio, Speech, and Language Processing
IS  - 4
SN  - 1558-7924
VO  - 15
VL  - 15
JA  - IEEE Transactions on Audio, Speech, and Language Processing
Y1  - May 2007
AB  - Audio feature extraction plays an important role in analyzing and characterizing audio content. Auditory scene analysis, content-based retrieval, indexing, and fingerprinting of audio are few of the applications that require efficient feature extraction. The key to extract strong features that characterize the complex nature of audio signals is to identify their discriminatory subspaces. In this paper, we propose an audio feature extraction and a multigroup classification scheme that focuses on identifying discriminatory time-frequency subspaces using the local discriminant bases (LDB) technique. Two dissimilarity measures were used in the process of selecting the LDB nodes and extracting features from them. The extracted features were then fed to a linear discriminant analysis-based classifier for a three-level hierarchical classification of audio signals into ten classes. In the first level, the audio signals were grouped into artificial and natural sounds. Each of the first level groups were subdivided to form the second level groups viz. instrumental, automobile, human, and nonhuman sounds. The third level was formed by subdividing the four groups of the second level into the final ten groups (drums, flute, piano, aircraft, helicopter, male, female, animals, birds and insects). A database of 213 audio signals were used in this study and an average classification accuracy of 83% for the first level (113 artificial and 100 natural sounds), 92% for the second level (73 instrumental and 40 automobile sounds; 40 human and 60 nonhuman sounds), and 89% for the third level (27 drums, 15 flute, and 31 piano sounds; 23 aircraft and 17 helicopter sounds; 20 male and 20 female speech; 20 animals, 20 birds and 20 insects sounds) were achieved. In addition to the above, a separate classification was also performed combining the LDB features with the mel-frequency cepstral coefficients. The average classification accuracies achieved using the combined features were 91% for the first level, 99% for the second level, and 95% for the third level
ER  - 


TY  - CONF
TI  - Aircraft Diagnostics Using Convolutional Neural Networks
T2  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
SP  - 36
EP  - 40
AU  - D. Mika
AU  - J. Józwik
AU  - A. Tofil
AU  - J. Pytka
AU  - P. Pioś
AU  - P. Tomiło
AU  - G. Skorulski
PY  - 2024
KW  - Training
KW  - Representation learning
KW  - Splicing
KW  - Neurons
KW  - Computer architecture
KW  - Metrology
KW  - Feature extraction
KW  - convolutional neural networks
KW  - fault diagnosis
KW  - blind signal identification
KW  - aircraft engineering
DO  - 10.1109/MetroAeroSpace61015.2024.10591556
JO  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
IS  - 
SN  - 2575-7490
VO  - 
VL  - 
JA  - 2024 11th International Workshop on Metrology for AeroSpace (MetroAeroSpace)
Y1  - 3-5 June 2024
AB  - In this paper we present the application of convolutional neural networks to identify faults in selected aircraft components. The theoretical part of the article includes the definition and discussion of machine learning issues. In the simulation part, the input data of the network in the form of a set of images of selected aircraft components were prepared and the architecture of the convolutional neural network was created. Experimental results confirming the high efficiency of aircraft damage identification using the proposed convolutional neural network architecture.
ER  - 


TY  - CONF
TI  - Fast Terrain Classification Using Variable-Length Representation for Autonomous Navigation
T2  - 2007 IEEE Conference on Computer Vision and Pattern Recognition
SP  - 1
EP  - 8
AU  - A. Angelova
AU  - L. Matthies
AU  - D. Helmick
AU  - P. Perona
PY  - 2007
KW  - Classification algorithms
KW  - Costs
KW  - Human robot interaction
KW  - Image sensors
KW  - Soil
KW  - Aircraft navigation
KW  - Computer science
KW  - Propulsion
KW  - Laboratories
KW  - Information retrieval
DO  - 10.1109/CVPR.2007.383024
JO  - 2007 IEEE Conference on Computer Vision and Pattern Recognition
IS  - 
SN  - 1063-6919
VO  - 
VL  - 
JA  - 2007 IEEE Conference on Computer Vision and Pattern Recognition
Y1  - 17-22 June 2007
AB  - We propose a method for learning using a set of feature representations which retrieve different amounts of information at different costs. The goal is to create a more efficient terrain classification algorithm which can be used in real-time, onboard an autonomous vehicle. Instead of building a monolithic classifier with uniformly complex representation for each class, the main idea here is to actively consider the labels or misclassification cost while constructing the classifier. For example, some terrain classes might be easily separable from the rest, so very simple representation will be sufficient to learn and detect these classes. This is taken advantage of during learning, so the algorithm automatically builds a variable-length visual representation which varies according to the complexity of the classification task. This enables fast recognition of different terrain types during testing. We also show how to select a set of feature representations so that the desired terrain classification task is accomplished with high accuracy and is at the same time efficient. The proposed approach achieves a good trade-off between recognition performance and speedup on data collected by an autonomous robot.
ER  - 


TY  - CONF
TI  - Weighted Focus-Attention Deep Network for Fine-grained Image Classification
T2  - 2019 IEEE International Conference on Big Data (Big Data)
SP  - 5116
EP  - 5125
AU  - C. Zou
AU  - R. Wang
AU  - X. Cao
AU  - F. Lv
PY  - 2019
KW  - Feature extraction
KW  - Visualization
KW  - Convolution
KW  - Training
KW  - Task analysis
KW  - Interference
KW  - Machine learning
KW  - recognition
KW  - fine-grained
KW  - deep learning
DO  - 10.1109/BigData47090.2019.9006580
JO  - 2019 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Big Data (Big Data)
Y1  - 9-12 Dec. 2019
AB  - Fine-Grained Visual Classification (FGVC) is a challenging task, due to the small variation of visual representations from different categories. An effective solution is utilizing the bounding boxes centering the object parts to extract the discriminative representations. However, regular rectangles contains the background when the shape of the part is irregular, which may interfere with the classification. In this paper, we propose a weighted focus-attention deep network (FA-Net) to address the problem of background interference in fine-grained classification. In our FA-Net, a focus-attention module is proposed to identify the foreground region from the class activation map and remove the background. Two branches are employed to obtain the primary and secondary attention regions with focus-attention module, and a weighted layer is utilized to integrate the attention regions. Experiment results on three challenging fine-grained classification datasets (e.g., CUB-200-2011, Stanford Dogs and FGVC Aircraft) show that our FA-Net obtains state-of-the-art results and outperforms the other fine-grained algorithms.
ER  - 


TY  - CONF
TI  - Aircraft remote sensing image target detection based on shallow feature enhancement and k-means clustering algorithm
T2  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
SP  - 237
EP  - 243
AU  - H. Wang
AU  - Z. Guo
PY  - 2021
KW  - Heuristic algorithms
KW  - Clustering algorithms
KW  - Object detection
KW  - Military aircraft
KW  - Feature extraction
KW  - Real-time systems
KW  - Classification algorithms
KW  - Target detection
KW  - remote sensing image
KW  - feature fusion
KW  - Soft- NMS
DO  - 10.1109/CISAI54367.2021.00052
JO  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)
Y1  - 17-19 Sept. 2021
AB  - Target detection is one of the basic tasks in the field of computer vision. The target detection of aircraft remote sensing images has important research value and application value. Aiming at the problem that the accuracy and real-time performance of current aircraft remote sensing image target detection algorithms can not balance, this paper presents a target detection algorithm aircraft remote sensing image based on the Single Shot MultiBox Detector(SSD). For the problem of poor accuracy of the SSD algorithm for aircraft remote sensing target detection, especially for small aircraft targets, the paper designs a module for shallow feature enhancement. In the end, the shallow network obtains feature information with rich structure and wide perception field, which improves the detection accuracy of the network for small remote sensing targets on low-level feature maps. In order to enable the neural network to learn more effective information, first use the k-means clustering algorithm clustering algorithm to obtain suitable anchor size for the aircraft target, then use the Non-maximum suppression(Soft-NMS) in the post-processing part of the SSD algorithm and focus classification loss function. Soft-NMS can reduce the missed detection of aircraft targets, and the focus classification loss can solve the problem of imbalance between positive and negative samples to a certain extent. Related experiments on the aircraft remote sensing dataset, average accuracy reaches 91.88% and frame per second is 39.3. The results show that the improved SSD algorithm can balance detection accuracy and real-time performance at the same time.
ER  - 


TY  - CONF
TI  - The Need for Multi-Source, Multi-Scale Hyperspectral Imaging to Boost Non-Invasive Mineral Exploration
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 7430
EP  - 7433
AU  - R. Gloaguen
AU  - P. Ghamisi
AU  - S. Lorenz
AU  - M. Kirsch
AU  - R. Zimmermann
AU  - R. Booysen
AU  - L. Andreani
AU  - R. Jackisch
AU  - E. Hermann
AU  - L. Tusa
AU  - G. Unger
AU  - C. Contreras
AU  - M. Khodadadzadeh
AU  - M. Fuchs
PY  - 2018
KW  - Hyperspectral imaging
KW  - Minerals
KW  - Metals
KW  - Earth
KW  - Raw materials
KW  - Mineral exploration
KW  - hyperspectral images
KW  - classification
KW  - ensemble based approaches
KW  - remote sensing
DO  - 10.1109/IGARSS.2018.8518742
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - The high demand for raw materials in our post-industrial societies contrasts the increasing difficulties to find new mineral deposits. In Europe, accessible and high-grade deposits are mostly exhausted or currently mined. Hence, future exploration must focus on the remaining, more remote locations or penetrate much deeper into the Earth's crust. Sustaining mining activities in Europe would allow the development of key technologies but also sustainable and ethical production of technological metals. Thus, we suggest to focus research on advances in multi-scale and multi-sensor remote sensing-based Earth integration techniques. The scale should range from satellite to air- and drone-borne systems and include ground validation. Multi-sensor downscaling methods involving SAR and optical data are particularly promising. We demonstrate that the integration with other sensors and/or measures such as geophysical/geochemical data as well as non-conventional remote sensing features such as textures and geometries are of interest. Thus, ultimately, our objective is to boost the competitiveness, growth, sustainability and attractiveness of the raw material sector in Europe. While we focus on the raw material sector as it is currently of strategic importance, the required methods are transferable to most environmental studies.
ER  - 


TY  - CONF
TI  - Hierarchical Deep Learning Framework for Enhanced UAV Classification Mitigating Bluetooth and WiFi Interference
T2  - 2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring)
SP  - 1
EP  - 6
AU  - C. Kumari
AU  - N. L. Prasad
AU  - U. Satija
AU  - B. Ramkumar
PY  - 2024
KW  - Deep learning
KW  - Bluetooth
KW  - Databases
KW  - Phantoms
KW  - Autonomous aerial vehicles
KW  - Steady-state
KW  - Convolutional neural networks
KW  - RF signatures
KW  - CNN
KW  - Deep learning
KW  - UAV classification
DO  - 10.1109/VTC2024-Spring62846.2024.10683639
JO  - 2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring)
IS  - 
SN  - 2577-2465
VO  - 
VL  - 
JA  - 2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring)
Y1  - 24-27 June 2024
AB  - In recent years, there has been a widespread use of unmanned aerial vehicles (UAVs) or drones on both commercial and defense applications. There is an increasing security concern as these UAVs can also be used for illegal activities by malicious users. It is necessary to detect and classify these malicious UAVs or drones in order to neutralize them. In this paper, a hierarchical scheme is proposed for detecting and classifying UAVs based on the RF signatures obtained from the RF link (both uplink and downlink) that is used to control the UAV from the ground station or UAV controller. The work also addresses the challenge of classifying UAV RF signatures in the presence of interferences like WiFi and Bluetooth as they also operate in the same frequency band. The method involves entropy-based steady-state extraction, segmentation, and spectrogram image conversion followed by a hierarchical classification based on different variants of convolutional neural network (CNN). The proposed method is validated on signals taken from the CardRF database and achieves an average detection accuracy of 99.7% and an average precision of 99.93%. As it is a hierarchical classification scheme, performance measures at each hierarchical stages are reported in the simulation results.
ER  - 


TY  - CONF
TI  - Deep and Machine Learning-based Methods for Defect Classification in Jet Engines
T2  - 2023 Intermountain Engineering, Technology and Computing (IETC)
SP  - 43
EP  - 48
AU  - M. P. Schoen
AU  - M. Oettinger
AU  - D. Mimic
PY  - 2023
KW  - Training
KW  - Sensitivity
KW  - Instruments
KW  - Cameras
KW  - Feature extraction
KW  - Data mining
KW  - Aircraft propulsion
KW  - Deep learning
KW  - machine learning
KW  - defect classification
KW  - jet engines
KW  - hot gas path
DO  - 10.1109/IETC57902.2023.10152188
JO  - 2023 Intermountain Engineering, Technology and Computing (IETC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Intermountain Engineering, Technology and Computing (IETC)
Y1  - 12-13 May 2023
AB  - In this paper, the utility and accuracy of Machine Learning (ML) and Deep Learning (DL) methods are investigated for detecting defects in civil aircraft engines. Rather than to disassemble jet engines, the approach investigated in this study utilizes images of the exhaust of jet engines and infers defects in the turbine and burner section. While the proposed DL methods make use of one or two cameras, the ML methods depend on data obtained by extracting the density fields of the Hot Gas Path (HGP). The HPG data are computed from images acquired by an array of cameras. The corresponding ML features are crafted from these density fields. The proposed algorithms employ optimized hyperparameters and separate training as well as validation data sets. The study illustrates the potential of DL methods and the resulting simplification in the necessary instrumentation to accomplish near perfect defect classification outcomes.
ER  - 


TY  - CONF
TI  - An unsupervised neural network classifier for automatic aerial image recognition
T2  - Proceedings of 19th Convention of Electrical and Electronics Engineers in Israel
SP  - 212
EP  - 215
AU  - S. Greenberg
AU  - H. Guterman
AU  - S. R. Rotman
PY  - 1996
KW  - Neural networks
KW  - Image recognition
KW  - Subspace constraints
KW  - Resonance
KW  - Target recognition
KW  - Fourier transforms
KW  - Shape
KW  - Application software
KW  - Target tracking
KW  - Aircraft
DO  - 10.1109/EEIS.1996.566932
JO  - Proceedings of 19th Convention of Electrical and Electronics Engineers in Israel
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of 19th Convention of Electrical and Electronics Engineers in Israel
Y1  - 5-6 Nov. 1996
AB  - This article describes the application of the adaptive resonance theory (ART 2-A) network to the problem of automatic aerial image recognition (AAIR). The classification of aerial images independently of their position and orientation is required for automatic tracking and target recognition. Invariance is achieved by using different invariant feature spaces in combination with an unsupervised neural network. The performance of the neural network based classifier in conjunction with several types of invariant AAIR global features, such as the Fourier transform (FT) space, Zernike moments, central moments and polar transforms, are examined. The advantages of this approach are discussed. The ART 2-A distinguished itself with its speed and low number of training vectors. Although a large image data base would be necessary before this approach could be fully validated, the initial results are very promising.
ER  - 


TY  - JOUR
TI  - Separation and Classification of Corona Discharges Under Low Pressures Based on Deep Learning Method
T2  - IEEE Transactions on Dielectrics and Electrical Insulation
SP  - 319
EP  - 326
AU  - M. Borghei
AU  - M. Ghassemi
PY  - 2022
KW  - Partial discharges
KW  - Discharges (electric)
KW  - Insulation
KW  - Corona
KW  - Deep learning
KW  - Aircraft
KW  - Fault location
KW  - Global warming
KW  - Deep learning
KW  - Corona discharge
KW  - deep learning (DL)
KW  - high-voltage systems
KW  - insulation systems
KW  - low-pressure conditions
KW  - machine learning
KW  - more electric aircraft
KW  - partial discharge (PD)
DO  - 10.1109/TDEI.2022.3146608
JO  - IEEE Transactions on Dielectrics and Electrical Insulation
IS  - 1
SN  - 1558-4135
VO  - 29
VL  - 29
JA  - IEEE Transactions on Dielectrics and Electrical Insulation
Y1  - Feb. 2022
AB  - With the growing concern toward the global warming crisis, the electrification of commercial aircraft is targeted to reduce greenhouse gas emissions from the aviation industry. However, the environment that an aircraft operates in provides significant design challenges. Moreover, the technologies that enhance the power density of the powertrain (such as higher voltage levels and wide bandgap devices) lead to severe tension on the insulation systems. The combination of harsh environmental conditions and insulation-threatening technologies raises concern about the reliability of electrical equipment, such as power generators, motors, and cables. Since the failure of the insulation system translates into the failure of the entire equipment, it is crucial to investigate the behavior of discharge sources under low-pressure conditions. In this regard, this study develops a dense convolutional neural network (DenseNet) model based on experimental data to separate and classify various sources of corona discharge under low-pressure conditions. The results show that DenseNet models can achieve high accuracy within a reasonable training time. The accurate detection and classification of discharge sources provide the backbone of a dielectric online condition monitoring system (DOCMS) that can actively monitor the health of electrical equipment in an electric aircraft.
ER  - 


TY  - CONF
TI  - Robust aircraft classification using moment invariants, neural network, and split inversion learning
T2  - IJCNN-91-Seattle International Joint Conference on Neural Networks
SP  - 941 vol.2
EP  - 
AU  - A. McAulay
AU  - A. Coker
AU  - K. Saruhan
PY  - 1991
KW  - Neural networks
KW  - Cameras
KW  - Noise robustness
KW  - Aircraft propulsion
KW  - Computer networks
KW  - Noise level
KW  - Computer science
KW  - Aerospace engineering
KW  - Computational modeling
KW  - Drives
DO  - 10.1109/IJCNN.1991.155541
JO  - IJCNN-91-Seattle International Joint Conference on Neural Networks
IS  - 
SN  - 
VO  - ii
VL  - ii
JA  - IJCNN-91-Seattle International Joint Conference on Neural Networks
Y1  - 8-12 July 1991
AB  - Summary form only given, as follows. A robust approach for classifying aircraft in the presence of noise was proposed and simulated. Preprocessing provides constant moment invariants for images in which the object is translated in position, rotated, or changed in scale. In the presence of noise, the moments are shown to be rotation-invariant. These moment invariants, computed for different levels of noise, are used to train a neural network to identify the aircraft. It is shown that training is very much faster for a split inversion algorithm than for back propagation. The resulting network provides accurate classification in high levels of noise.<>
ER  - 


TY  - JOUR
TI  - Fine-Grained Classification via Hierarchical Bilinear Pooling With Aggregated Slack Mask
T2  - IEEE Access
SP  - 117944
EP  - 117953
AU  - M. Tan
AU  - G. Wang
AU  - J. Zhou
AU  - Z. Peng
AU  - M. Zheng
PY  - 2019
KW  - Feature extraction
KW  - Visualization
KW  - Task analysis
KW  - Noise measurement
KW  - Data mining
KW  - Detectors
KW  - Image representation
KW  - Fine-grained classification
KW  - image mask
KW  - multi-scale
KW  - RoI feature
KW  - deep learning
DO  - 10.1109/ACCESS.2019.2936118
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 7
VL  - 7
JA  - IEEE Access
Y1  - 2019
AB  - Extracting discriminative fine-grained features is essential for fine-grained image recognition tasks. Many researchers utilize expensive human annotations to learn discriminative part models, which may be impossible for real-world applications. Recently, bilinear pooling has been frequently adopted and has shown its effectiveness owing to its learning discriminative regions automatically. However, most bilinear pooling models still utilize the all convolutional part/region features for recognition, including those noisy or even harmful feature elements. In this paper, we devise a novel fine-grained image classification approach by the Hierarchical Bilinear Pooling with Aggregated Slack Mask (HBPASM) model. The proposed model generates a RoI-aware image feature representation for better performance. We conduct experiments on three frequently used fine-grained image classification datasets. The experimental results demonstrate that HBPASM achieves competitive performance or even match the state-of-the-art methods on CUB-200-2011, Stanford Cars, and FGVC-Aircraft, respectively.
ER  - 


TY  - CONF
TI  - Robust ISAR image classification using Abridged Shape Matrices
T2  - 2016 International Conference on Emerging Trends in Engineering, Technology and Science (ICETETS)
SP  - 1
EP  - 6
AU  - H. K. Kondaveeti
AU  - V. K. Vatsavayi
PY  - 2016
KW  - Shape
KW  - Interpolation
KW  - Aircraft
KW  - Transmission line matrix methods
KW  - Quantization (signal)
KW  - Feature extraction
KW  - Gravity
DO  - 10.1109/ICETETS.2016.7603025
JO  - 2016 International Conference on Emerging Trends in Engineering, Technology and Science (ICETETS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Emerging Trends in Engineering, Technology and Science (ICETETS)
Y1  - 24-26 Feb. 2016
AB  - In this paper a new classification method is proposed to classify the ISAR images automatically based on Abridged Shape Matrices (ASMs). Initially, a considerate noise removal procedure is used to abate the noise present in the ISAR images and the target is segmented out from the background clutter. ASMs are obtained from the segmented targets by careful interpolation of the shape details of the target by the polar quantization. ASMs are concise and non redundant as the over sampling is avoided in interpolating the inner shape details of the target. The avoidance of superfluous interpolations in polar quantization reduces the generation time of the ASMs, which in turn curtails the over all execution time of the classifier. Experiments were conducted on synthesized ISAR aircraft image dataset and results are presented. The experimental results show that the proposed method is sturdy and more meticulous than existing methods.
ER  - 


TY  - JOUR
TI  - Projection pursuit classification of multiband polarimetric SAR land images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 2380
EP  - 2386
AU  - D. B. Trizna
AU  - C. Bachmann
AU  - M. Sletten
AU  - N. Allan
AU  - J. Toporkov
AU  - R. Harris
PY  - 2001
KW  - L-band
KW  - Spatial resolution
KW  - Optical polarization
KW  - Aircraft
KW  - Laboratories
KW  - Motion compensation
KW  - Transmitting antennas
KW  - Synthetic aperture radar
KW  - Layout
KW  - Radar remote sensing
DO  - 10.1109/36.964974
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 11
SN  - 1558-0644
VO  - 39
VL  - 39
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - Nov. 2001
AB  - Results are presented for an experiment utilizing a pastoral land scene with a variety of eight classes, imaged by the NRL dual band (X and L) polarimetric synthetic aperture radar (NUWSAR) at a spatial resolution of 1.2 m. Projection pursuit (PP) statistical analysis tools were applied to a set of simultaneous L-band and X-band fully polarized images (six independent channels) to demonstrate the utility of land classification at high spatial resolution from a light aircraft using SAR. The statistical confusion matrix was used as a quantitative optimization measure of classification. Samples of eight classes from a portion of the scene were used to define a training set, then PP tools were used for classification. It is clear that L-band and X-band fully polarized data view the classes in a significantly different manner, and each brings independent information to the analysis. These results are not meant to be exhaustive at this time but to demonstrate the utility of applying PP tools to multiband and polarization SAR data and to give an indication of the quality of classification one can achieve with moderately high spatial resolution SAR data using a light plane platform.
ER  - 


TY  - CONF
TI  - Automatic target recognition of aircrafts using translation invariant features and neural networks
T2  - 2008 9th International Conference on Signal Processing
SP  - 2271
EP  - 2274
AU  - Zun-hua Guo
AU  - Shao-hong Li
AU  - Wei-xin Xie
PY  - 2008
KW  - Target recognition
KW  - Neural networks
KW  - Feature extraction
KW  - Radar scattering
KW  - Aircraft propulsion
KW  - Airborne radar
KW  - Aerospace electronics
KW  - Radar imaging
KW  - Multi-layer neural network
KW  - Feedforward systems
KW  - automatic target recognition
KW  - feature extraction
KW  - neural networks
KW  - high range resolution profiles
DO  - 10.1109/ICOSP.2008.4697602
JO  - 2008 9th International Conference on Signal Processing
IS  - 
SN  - 2164-523X
VO  - 
VL  - 
JA  - 2008 9th International Conference on Signal Processing
Y1  - 26-29 Oct. 2008
AB  - Automatic target recognition (ATR) of aircrafts using translation invariant features derived from high range resolution (HRR) profiles and multilayered neural network is presented in this paper. The HRR profile sequences are translation variant in the range resolution cell because of the non-cooperative target maneuvering. The differential power spectrum (DPS) is introduced to extract the translation invariant features. Several learning algorithms of feed-forward neural network are implemented to determine an optimal choice in the recognition phase. The range profiles are obtained using the two-dimensional backscatters distribution data of four different scaled aircraft models. Simulations are presented to evaluate the classification performance with the DPS based features and neural networks. The results show that this method is effective for the application of radar target recognition.
ER  - 


TY  - CONF
TI  - Aircraft and High Altitude Platform System Onboard Circularly Polarized Synthetic Aperture Radar (CP-SAR)
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 8515
EP  - 8518
AU  - J. T. Sri Sumantyo
AU  - C. M. Yam
AU  - C. E. Santosa
AU  - A. Takahashi
AU  - K. Ito
PY  - 2021
KW  - Radio frequency
KW  - Polarization
KW  - C-band
KW  - Geoscience and remote sensing
KW  - Aircraft
KW  - Synthetic aperture radar
KW  - Monitoring
KW  - CP-SAR
KW  - Airborne
KW  - HAPS
KW  - Circular polarization
KW  - Hinotori-C2
KW  - CN235MPA
DO  - 10.1109/IGARSS47720.2021.9554497
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - Chiba University proposed and developed airborne and high altitude platform system (HAPS) onboard C band circularly polarized synthetic aperture radar (CP-SAR) for environment and disaster monitoring on altitude 2–20 km. This paper explains concept, system configuration, RF system and antenna, and flight test of CP-SAR in Hinotori-C2 (Firebird-C2) mission onboard CN235MPA aircraft on 14–15 March 2018 at Indonesia. The result of flight test depicts full polarimetric CP images that shows good performance of CP-SAR. The assessment of CP image analysis, and novel image classification using axial ratio (AR), ellipticity (ε), and polarization ratio (p) are discussed. We plan to hold the flight test of HAPS onboard CP-SAR in 2023.
ER  - 


TY  - CONF
TI  - Recognition Method of Airport Typical Motion Behavior Based on Infrared Image
T2  - 2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT
SP  - 897
EP  - 903
AU  - X. Wu
AU  - M. Ding
AU  - X. Wang
AU  - X. Li
PY  - 2020
KW  - Target tracking
KW  - Image recognition
KW  - Target recognition
KW  - Video sequences
KW  - Airports
KW  - Feature extraction
KW  - Long short term memory
KW  - Human behavior recognition
KW  - infrared image
KW  - target tracking
KW  - macroscopic features of motion
KW  - long short term memory network
KW  - airport surface
DO  - 10.1109/ICCASIT50869.2020.9368559
JO  - 2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT
Y1  - 14-16 Oct. 2020
AB  - Aiming at the problem of airport surface behavior recognition in low illuminance environment, based on infrared monitoring image data, the eco (efficient fluctuation operators for tracking) target tracking method is proposed to track different targets on airport surface including non-cooperative targets. The method of extracting macro motion features from tracking results is studied. On this basis, the long-term motion features are learned recursively in the framework of LSTM (long short term memory) network to realize the recognition of typical human actions on the airport surface. The experimental results on the self-made video data set show that the model can make full use of the long-term motion information in the video sequence. It is suitable for the airport scene in low illumination environment, and has a certain recognition effect.
ER  - 


TY  - CONF
TI  - Neural Network Based Landing Assist Using Remote Sensing Data
T2  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
SP  - 116
EP  - 120
AU  - D. Bharti
AU  - M. Kothari
AU  - K. S. Venkatesh
PY  - 2022
KW  - Visualization
KW  - Satellite broadcasting
KW  - Artificial neural networks
KW  - Sensor systems
KW  - Safety
KW  - Sensors
KW  - Pattern recognition
KW  - Autoland
KW  - Faster R-CNN
KW  - Instrument Landing System
KW  - Neural Networks
KW  - Runway Markings
DO  - 10.1109/SITIS57111.2022.00025
JO  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
Y1  - 19-21 Oct. 2022
AB  - This paper presents the application of computer vision and artificial neural networks for autonomous approach and landing and taxiing for an aircraft. In civil aviation and unmanned aircraft system industry, safety has always been the prime concern. We present a system which uses modern pattern recognition algorithm to aid in the landing of all types of aerial vehicles. The auto-land systems used today in aviation sector utilize a radio waves-based system known as Instrument Landing System (ILS) which has been in operation since decades. Although, it is efficient but might sometime be intermittent and is vulnerable to interference.Moreover, the auto-land system works in conjunction with different devices such as radio altimeter, ILS, Global Positioning System (GPS) and others. But, before reaching the Minimum Decision Altitude (MDA), pilots are expected to have the runway threshold marking, aiming point marking, displacement arrows and other touchdown markings/lights in-sight for landing. For this purpose, use of imaging sensors as an augmentation system for pilots during landing can improve the safety manifolds. Our method uses modern artificial neural networks to learn to recognize and localize important visual references during landing and taxiing useful for pilots by utilizing the satellite imagery dataset from Google Earth Engine (GEE) cloud computing.
ER  - 


TY  - JOUR
TI  - Fine-Grained Feature Enhancement for Object Detection in Remote Sensing Images
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 1
EP  - 5
AU  - Y. Zhou
AU  - S. Wang
AU  - J. Zhao
AU  - H. Zhu
AU  - R. Yao
PY  - 2022
KW  - Feature extraction
KW  - Remote sensing
KW  - Object detection
KW  - Task analysis
KW  - Semantics
KW  - Detectors
KW  - Transformers
KW  - Fine-grained classification
KW  - object detection
KW  - remote sensing
DO  - 10.1109/LGRS.2022.3163161
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 
SN  - 1558-0571
VO  - 19
VL  - 19
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - 2022
AB  - Recently, object detection in aerial images has ushered in a new challenge—a new benchmark for fine-grained object recognition in high-resolution remote sensing imagery called FAIR1M has been proposed. Fine-grained categories usually have smaller inter class differences and intra-class similarities, which is more difficult to classify with existing object detectors. To address this problem, we propose two enhanced strategies on the current two-stage object detection algorithm. The first strategy uses attention-based group feature enhancement called group enhance module (GEM). By extending and grouping feature channels, the model can improve the ability to extract various discriminative features. The second strategy is to emphasize the sub-saliency feature learning, avoiding the network only focusing on the most significant part of the feature and ignoring the other parts. Our method is easy to implement and effective, and experiments show that our method can improve the Oriented regions with convolutional neural networks features (R-CNN) by about 1.45 mAP on the FAIR1M benchmark.
ER  - 


TY  - CONF
TI  - Application of Fusion Technique and Support Vector Machine for Identifying Specific Vegetation Type
T2  - 2019 IEEE 5th International Conference for Convergence in Technology (I2CT)
SP  - 1
EP  - 5
AU  - K. V. Joshi
AU  - D. D. Shah
AU  - A. Deshpande
PY  - 2019
KW  - Support vector machines
KW  - Agriculture
KW  - Satellites
KW  - Remote sensing
KW  - Delays
KW  - Image color analysis
KW  - Earth
KW  - fusion
KW  - classification
KW  - Principal component analysis
KW  - brovey
KW  - support vector machine
DO  - 10.1109/I2CT45611.2019.9033884
JO  - 2019 IEEE 5th International Conference for Convergence in Technology (I2CT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE 5th International Conference for Convergence in Technology (I2CT)
Y1  - 29-31 March 2019
AB  - Agriculture is the nurturer of almost all living creatures on the earth. Various types of crop are being grown depending on the type of land. The overall yield plays a prime entity of concern. To acquire the knowledge about crops, various investigations had been carried out with the aid of image processing. The sources of farm images are quodcopters, aircrafts, satellite Numerous image classification techniques have proven its ability to bifurcate the image and achieve the target. In this research the satellite images are used to gain the precise knowledge of specific type of vegetation.. Support Vector Machine is used as classifier . the result has proven that the species are correctly identified from the arable land. Author has achieved accuracy of 90.6% with processing delay of 105.2 msec for 1600 blocks training in SVM.
ER  - 


TY  - CONF
TI  - Classifying common wetland plants using hyperspectral data to identify optimal spectral bands for species mapping using a small unmanned aerial systems — A case study
T2  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
SP  - 5924
EP  - 5927
AU  - S. Samiappan
AU  - G. Turnage
AU  - L. Hathcock
AU  - H. Yao
AU  - R. Kincaid
AU  - R. Moorhead
AU  - S. Ashby
PY  - 2017
KW  - Wetlands
KW  - Hyperspectral imaging
KW  - Cameras
KW  - Vegetation mapping
KW  - Payloads
KW  - Maximum likelihood estimation
KW  - small UAS
KW  - species classification
KW  - multispectral
KW  - hyperspectral
KW  - land cover classification
KW  - wetland monitoring
KW  - ecological monitoring
DO  - 10.1109/IGARSS.2017.8128357
JO  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
Y1  - 23-28 July 2017
AB  - Plant species monitoring in wetland ecosystems is crucial for preservation of water quality and many other ecological functions. Difficulties associated with conducting field work (i.e., hazardous terrain) can affect the ability of resource managers to correctly identify wetland plant species in a timely fashion. Thus, in wetland sites where access can be difficult, differentiation of plant species from aerial imagery can be of use. In this work, we studied laboratory produced hyperspectral data to classify various wetland plants thereby determining optimal multispectral bands that can distinguish these species. We then used an unmanned aerial systems (UAS) mountable 5-band multispectral sensor to classify 11 different wetland plant species to determine effectiveness of the multispectral bands chosen by the preliminary hyperspectral analysis. Classification experiments on the hyperspectral data produced the overall accuracies of 94-97% with 20% training samples. Experiments on the UAS collected multispectral data produced overall accuracy of 75% when considering four classes and 58% overall accuracy when considering 11 classes.
ER  - 


TY  - JOUR
TI  - Category-Aware Aircraft Landmark Detection
T2  - IEEE Signal Processing Letters
SP  - 61
EP  - 65
AU  - Y. Li
AU  - Y. Chang
AU  - Y. Ye
AU  - X. Zou
AU  - S. Zhong
AU  - L. Yan
PY  - 2021
KW  - Aircraft
KW  - Feature extraction
KW  - Streaming media
KW  - Aircraft manufacture
KW  - Visualization
KW  - Training
KW  - Image edge detection
KW  - Aircraft
KW  - landmark detection
KW  - category information
KW  - convolutional neural networks
DO  - 10.1109/LSP.2020.3045623
JO  - IEEE Signal Processing Letters
IS  - 
SN  - 1558-2361
VO  - 28
VL  - 28
JA  - IEEE Signal Processing Letters
Y1  - 2021
AB  - Aircraft landmark detection (ALD) aims at detecting the keypoints of aircraft, which can serve as an important role for subsequent applications such as fine-grained aircraft recognition. In ALD, the physical size discrepancy between different kinds of aircraft may lead to inconsistent landmark structure, which significantly harms landmark detection results. In this letter, we take advantage of the category prior to alleviate the size discrepancy in ALD. The proposed category-aware landmark detection network (CALDN) possesses two streams: a classification stream for size categorization and a localization stream for landmark detection. Instance-level size category information captured by classification stream serves as the guidance in the localization stream for robust landmark detection. Moreover, a category attention module (CAM) is proposed for better-utilizing category information to guide ALD. Benefitting from the adaptive attention mechanism, CAM can automatically highlight category-specific features for ulteriorly reducing the influence of size discrepancy. Furthermore, to advance ALD research, we contribute the first perspective-variant aircraft landmark dataset. Solid experiments demonstrate the superiority of our method.
ER  - 


TY  - CONF
TI  - Object classification from aerial visual imagery
T2  - SENSORS, 2010 IEEE
SP  - 1936
EP  - 1945
AU  - C. Ippolito
AU  - A. Nefian
PY  - 2010
KW  - Image edge detection
KW  - Vehicles
KW  - Diffusion tensor imaging
KW  - Classification algorithms
KW  - Histograms
KW  - Pipelines
KW  - Kernel
DO  - 10.1109/ICSENS.2010.5689985
JO  - SENSORS, 2010 IEEE
IS  - 
SN  - 1930-0395
VO  - 
VL  - 
JA  - SENSORS, 2010 IEEE
Y1  - 1-4 Nov. 2010
AB  - Aerial oil pipeline inspection is a dangerous endeavor in the current practice, where a pilot flying in a general aviation class aircraft flies slowly at low altitudes while concurrently looking at the ground for pipeline hazards with the unaided eye; high pilot workload in a dangerous low-speed, low-altitude environment results in an unacceptable number of accidents and loss of life each year. Automation of image acquisition and threat recognition has the potential to reduce pilot workload, improving the safety of the pilots and increasing efficiency. Towards these goals, this paper describes an image classification architecture and algorithm that utilizes several classifiers on different features extracted from the image to automate the threat detection process. The resulting classifier meets the requirement of greater than 80% accuracy in classification. The results will be discussed, and improvements will be proposed for continued research.
ER  - 


TY  - CONF
TI  - Novel side-view imaging of ships at sea for airborne ISAR
T2  - 2010 IEEE Radar Conference
SP  - 767
EP  - 772
AU  - L. Wang
AU  - X. Ye
AU  - D. Zhu
AU  - Z. Zhu
PY  - 2010
KW  - Marine vehicles
KW  - Radar scattering
KW  - Frequency estimation
KW  - Radar imaging
KW  - Geometry
KW  - Chirp
KW  - Military aircraft
KW  - Target recognition
KW  - Numerical simulation
KW  - Educational institutions
DO  - 10.1109/RADAR.2010.5494517
JO  - 2010 IEEE Radar Conference
IS  - 
SN  - 2375-5318
VO  - 
VL  - 
JA  - 2010 IEEE Radar Conference
Y1  - 10-14 May 2010
AB  - For airborne ISAR imaging of ship targets at sea, both ship's three-dimensional rotation driven by waves in the sea surface and the relative tangential translational motion between the platform and the ship contribute to the imaging. At high sea-state and in long-range and low-altitude observation, the rotation component produced by the relative tangential motion between the radar and the ship is negligible. A well-focused side-view image of a ship formed by utilizing the ship's dominant roll and pitch is always available via a suitable imaging time selection scheme. However, the effect of the rotation due to the platform tangential motion with respect to the ship, which provides much top-view information of the ship, has to be taken into account in some cases, such as when the ship is observed in a relatively close range. In these cases, the resulting image presents a mixture of the ship side-view and top-view that is less valuable than a pure side-view image for further classification. We present a ship side-view imaging method in this paper to solve this issue. The Doppler frequency derivative of each scatterer is shown to approximately only contain the height information and is estimated and utilized for the side-view image formation, with the use of 'CLEAN' technique and a concentration measure that is introduced to increase the estimation accuracy of the Doppler frequency derivative. For a short imaging interval, the Doppler frequency derivative is approximate to a constant chirp rate. The corresponding estimation reduces to a chirp rate estimation. The proposed method is verified by experiments on simulated data.
ER  - 


TY  - CONF
TI  - Object Detection Method Based on Aerial Image Instance Segmentation by Unmanned Aerial System in the Framework of Decision Making System
T2  - 2019 3rd International Conference on Advanced Information and Communications Technologies (AICT)
SP  - 332
EP  - 335
AU  - S. Kovbasiuk
AU  - L. Kanevskyy
AU  - M. Romanchuk
PY  - 2019
KW  - Object detection
KW  - Task analysis
KW  - Decision making
KW  - Neural networks
KW  - Computer architecture
KW  - Computational modeling
KW  - Automobiles
KW  - recognition
KW  - object detection
KW  - aerial images
KW  - instance segmentation
KW  - focal loss
KW  - unmanned aerial systems
DO  - 10.1109/AIACT.2019.8847875
JO  - 2019 3rd International Conference on Advanced Information and Communications Technologies (AICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 3rd International Conference on Advanced Information and Communications Technologies (AICT)
Y1  - 2-6 July 2019
AB  - The article analyses the capabilities of unmanned aerial system application in the framework of decision making in the crisis situations that require the object detection on aerial images acquired by the unmanned aerial system. To increase the operational capability and credibility of the automotive vehicles detection at the aerial images acquired by the unmanned aerial systems for more efficient use of acquired information in the framework of decision making support model Mask R-CNN was selected. This model is more appropriate for solving the problem of multiclass classification and object detection of small-size objects on the image. To improve this model, the article recommends using small-size anchors taking into account height-to-width aspect ratio according to greater amount of classes that along with test time augmentation usage enables to augment the mAP.
ER  - 


TY  - JOUR
TI  - Pattern representations and syntactic classification of radar measurements of commercial aircraft
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 204
EP  - 211
AU  - O. S. Sands
AU  - F. D. Garber
PY  - 1990
KW  - Radar measurements
KW  - Aircraft
KW  - Pattern recognition
KW  - Algorithm design and analysis
KW  - Signal processing
KW  - Inference algorithms
KW  - Signal design
KW  - Backscatter
KW  - Testing
KW  - Maximum likelihood estimation
DO  - 10.1109/34.44406
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 2
SN  - 1939-3539
VO  - 12
VL  - 12
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - Feb. 1990
AB  - A syntactic pattern recognition system is evaluated for applications to radar signal identification. Three different level-crossing-based pattern representation algorithms are considered. The utility of the resulting symbolic pattern representations is assessed by evaluating the performance of a maximum-likelihood classifier when the observed symbol strings are used as inputs to the decision algorithm. A syntax analysis algorithm is derived from the likelihood function classifier. Performance results of simulated classification experiments for both maximum-likelihood and language-theoretic classifiers are presented.<>
ER  - 


TY  - CONF
TI  - Supervised vs Unsupervised Approaches for Real Time Hyperspectral Imaging Maritime Target Detection
T2  - 2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO)
SP  - 1
EP  - 6
AU  - S. Freitas
AU  - H. Silva
AU  - J. Almeida
AU  - A. Martins
AU  - E. Silva
PY  - 2018
KW  - Support vector machines
KW  - Hyperspectral imaging
KW  - Kernel
KW  - Cameras
KW  - Boats
KW  - Training
DO  - 10.1109/OCEANSKOBE.2018.8559324
JO  - 2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO)
Y1  - 28-31 May 2018
AB  - This paper addresses the use of supervised and unsupervised methods for classification of hyperspectral imaging data in maritime border surveillance domain. In this work supervised (SVM) and unsupervised (HYDADE) approaches were implemented. An evaluation benchmark was performed in order to compare methods results using real hyperspectral imaging data taken from an Unmanned Aerial Vehicle in maritime border surveillance scenario.
ER  - 


TY  - JOUR
TI  - A hierarchical knowledge based system for airplane classification
T2  - IEEE Transactions on Software Engineering
SP  - 1829
EP  - 1834
AU  - D. I. Moldovan
AU  - C. . -I. Wu
PY  - 1988
KW  - Knowledge based systems
KW  - Airplanes
KW  - Military aircraft
KW  - Air traffic control
KW  - Shape
KW  - Computer vision
KW  - Software packages
KW  - Image classification
KW  - Control system synthesis
KW  - Image recognition
DO  - 10.1109/32.9066
JO  - IEEE Transactions on Software Engineering
IS  - 12
SN  - 1939-3520
VO  - 14
VL  - 14
JA  - IEEE Transactions on Software Engineering
Y1  - Dec. 1988
AB  - Airplane classification is used as an application domain to illustrate how hierarchical reasoning on large knowledge bases can be implemented. The knowledge base is organized as a two-dimensional hierarchy: one dimension corresponds to the levels of complexity often seen in computer vision, and the other dimension corresponds to the complexity of hypothesis used in the reasoning process. Reasoning proceeds top-down, from more abstract levels with fewer details toward levels with more details. Whenever possible, with the help of domain knowledge, decision is taken at a higher level, which significantly reduces processing time. A software package called RuBICS (Rule-Based Image Classification System) is described, and some examples of airplane classification are shown.<>
ER  - 


TY  - CONF
TI  - Requirement Study on a Specialized Hyperspectral Aerial Imaging System for Marine Plastic Litter Classification
T2  - OCEANS 2024 - Halifax
SP  - 1
EP  - 7
AU  - T. Schmid
AU  - J. Wellhausen
AU  - M. Kumm
AU  - T. Binkele
AU  - C. Tholen
AU  - O. Wurl
PY  - 2024
KW  - Data integrity
KW  - Wavelength measurement
KW  - Oceans
KW  - Noise
KW  - Sea measurements
KW  - Imaging
KW  - Plastics
KW  - System analysis and design
KW  - Monitoring
KW  - Hyperspectral imaging
KW  - remote sensing
KW  - hyperspectral imaging
KW  - maritime plastic litter
KW  - short wave infrared
KW  - SWIR
KW  - classification
KW  - PlasticObs+
DO  - 10.1109/OCEANS55160.2024.10754274
JO  - OCEANS 2024 - Halifax
IS  - 
SN  - 2996-1882
VO  - 
VL  - 
JA  - OCEANS 2024 - Halifax
Y1  - 23-26 Sept. 2024
AB  - This paper presents the predesign research for an affordable hyperspectral aerial measurement system for short wave infrared wavelengths. It is shown which design specifications need to be considered and a solution approach for aircraft related tradeoffs is presented, as well as an assessment for data quality and system design parameters for material classification for maritime plastic litter. It is presented that an affordable approach for broad aerial monitoring methods with hyperspectral technology can succeed from a technical point of view. To quantify required data quality, laboratory generated data are used, artificially impaired and analyzed in terms of acceptable noise impairment. Findings as to how high the noise impairment can be for expected classification results of the chosen plastic litter materials are given.
ER  - 


TY  - CONF
TI  - Monitoring of airplanes in infrared image
T2  - 2011 International Conference on Multimedia Technology
SP  - 3566
EP  - 3569
AU  - Wenjie Zhao
AU  - Hui Liu
PY  - 2011
KW  - Airplanes
KW  - Engines
KW  - Military aircraft
KW  - Image segmentation
KW  - Monitoring
KW  - Airports
KW  - early warning
KW  - image interpretation
KW  - minimum out-connected rectangle
DO  - 10.1109/ICMT.2011.6002205
JO  - 2011 International Conference on Multimedia Technology
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2011 International Conference on Multimedia Technology
Y1  - 26-28 July 2011
AB  - This paper proposes a novel method of monitoring airplanes in infrared image. First classification of airplanes is implemented. Recognition of aircraft conditions can be realized. Changes of airplanes including types, quantity and states can be obtained compared with the used data. Based on the result, a set of warning conditions are generated. It can be concluded that such an approach is effective for monitoring airfield dynamics.
ER  - 


TY  - CONF
TI  - Object classification and registration by Radon transform based invariants
T2  - ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing
SP  - 225
EP  - 228
AU  - H. Dohse
AU  - J. Sanz
AU  - A. Jain
PY  - 1987
KW  - Fourier transforms
KW  - Object recognition
KW  - Computer vision
KW  - Laboratories
KW  - Shape
KW  - Machine vision
KW  - Hardware
KW  - Image recognition
KW  - Oceans
KW  - Aircraft
DO  - 10.1109/ICASSP.1987.1169670
JO  - ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing
IS  - 
SN  - 
VO  - 12
VL  - 12
JA  - ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing
Y1  - 6-9 April 1987
AB  - An algorithm for object recognition which is based on translation and rotation invariant signatures is presented. The Radon transform is used as a tool for efficient implementation of the algorithm. Good classification results where obtained even with objects having similar shapes.
ER  - 


TY  - JOUR
TI  - Vehicle Detection in Aerial Surveillance Using Dynamic Bayesian Networks
T2  - IEEE Transactions on Image Processing
SP  - 2152
EP  - 2159
AU  - H. -Y. Cheng
AU  - C. -C. Weng
AU  - Y. -Y. Chen
PY  - 2012
KW  - Image color analysis
KW  - Vehicles
KW  - Feature extraction
KW  - Vehicle detection
KW  - Image edge detection
KW  - Training
KW  - Surveillance
KW  - Aerial surveillance
KW  - dynamic Bayesian networks (DBNs)
KW  - vehicle detection
DO  - 10.1109/TIP.2011.2172798
JO  - IEEE Transactions on Image Processing
IS  - 4
SN  - 1941-0042
VO  - 21
VL  - 21
JA  - IEEE Transactions on Image Processing
Y1  - April 2012
AB  - We present an automatic vehicle detection system for aerial surveillance in this paper. In this system, we escape from the stereotype and existing frameworks of vehicle detection in aerial surveillance, which are either region based or sliding window based. We design a pixelwise classification method for vehicle detection. The novelty lies in the fact that, in spite of performing pixelwise classification, relations among neighboring pixels in a region are preserved in the feature extraction process. We consider features including vehicle colors and local features. For vehicle color extraction, we utilize a color transform to separate vehicle colors and nonvehicle colors effectively. For edge detection, we apply moment preserving to adjust the thresholds of the Canny edge detector automatically, which increases the adaptability and the accuracy for detection in various aerial images. Afterward, a dynamic Bayesian network (DBN) is constructed for the classification purpose. We convert regional local features into quantitative observations that can be referenced when applying pixelwise classification via DBN. Experiments were conducted on a wide variety of aerial videos. The results demonstrate flexibility and good generalization abilities of the proposed method on a challenging data set with aerial surveillance images taken at different heights and under different camera angles.
ER  - 


TY  - CONF
TI  - Urban target classifications using time-frequency micro-Doppler signatures
T2  - 2007 9th International Symposium on Signal Processing and Its Applications
SP  - 1
EP  - 4
AU  - P. Setlur
AU  - M. Amin
AU  - F. Ahmad
PY  - 2007
KW  - Time frequency analysis
KW  - Doppler radar
KW  - Animation
KW  - Vibrations
KW  - Radar imaging
KW  - Electromagnetic scattering
KW  - Aircraft propulsion
KW  - Motion detection
KW  - Motion estimation
KW  - Parameter estimation
DO  - 10.1109/ISSPA.2007.4555595
JO  - 2007 9th International Symposium on Signal Processing and Its Applications
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2007 9th International Symposium on Signal Processing and Its Applications
Y1  - 12-15 Feb. 2007
AB  - Moving target indicators (MTI) find important applications in urban sensing. While motion detection can be achieved using simple CW radars, characterization of moving targets can be provided by estimating the key parameters of the target micro-Doppler signature. For indoor sensing, this signature has underlying instantaneous frequency features, which may depend on the radar viewing angle. This paper considers typical animate and inanimate moving objects and presents time-frequency motion classifiers using quadratic time-frequency distributions. The distinctions between the different target micro-Doppler parameter values and bounds are delineated.
ER  - 


TY  - JOUR
TI  - Texture and Scale in Object-Based Analysis of Subdecimeter Resolution Unmanned Aerial Vehicle (UAV) Imagery
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 761
EP  - 770
AU  - A. S. Laliberte
AU  - A. Rango
PY  - 2009
KW  - Unmanned aerial vehicles
KW  - Image analysis
KW  - Image texture analysis
KW  - Image resolution
KW  - Spatial resolution
KW  - Image segmentation
KW  - Decision trees
KW  - Monitoring
KW  - Digital cameras
KW  - Protocols
KW  - Object-based classification
KW  - rangelands
KW  - scale
KW  - texture
KW  - unmanned aircraft
DO  - 10.1109/TGRS.2008.2009355
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 3
SN  - 1558-0644
VO  - 47
VL  - 47
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - March 2009
AB  - Imagery acquired with unmanned aerial vehicles (UAVs) has great potential for incorporation into natural resource monitoring protocols due to their ability to be deployed quickly and repeatedly and to fly at low altitudes. While the imagery may have high spatial resolution, the spectral resolution is low when lightweight off-the-shelf digital cameras are used, and the inclusion of texture measures can potentially increase the classification accuracy. Texture measures have been used widely in pixel-based image analysis, but their use in an object-based environment has not been well documented. Our objectives were to determine the most suitable texture measures and the optimal image analysis scale for differentiating rangeland vegetation using UAV imagery segmented at multiple scales. A decision tree was used to determine the optimal texture features for each segmentation scale. Results indicated the following: (1) The error rate of the decision tree was lower; (2) prediction success was higher; (3) class separability was greater; and (4) overall accuracy was higher (high 90% range) at coarser segmentation scales. The inclusion of texture measures increased classification accuracies at nearly all segmentation scales, and entropy was the texture measure with the highest score in most decision trees. The results demonstrate that UAVs are viable platforms for rangeland monitoring and that the drawbacks of low-cost off-the-shelf digital cameras can be overcome by including texture measures and using object-based image analysis which is highly suitable for very high resolution imagery.
ER  - 


TY  - CONF
TI  - Classification of Oil Spill Thicknesses Using Multispectral UAS And Satellite Remote Sensing for Oil Spill Response
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
SP  - 5863
EP  - 5866
AU  - O. Garcia-Pineda
AU  - C. Hu
AU  - S. Sun
AU  - D. Garcia
AU  - J. Cho
AU  - G. Graettinger
AU  - L. DiPinto
AU  - E. Ramirez
PY  - 2019
KW  - UAS
KW  - multispectral
KW  - oil spill
KW  - oil thickness
KW  - oil emulsions
KW  - oil slicks
DO  - 10.1109/IGARSS.2019.8900170
JO  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 28 July-2 Aug. 2019
AB  - Unmanned Aerial Systems (UAS) are an operational tool for monitoring and assessment of oil spills. At the same time, satellite imagery has been used almost entirely to detect oil presence/absence, yet its ability to discriminate oil emulsions within a detected oil slick has not been fully exploited. Additionally, one of the challenges in the past has been the ability to deliver strategic information derived from satellite remote sensing in a timely fashion to responders in the field. This study presents UAS and satellite methods for the rapid classification of oil types and thicknesses, from which information about thick oil and oil emulsions (i.e., "actionable" oil) can be delivered in an operational timeframe to responders in the field. Experiments carried out at the OHMSETT test facility in New Jersey demonstrate that under specific viewing conditions satellites can record a signal variance between oil thicknesses and emulsions and non-emulsified oil. Furthermore, multispectral satellite data acquired by RADARSAT-2 and WorldView-2 were combined with data from a UAS field campaign to generate an oil/emulsion thickness classification based on a multispectral classification algorithm. Herein we present the classification methods to generate oil thickness products from UAS, validated by sea-truth observations, and quasi-synoptic multispectral satellite images acquired by WorldView-2. We tested the ability to deliver these products with minimum latency to responding vessels. During field operations in the Gulf of Mexico, we utilized the UAS multispectral system to identify areas of shoreline impacted by the oil spill. This proof-of-concept test using multispectral UAS data to detect emulsions and deliver a derived information product to a vessel in near-real-time sheds light on how UAS assets could be used in the near future for oil spill tactical response operations.
ER  - 


TY  - CONF
TI  - Improved S2ANet based on attention mechanism for small target detection in remote sensing images
T2  - 2021 CIE International Conference on Radar (Radar)
SP  - 942
EP  - 945
AU  - L. Dongdong
AU  - T. Wenjie
AU  - L. Songlin
AU  - Q. Xiaolan
PY  - 2021
KW  - Radar remote sensing
KW  - Radar detection
KW  - Object detection
KW  - Radar imaging
KW  - Optical imaging
KW  - Feature extraction
KW  - Adaptive optics
KW  - attention mechanism
KW  - CBAM
KW  - SAR
KW  - S2ANet
DO  - 10.1109/Radar53847.2021.10028564
JO  - 2021 CIE International Conference on Radar (Radar)
IS  - 
SN  - 2640-7736
VO  - 
VL  - 
JA  - 2021 CIE International Conference on Radar (Radar)
Y1  - 15-19 Dec. 2021
AB  - Deep learning has achieved remarkable results in the field of target detection and recognition. For small targets in images, image pyramid can be used to fuse multi-scale features to improve detection performance. However, when test on remote sensing images, it is found that some important small goals will still be missed. Aiming at the problems of small target detection in remote sensing images, this paper proposes two S2ANet improved network variants based on the attention mechanism for synthetic aperture radar(SAR) payload and optical payload: for SAR images, CBAM(Convolutional Block Attention Module) attention is added to the backbone network to increase the attention of the feature extraction network for small targets in the channel and space dimensions; For optical images, firstly increase the CBAM channel attention and spatial attention in the backbone, secondly, the SE(Squeeze And Excitation) channel attention is added to the input features before the channel dimension fusion of the pyramid structure, then the attention-enhanced targets are fully integrated through up-sampling and horizontal splicing operations to improve the missed alarm rate and recall rate of small remote sensing targets. Finally, a comparative verification was carried out on the constructed airborne KU-band SAR data set and the optical remote sensing aircraft data set composed of some DOTA1.5 and FAIR1M data sets. The experimental results proved that the designed two variant networks can be used for small remote sensing. There have been certain improvements in the missed alarm rate, accuracy rate, and recall rate of the target.
ER  - 


TY  - JOUR
TI  - A Mutual-Attention Guided Feature Extraction and Adaptative Decision Fusion Framework for Fine-Grained Dual-Band Radar Target Classification
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 7738
EP  - 7747
AU  - A. Zhong
AU  - S. Chen
AU  - T. Wang
AU  - Y. Ma
AU  - Y. Zhang
PY  - 2024
KW  - Radar
KW  - Feature extraction
KW  - Aircraft
KW  - Wideband
KW  - Narrowband
KW  - Convolutional neural networks
KW  - Airborne radar
KW  - Adaptive decision fusion (DF)
KW  - deep learning
KW  - dual-band
KW  - fine-grained classification
KW  - mutual-attention selection
DO  - 10.1109/JSTARS.2024.3375806
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 17
VL  - 17
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2024
AB  - Fine-grained radar target classification based on single-band, such as wideband or narrowband, poses challenges even when utilizing deep learning methods. Since different bands reflect distinct characteristics of the targets, we focus on the fine-grained classification of radar aircraft benefits from dual-band data. However, the selection of complementary dual-band features and the fusion of decisions from two bands are the key issues that need to be addressed. In order to tackle these issues, we propose a framework for fine-grained radar target classification called mutual-attention guided feature extraction and adaptative decision fusion. In this article, we propose a mutual attention selection mechanism to explore the complementary feature information between wideband and narrowband data. Furthermore, the wideband and narrowband features make distinct contributions to determine the class types. In order to address the uncertainty associated with the contribution of the wideband and narrowband data, we propose a new adaptive decision fusion strategy that adaptively assigns different weights to model the contribution uncertainty. We conducted extensive experiments on a homebrew simulated dual-band fine-grained aircraft dataset, which includes the high resolution range profile signal and the jet engine modulation signal. Compared with other classification methods, the proposed approach exhibits a remarkable classification accuracy of 95.5$\%$ in our homebrew dataset and maintains an impressive accuracy of 87.4$\%$ even in challenging environments with a 5 dB SNRs. Moreover, it achieves exceptional inference speeds of up to 3073 data pairs per second on the GPU: RTX3090. The results demonstrate the robustness and efficiency of the proposed method.
ER  - 


TY  - CONF
TI  - Using Near-Surface Observations for Optimizing the Timing of Overhead Image Acquisition for Applied Mapping of Woody Vegetation Species
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 5398
EP  - 5401
AU  - G. Weil
AU  - I. M. Lensky
AU  - Y. S. Resheff
AU  - N. Levin
PY  - 2018
KW  - Time series analysis
KW  - Vegetation mapping
KW  - Feature extraction
KW  - Cameras
KW  - Timing
KW  - Remote sensing
KW  - Sensors
KW  - vegetation species classification
KW  - near-surface observations
KW  - feature selection
KW  - unmanned aircraft vehicles
KW  - Mediterranean vegetation
DO  - 10.1109/IGARSS.2018.8517691
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - We present a phenology-based approach for optimizing the number and timing of unmanned aerial vehicle imagery acquisition, based on a priori near-surface observations. A ground-placed camera was used for generating annual time series of spectral indices in four different East Mediterranean sites. The time series dataset represented 1852 individuals of 12 common vegetation species. Feature selection was used for identifying the optimal dates for species classification. A UAV was flown for acquiring five overhead multiband orthomosaics, based on the five optimal dates identified in the feature selection of the near-surface time series of the previous year. An object-based classification was used for species classification, and resulted in an average overall accuracy of 85% and an average Kappa coefficient of 0.82. This cost-effective approach has high potential for detailed vegetation mapping, regarding the accessibility of UAV-produced time series, compared to hyper-spectral imagery with high spatial resolution which is more expensive and involves great difficulties in implementation over large areas.
ER  - 


TY  - CONF
TI  - Identification of the Marine Coast Area Affected by Oil Spill Using Multispectral Satellite and UAV Images in Ventanilla - Callao, Perú
T2  - 2024 IEEE Biennial Congress of Argentina (ARGENCON)
SP  - 1
EP  - 7
AU  - J. C. E. Llenque
AU  - M. M. Valiente
AU  - J. C. C. Fababa
AU  - L. E. Quiroz
AU  - M. C. Del Castillo
AU  - J. J. P. Gonzales
AU  - M. R. Sanchez
AU  - G. M. Lynes
AU  - J. M. Q. Ortiz
PY  - 2024
KW  - Accuracy
KW  - Satellites
KW  - Oils
KW  - Sea measurements
KW  - Organizations
KW  - Hydrocarbons
KW  - Satellite images
KW  - Object recognition
KW  - Surface treatment
KW  - Remotely piloted aircraft
KW  - Oil Spill
KW  - PeruSAT-1
KW  - RPAS
KW  - Reflectance
KW  - OBIA
DO  - 10.1109/ARGENCON62399.2024.10735911
JO  - 2024 IEEE Biennial Congress of Argentina (ARGENCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE Biennial Congress of Argentina (ARGENCON)
Y1  - 18-20 Sept. 2024
AB  - The primary objective of this research was to identify the sea and coastal areas affected by the oil spill that occurred on January 15, 2022, near the La Pampilla refinery in Ventanilla, Callao, Peru. High-resolution multispectral satellite images from the Peruvian Satellite System (PSS) and satellite images from the International Charter activation were utilized. Additionally, aerial images were collected using remotely piloted aircraft systems (RPAS) over the coastal zone from Ventanilla beach in Callao to Punta Salinas in Huacho, Huara-Lima. The satellite images were processed at the surface reflectance level, and a classification technique based on object detection was applied to enhance image interpretation by analyzing shapes, sizes, textures, and other features. This method improved the identification of the affected marine areas. For the aerial images, photointerpretation was employed to determine the extent of the area impacted by the oil spill in the coastal zones. The results from the multispectral images revealed estimated affected areas of 10669.90 ha, 7049.19 ha, 1732.10 ha, 502.03 ha, and 972.78 ha on January 18, 19, 25, 27, and February 4, 2022, respectively. For the RPAS images, an estimated littoral area of 390.41 ha was affected by the oil spill from Ventanilla-Callao beach to Cascajo-Chancay beach in Huaral-Lima on January 17 and 26, 2022. The results were validated using data collected during field campaigns conducted by the Environmental Evaluation and Supervision Organization (OEFA), achieving an overall accuracy of 97.98% and a kappa index of 0.56. The information obtained from this study has contributed to the environmental evaluation and monitoring processes carried out by the Ministry of Environment (MINAM) and the supervisory organization (OEFA).
ER  - 


TY  - CONF
TI  - A case study in using human similarity measure for automated object recognition
T2  - Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)
SP  - 555
EP  - 559 vol.1
AU  - B. Kamgar-Parsi
AU  - B. Kamgar-Parsi
AU  - A. K. Jain
PY  - 1999
KW  - Computer aided software engineering
KW  - Humans
KW  - Anthropometry
KW  - Object recognition
KW  - Testing
KW  - Military aircraft
KW  - Databases
KW  - Laboratories
KW  - Image recognition
KW  - Psychology
DO  - 10.1109/ICIP.1999.821690
JO  - Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)
Y1  - 24-28 Oct. 1999
AB  - Image understanding often involves object recognition, where a basic question is how to decide whether a match is correct. Typically the best match (among a set of prestored objects) is assumed to be the correct match. This may work well in controlled environments (closed world). But, in uncontrolled environments (open world), the test object may not belong to the prestored object classes. In uncontrolled environments, a metric similarity measure (e.g. Euclidean) in conjunction with a threshold is used. However, based on psychophysical studies this is very different from, and far inferior to, human capabilities. To accept or reject a match, we introduce an approach that avoids metric similarity measures and the use of thresholds as it attempts to employ similarity measures used by humans. In the absence of sufficient real data, the approach allows to specifically generate an arbitrarily large number of training exemplars projecting near classification boundary. For aircraft detection, the performance of a neural network trained on such a training set, was comparable to that of a human expert, and far better than a network trained only on the available real data. Furthermore, the results were considerably better than those obtained using a Euclidean discriminator.
ER  - 


TY  - JOUR
TI  - Classification of Partial 2-D Shapes Using Fourier Descriptors
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 686
EP  - 690
AU  - C. C. Lin
AU  - R. Chellappa
PY  - 1987
KW  - Shape
KW  - Aircraft
KW  - Parameter estimation
KW  - Least squares approximation
KW  - Pattern analysis
KW  - Pattern recognition
KW  - Biological cells
KW  - Image analysis
KW  - X-ray detection
KW  - X-ray detectors
KW  - Fourier descriptors
KW  - partial shape classification
KW  - shape recognition
DO  - 10.1109/TPAMI.1987.4767963
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 5
SN  - 1939-3539
VO  - PAMI-9
VL  - PAMI-9
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - Sept. 1987
AB  - We present a method for the classification of 2-D partial shapes using Fourier descriptors. We formulate the problem as one of estimating the Fourier descriptors of the unknown complete shape from the observations derived from an arbitrarily rotated and scaled shape with missing segments. The method used for obtaining the estimates of the Fourier descriptors minimizes a sum of two terms; the first term of which is a least square fit to the given data subject to the condition that the number of missing boundary points is not known and the second term is the perimeter2/area of the unknown shape. Experiments with synthetic and real boundaries show that estimates closer to the true values of Fourier descriptors of complete boundaries are obtained. Also, classification experiments performed using real boundaries indicate that reasonable classification accuracies are obtained even when 20-30 percent of the data is missing.
ER  - 


TY  - CONF
TI  - An Efftcient Content Based Remote Sensing Image Retrieval Using Artiftcial Neural Network
T2  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
SP  - 610
EP  - 614
AU  - L. Aswathi
AU  - K. Anoop
PY  - 2020
KW  - Conferences
KW  - ANN
KW  - remote sensing
KW  - GLCM Feature
DO  - 10.1109/ICOSEC49089.2020.9215284
JO  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Smart Electronics and Communication (ICOSEC)
Y1  - 10-12 Sept. 2020
AB  - Remote sensing is being used in different fields like agriculture, research etc.. Remote sensed images contains complex visual contents. This paper explains about the content based remote sensing image retrieval using ANN. In remote sensing method the sensors which will be fixed on an aircraft or satellite is used for capturing remote sensing images. Due to the increase in the use of remote sensing technology and also the number of satellites used, the volume of image dataset is increasing exponentially. Content Based Remote sensing Image Retrieval is used as to reduce the difficult in managing large volume of earth data.
ER  - 


TY  - JOUR
TI  - Spatial Coordinates Correction Based on Multi-Sensor Low-Altitude Remote Sensing Image Registration for Monitoring Forest Dynamics
T2  - IEEE Access
SP  - 18483
EP  - 18496
AU  - R. Yu
AU  - M. Lyu
AU  - J. Lu
AU  - Y. Yang
AU  - G. Shen
AU  - F. Li
PY  - 2020
KW  - Hyperspectral imaging
KW  - Forestry
KW  - Feature extraction
KW  - Monitoring
KW  - Image registration
KW  - Forest dynamics
KW  - small unmanned aerial vehicle
KW  - multi-sensor image registration
KW  - spatial coordinates correction
KW  - hyperspectral images
DO  - 10.1109/ACCESS.2020.2968335
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - Tree species diversity plays a significant role in our ecosystem. In order to monitor forest dynamics, hyperspectral remote sensing equipped on a small unmanned aerial vehicle (UAV) is commonly applied, such as individual tree detection and classification. However, low resolution, positioning errors and the imaging perspective of small UAV affected by wind speed/direction, complex terrain, battery capacity, aircraft posture, flying height and other human factors result in relatively large positional errors (i.e., GPS errors) in such hyperspectral images, and the precise forest dynamics monitoring is limited, especially in spatial analysis. In order to reduce the positional errors of hyperspectral images captured from a small UAV and provide a precise forest dynamics monitoring, we present a novel spatial coordinates correction approach by registering low-altitude UAV visible light and hyperspectral images. The proposed method first employs visible light images and ground control points to stitch a geographic coordinate system as our groundtruth. Hyperspectral images (UHI) are then registered onto the stitched visible light image (UVI) via a novel image registration method. Finally, spatial coordinates of the registered hyperspectral images are updated by using the aforementioned groundtruth. Extensive experiments on image registration and spatial coordinates correction demonstrate the favorable performance of our method. Compared against four state-of-the-art registration methods, our method shows the best registration performance, and the positional errors of hyperspectral images are significantly reduced. Such accuracy is considered very high in this research.
ER  - 


TY  - CONF
TI  - Research Progresses and Trends of Power Line Extraction based on Machine Learning
T2  - 2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)
SP  - 211
EP  - 215
AU  - K. Zou
AU  - Z. Jiang
AU  - Q. Zhang
PY  - 2021
KW  - Deep learning
KW  - Performance evaluation
KW  - Fault diagnosis
KW  - Image processing
KW  - Training data
KW  - Inspection
KW  - Benchmark testing
KW  - Power Line Extraction (PLE)
KW  - Machine Learning
KW  - Deep Learning
KW  - Image Processing
KW  - Aerial Image
DO  - 10.1109/ISCEIC53685.2021.00051
JO  - 2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)
Y1  - 6-8 Aug. 2021
AB  - Power Line Extraction (PLE) is useful for low-altitude aircraft avoiding the high-voltage power line, and it also can be used in the power line autonomous inspection. PLE based on aerial images has caused many researchers to study with enthusiasm, because machine learning methods play an important role in PLE. The PLE methods based on machine learning are summarized in this paper, and then the research progresses of PLE methods based on traditional image processing, machine learning and deep learning are analyzed; then the future research trends of PLE are predicted based on the survey of novel methods proposed within the pasted two years. The PLE belongs to the interdisciplinary research direction, and it has certain reference value for researchers with research fields such as power fault diagnosis, image processing, and machine learning.
ER  - 


TY  - CONF
TI  - Multilevel Fast Multipole Acceleration for Fast ISAR Imaging based on Compressive Sensing
T2  - 2018 International Conference on Electrical Sciences and Technologies in Maghreb (CISTEM)
SP  - 1
EP  - 5
AU  - A. E. Mahdaoui
AU  - A. Ouahabi
AU  - M. S. Moulay
PY  - 2018
KW  - Imaging
KW  - Compressed sensing
KW  - Radar imaging
KW  - Scattering
KW  - Integral equations
KW  - Image reconstruction
KW  - multilevel fast multipole acceleration (MLFMA)
KW  - compressive sensing (CS)
KW  - electromagnetic (EM)
KW  - inversed synthetic aperture radar (ISAR)
KW  - fast fourier transform (FFT)
DO  - 10.1109/CISTEM.2018.8613411
JO  - 2018 International Conference on Electrical Sciences and Technologies in Maghreb (CISTEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 International Conference on Electrical Sciences and Technologies in Maghreb (CISTEM)
Y1  - 28-31 Oct. 2018
AB  - In this paper, compressive sensing (CS) is introduced into an efficient numerical method multilevel fast multipole acceleration (MLFMA) for the electromagnetic scattering problem over a wide incident angle. The resulted data from CS based MLFMA are processed to inverse synthetic aperture radar (ISAR) imaging. Simulation results in maritime surveillance for the classification of ships and aircrafts show the received data for ISAR imaging from MLFMA with CS can reach the outperforms of MLFMA and achieves a quality similar to that of classical ISAR imaging. Furthermore, the computational complexity is improved by CS obviously through the reduced matrix computation for the fewer incident waves. An illustration with Aircraft Model Imaging confirms the usefulness of CSMLFMA.
ER  - 


TY  - CONF
TI  - Measures to compare the shape of objects in remote sensing images
T2  - 2020 International Conference on Information Technology and Nanotechnology (ITNT)
SP  - 1
EP  - 5
AU  - L. M. Mestetskiy
AU  - A. B. Semenov
PY  - 2020
KW  - Image segmentation
KW  - Shape
KW  - Shape measurement
KW  - Aircraft
KW  - Task analysis
KW  - Remote sensing
KW  - Nanotechnology
KW  - continuous morphology
KW  - shape comparison
KW  - contour analysis
KW  - skeleton graph
KW  - radial function
DO  - 10.1109/ITNT49337.2020.9253341
JO  - 2020 International Conference on Information Technology and Nanotechnology (ITNT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Information Technology and Nanotechnology (ITNT)
Y1  - 26-29 May 2020
AB  - We consider the urgent task of recognizing and classifying objects in remote sensing images by comparing their shapes with reference images. Measures of similarity of the image shapes based on a comparison of the outlines and medial representations of the object silhouettes are proposed. The solution to the problem is considered on the example of classification of images of aircraft. The study of the proposed approach was performed for images of aircraft of more than 40 types obtained as a result of segmentation of real remote sensing images. Computational experiments demonstrate a rather high level of recognition quality (Top1-71%, Top3-90%, Top5-95%), which confirms the possibility of practical use of the proposed solution.
ER  - 


TY  - CONF
TI  - A Novel Statistical-Based Scale-Independent Approach to Unsupervised Water Segmentation of SAR Images
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1057
EP  - 1060
AU  - F. Asaro
PY  - 2019
KW  - Speckle
KW  - Image segmentation
KW  - Rivers
KW  - Water resources
KW  - Monitoring
KW  - Radar polarimetry
KW  - Probability density function
KW  - SAR
KW  - water mapping
KW  - segmentation
KW  - Sentinel-1
DO  - 10.1109/IGARSS.2019.8899055
JO  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 28 July-2 Aug. 2019
AB  - In this work, a novel approach to water segmentation of Synthetic Aperture Radar images is presented. The proposed methodology, suitable for any VV-VH dual-pol data source, it is based only on the statistical and morphological content of the two polarimetric channels. No external information or supervision is required to successfully complete the segmentation task, allowing a complete automation of the process. The methodology has been applied to Sentinel-1 Interferometric Wide-Swath data and validated using very high-resolution optical images acquired by an Unmanned Aerial System over two Italian rivers, during different seasons. The performances of the algorithm have been estimated carrying out a pixel-based validation, deriving from the binary confusion matrix two classical figures of merit such as Global accuracy and F1-score. In the validation procedure, it has been assessed an overall Global Accuracy of 0.92 and an overall F1-score of 0.81, suggesting that the presented methodology applied to S-1 data is well-suited for the monitoring of rivers characterized by a wet channel width greater than 60 m.
ER  - 


TY  - CONF
TI  - A multi-sensor approach to airport surface traffic tracking
T2  - [1993 Proceedings] AIAA/IEEE Digital Avionics Systems Conference
SP  - 430
EP  - 432
AU  - D. Stauffer
AU  - H. French
AU  - J. Lenz
AU  - G. Rouse
PY  - 1993
KW  - Airports
KW  - Sensor systems
KW  - Radar tracking
KW  - Magnetic sensors
KW  - Air traffic control
KW  - Infrared sensors
KW  - Land vehicles
KW  - Military aircraft
KW  - FAA
KW  - Costs
DO  - 10.1109/DASC.1993.283512
JO  - [1993 Proceedings] AIAA/IEEE Digital Avionics Systems Conference
IS  - 
SN  - 
VO  - 
VL  - 
JA  - [1993 Proceedings] AIAA/IEEE Digital Avionics Systems Conference
Y1  - 25-28 Oct. 1993
AB  - A dual sensor system using a large number of inexpensive short range sensors can act as a valuable adjunct to surface surveillance radars, as well as provide a low cost tracking system for non-radar equipped airports. A measurement program was conducted to collect data on magnetic and infrared signatures of aircraft and other vehicles on airport surfaces. The data indicates that short range sensors can provide detection and tracking of vehicles, and may be able to perform some classification function.<>
ER  - 


TY  - CONF
TI  - Insulator identification from aerial images using Support Vector Machine with background suppression
T2  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 892
EP  - 897
AU  - X. Wang
AU  - Y. Zhang
PY  - 2016
KW  - Insulators
KW  - Support vector machines
KW  - Feature extraction
KW  - Image recognition
KW  - Inspection
KW  - Training
KW  - Videos
DO  - 10.1109/ICUAS.2016.7502544
JO  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 7-10 June 2016
AB  - Insulator identification in aerial videos is one of the key procedures to the condition analysis for aerial power line inspections. This paper proposes a novel insulator recognition method for images taken by Unmanned Aerial Vehicles (UAVs) with highly cluttered background, which is to adopt a machine learning algorithm Support Vector Machine (SVM) as a classifier to distinguish insulator from the cluttered background based on Gabor features. An innovative background suppression method is proposed to remove the redundant background information as much as possible. The test results show that not only the proposed method can successfully recognize insulator in the aerial images with complex and cluttered background, but also the background suppression procedure can greatly drop the computational load and reduce faulty classification.
ER  - 


TY  - CONF
TI  - Image Processing and Artificial Intelligence for Precision Agriculture
T2  - 2022 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)
SP  - 1
EP  - 8
AU  - S. G
AU  - K. Rajamohan
PY  - 2022
KW  - Productivity
KW  - Deep learning
KW  - Wireless sensor networks
KW  - Satellites
KW  - Image processing
KW  - Wind speed
KW  - Crops
KW  - Image processing
KW  - Precision agriculture
KW  - neural network
KW  - Satellite imagery
KW  - UAV images
DO  - 10.1109/ICSES55317.2022.9914148
JO  - 2022 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)
Y1  - 15-16 July 2022
AB  - Precision agriculture is a novel approach to increase the productivity of crops that employs recent technologies such as Artificial Intelligence, WSN, cloud computing, Machine Learning, and IoT. This paper reviews the development of different techniques effectively used in precision agriculture. The paper details the technological impact on precision agriculture followed by the different image processing schemes such as Satellite imagery and unmanned aerial vehicle (UAV). The role of precision agriculture is disease detection, weed detection from UAV images, and detection of trees and contaminated soils from satellite imagery is discussed. It reviews the impact of artificial intelligence (AI) namely machine learning & deep learning in precision agriculture. The performance of the recent image processing schemes in precision agriculture is analyzed. The paper also discusses the challenges that exist in implementing the precision agriculture system.
ER  - 


TY  - CONF
TI  - Automatic visual inspection system for stamped sheet metals (AVIS3M)
T2  - 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)
SP  - 661
EP  - 665
AU  - A. R. Rababaah
AU  - Y. Demi-Ejegi
PY  - 2012
KW  - Inspection
KW  - Visualization
KW  - Metals
KW  - Surface treatment
KW  - Humans
KW  - Image segmentation
KW  - Classification algorithms
KW  - automatio
KW  - machine vision
KW  - stamped sheet metal
KW  - visual inspection
DO  - 10.1109/CSAE.2012.6272855
JO  - 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)
Y1  - 25-27 May 2012
AB  - This paper presents a development of an automatic system for visual inspection of stamped sheet metals. Visual inspection operations in manufacturing processes can be challenging for humans. Human operators have several disadvantages compared to intelligent machines including; subjectivity, productivity, consistency, repeatability, etc. In this study we present an investigation of an automatic visual inspection system to improve some of these deficiencies. Our approach proposes an integrated software system based on machine vision theory that includes: image preprocessing, image segmentation, connected component analysis, region analysis, feature extraction, defect detection, quantification and classification and decision making. Our proposed demonstrated superior efficiency, reliability and stability compared to human subjects.
ER  - 


TY  - CONF
TI  - Defect detection of bottled liquor based on deep learning
T2  - CSAA/IET International Conference on Aircraft Utility Systems (AUS 2020)
SP  - 1259
EP  - 1264
AU  - X. Zhang
AU  - L. Yan
AU  - H. Yan
PY  - 2020
DO  - 10.1049/icp.2021.0415
JO  - CSAA/IET International Conference on Aircraft Utility Systems (AUS 2020)
IS  - 
SN  - 
VO  - 2020
VL  - 2020
JA  - CSAA/IET International Conference on Aircraft Utility Systems (AUS 2020)
Y1  - 18-21 Sept. 2020
AB  - In the production process of bottled liquor, due to the influence of raw material quality, processing technology and other factors, there may be various types of defects in the product that affect the product quality. Due to the variety of defects and the small size of some defects (such as liquor defects), it is difficult to detect, and manufacturers often need to invest a lot of labor costs for product quality inspection. In response to the above problems, we adopt two methods. First of all, for the dynamic liquor, we adopt a more unique image processing method, which can not only retain the original information and location information of the picture, but also keep the difference information as a guide. Secondly ,our propose a defect detection algorithm for bottled liquor based on deep learning, which contains many structures that can improve the defect detection performance, it has many advantages: 1) ROI Align replaces ROI Pooling, eliminating quantization error, 2) FPN can greatly improve the detection performance of small objects without increasing the calculation of the original structure, 3) The cascade algorithm makes the output distribution of each stage of detectors conducive to training a higher quality detector in the next stage with a higher IoU threshold, 4) Deformable convolution network(DCN) better fits the target of bottled liquor defects, meanwhile, some other techniques are also used to improve the accuracy. Experiments show that the above method can greatly improve the accuracy, and we also test the time requirement to ensure that the accuracy of the model decreases slightly while the model has a faster detection speed.
ER  - 


TY  - CONF
TI  - A real-time supervised learning approach for sky segmentation onboard unmanned aerial vehicles
T2  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 8
EP  - 14
AU  - A. Carrio
AU  - C. Sampedro
AU  - C. Fu
AU  - J. -F. Collumeau
AU  - P. Campoy
PY  - 2016
KW  - Image color analysis
KW  - Image segmentation
KW  - Aircraft
KW  - Image edge detection
KW  - Histograms
KW  - Real-time systems
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICUAS.2016.7502586
JO  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 7-10 June 2016
AB  - Vision-based sky segmentation and horizon line detection can be extremely useful to perform important tasks onboard Unmanned Aerial Vehicles (UAVs), such as pose estimation and collision avoidance. Most of the existing vision-based solutions use traditional image processing methods to identify the horizon line. This results in good overall accuracy and fast computation times. However, difficult environmental conditions such as a foggy or cloudy skies hinder correct sky segmentation. This paper proposes a solution for sky segmentation in RGB images using a supervised Machine Learning approach by first splitting the image into fixed-size patches, extracting and classifying color descriptors for each patch and performing a final post-processing stage to improve segmentation quality. A method for automatic horizon line detection is also proposed. The performance of our approach was evaluated on flight images captured onboard UAVs, achieving performance accuracies above 93% at real-time frame rates.
ER  - 


TY  - CONF
TI  - Fully automated mosaicking of pushbroom aerial imagery
T2  - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
SP  - 1105
EP  - 1108
AU  - C. Cariou
AU  - Kacem Chehdi
PY  - 2008
KW  - Global Positioning System
KW  - Mutual information
KW  - Cameras
KW  - Pixel
KW  - Aircraft
KW  - Deformable models
KW  - Laboratories
KW  - Robustness
KW  - Multispectral imaging
KW  - Information theory
KW  - Information theory
KW  - Image registration
KW  - Variational methods
KW  - Image processing
KW  - Geometry
DO  - 10.1109/ICASSP.2008.4517807
JO  - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
IS  - 
SN  - 2379-190X
VO  - 
VL  - 
JA  - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
Y1  - 31 March-4 April 2008
AB  - This communication addresses the problem of the automatic mosaicking of raw images acquired by airborne pushbroom imagers. Using appropriate ancillary data issued from GPS and inertial measurements, we show how the mutual information criterion can be used to improve the co-registration and direct georeferencing of overlapping flight lines by estimating unknown or inaccurate elevation data. The proposed approach does not require any control point to work, and requires only few iterations to improve the initial pose. The mosaicking task itself is performed in a very simple manner. We describe the proposed system, and assess the robustness of our method with an example of application to the mosaicking of multi-track real multispectral image data.
ER  - 


TY  - JOUR
TI  - Fine-Grained Classification via Hierarchical Feature Covariance Attention Module
T2  - IEEE Access
SP  - 35670
EP  - 35679
AU  - Y. Jung
AU  - N. S. Syazwany
AU  - S. Kim
AU  - S. -C. Lee
PY  - 2023
KW  - Covariance matrices
KW  - Feature extraction
KW  - Task analysis
KW  - Visualization
KW  - Principal component analysis
KW  - Matrix decomposition
KW  - Attention module
KW  - covariance
KW  - feature map
KW  - fine-grained classification
DO  - 10.1109/ACCESS.2023.3265472
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Fine-Grained Visual Classification (FGVC) has consistently been challenging in various domains, such as aviation and animal breeds. It is mainly due to the FGVC’s criteria that differ with a considerably small range or subtle pattern differences. In the deep convolutional neural network, the covariance between feature maps positively affects the selection of features to learn discriminative regions automatically. In this study, we propose a method for a fine-grained classification model by inserting an attention module that uses covariance characteristics. Specifically, we introduce a feature map attention module (FCA) to extract the feature map between convolution blocks, constituting the existing classification model. The FCA module then applies the corresponding value of the covariance matrix to the channel to focus on the salient area. We demonstrate the need for fine-grained classification in a hierarchical manner by focusing on the diverse scale representation. Additionally, we implemented two ablation studies to show how each suggested strategy affects classification performance. Our experiments are conducted on three datasets, CUB-200-2011, Stanford Cars, and FGVC-Aircraft, primarily used for fine-grained classification tasks. Our method outperforms the state-of-the-art models by a margin of 0.4%, 1.1%, and 1.4%.
ER  - 


TY  - CONF
TI  - Modulation Recognition Based on Lightweight Neural Networks
T2  - 2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
SP  - 468
EP  - 472
AU  - T. Wang
AU  - Y. Jin
PY  - 2020
KW  - Convolution
KW  - Image recognition
KW  - Modulation
KW  - Signal processing algorithms
KW  - Constellation diagram
KW  - Feature extraction
KW  - Classification algorithms
KW  - modulation recognition
KW  - constellation
KW  - deep learning
KW  - lightweight neural network
KW  - MobileNet
DO  - 10.1109/CISP-BMEI51763.2020.9263501
JO  - 2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)
Y1  - 17-19 Oct. 2020
AB  - In order to solve the problems of complex networks, large amount of calculation and high equipment requirements in the current deep learning method to complete the modulation recognition process, this paper proposes a modulation recognition algorithm based on lightweight neural networks. First, map the common 8 kinds of modulated signals to constellation diagrams to make image data sets. In the process of retaining the original signals, make full use of the performance of the neural network, build a representative the MobileNet neural network in the neural network to complete the training of the data set, use the test samples Verify the effectiveness of the lightweight neural networks used. Simulation experiment results show that the overall recognition rate of modulation reaches 98% when the SNR is greater than 2dB, but the training speed is greatly improved.
ER  - 


TY  - CONF
TI  - A method of aircraft target recognition for meter wave-radar based on convolutional neural network
T2  - IET International Radar Conference (IET IRC 2020)
SP  - 1205
EP  - 1210
AU  - Q. Zhao
AU  - H. Liu
AU  - Y. Lu
PY  - 2020
DO  - 10.1049/icp.2021.0612
JO  - IET International Radar Conference (IET IRC 2020)
IS  - 
SN  - 
VO  - 2020
VL  - 2020
JA  - IET International Radar Conference (IET IRC 2020)
Y1  - 4-6 Nov. 2020
AB  - For meter-wave radar, the time-frequency image is an important technique for aircraft target recognition because of its rich micro-motion characteristics. The traditional pattern recognition method is difficult to extract the important features of time-frequency image, the probability of target recognition is low, and the recognition robustness is poor under low SNR. In order to solve these problems, this paper presents an aircraft target recognition method based on convolutional neural network, whose core idea is to take advantage of automatic and comprehensive image feature extraction by convolutional neural network, improve the recognition probability and noise robustness. Firstly, the time-frequency image datasets for different attitude are generated by modeling the common aircraft fretting characteristics. Secondly, four types of classic network are trained, including AlexNet, VGG19, GoogLeNet and ResNet50. Thirdly, aircraft recognition is implemented by the trained network, and the performance of network is analyzed. The simulation dataset proved the deep learning method is more than 14.5% more accurate than the traditional time-frequency feature and classification method, and more robust for noise. Furthermore, the excellent performance of proposed method is proved by measured dataset, with the recognition rate's improvement of more the 21.8%.
ER  - 


TY  - CONF
TI  - A joint classification and compression system: Processing hyperspectral imagery onboard storageless reconnaissance platforms to support real-time decision making
T2  - 2007 IEEE International Conference on System of Systems Engineering
SP  - 1
EP  - 6
AU  - M. J. Mendenhall
PY  - 2007
KW  - Image storage
KW  - Image coding
KW  - Hyperspectral imaging
KW  - Reconnaissance
KW  - Real time systems
KW  - Decision making
KW  - Remote sensing
KW  - Fuels
KW  - Aircraft propulsion
KW  - Unmanned aerial vehicles
DO  - 10.1109/SYSOSE.2007.4304329
JO  - 2007 IEEE International Conference on System of Systems Engineering
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2007 IEEE International Conference on System of Systems Engineering
Y1  - 16-18 April 2007
AB  - Compressing hyperspectral data such that class discrimination is maintained is a difficult task. In a supervised classification scenario, one has hope of meeting this goal since all information needed to classify the data is present. The challenge then is to determine which subset of features are important for classification and which are not. We summarize our generalized relevance learning vector quantization-improved (GRLVQI) [1], based on Hammer and Villmann's GRLVQ [2], for the joint classification and feature extraction of hyperspectral data. Our previous results show exceptional classification and feature reduction results on 23 and 35 classes of a real hyperspectral data set [1], [3]. We present a scalable architecture for GRLVQI targeted for hardware implementation enabling hyperspectral- based real-time decision making functions. Implementing the GRLVQI system with floating point gate array technology likely provides a means of developing a partial reconfigurable GRLVQI system that allows one add or remove classes on-the-fly without adversely affecting the current state of the classifier or feature extractor.
ER  - 


TY  - CONF
TI  - Discrete Structure Aggregation and Global-region Query-located Network for Fine-grained Visual Classification
T2  - 2024 International Joint Conference on Neural Networks (IJCNN)
SP  - 1
EP  - 8
AU  - D. Ren
AU  - W. Huang
AU  - H. Sun
AU  - Y. Yao
AU  - S. Ren
PY  - 2024
KW  - Training
KW  - Visualization
KW  - Computer vision
KW  - Image recognition
KW  - Codes
KW  - Filtering
KW  - Aggregates
KW  - Fine-grained image classification
KW  - Attention mechanism
KW  - Deep learning
KW  - Plant image recognition
DO  - 10.1109/IJCNN60899.2024.10650389
JO  - 2024 International Joint Conference on Neural Networks (IJCNN)
IS  - 
SN  - 2161-4407
VO  - 
VL  - 
JA  - 2024 International Joint Conference on Neural Networks (IJCNN)
Y1  - 30 June-5 July 2024
AB  - In recent years, the fine-grained classification issue in plants is a hot topical in computer vision. However, most existing fine-grained classification methods focus on finding discriminative regions. These methods overlook the structural information, which is significant to learn more representative structural features and enhance the ability of modules to extract structural features. Moreover, numerous methods use local attention to position regions. They fail to ascertain whether the identified region is the most discriminative on a global scale, which determines whether modules can learn the most discriminative detail features. To address these issues, we propose a Discrete Structure Aggregation and Global-region Querylocated Network (DG-Net) for fine-grained visual classification. Specifically, a Discrete Structure Aggregation Spatial Attention (DSAS) module is proposed to aggregate discrete features and locate the complete object, enhancing the ability of the network to extract the representative structural features. Furthermore, we introduce a Global-region Query-located Attention (GRQL) module, which uses a quadratic attention matrix to assign global importance values for each region, prioritizing the most significant ones and learning the discriminative features. Our method achieves state-of-the-art performance on the 21-Plant, 18-Plant and FGVC-Aircraft datasets. The code is available at https://github.com/0mycode0/code/tree/main/DG-Net.
ER  - 


TY  - CONF
TI  - Damage Estimation and Localization from Sparse Aerial Imagery
T2  - 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)
SP  - 128
EP  - 134
AU  - R. G. Franceschini
AU  - J. Liu
AU  - S. Amin
PY  - 2021
KW  - Location awareness
KW  - Structure from motion
KW  - Conferences
KW  - Estimation
KW  - Manuals
KW  - Machine learning
KW  - Cameras
KW  - Weakly-supervised learning
KW  - Class activation mapping
KW  - Structure from motion
KW  - GIS
KW  - Disaster response
DO  - 10.1109/ICMLA52953.2021.00028
JO  - 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)
Y1  - 13-16 Dec. 2021
AB  - Aerial images provide important situational awarness for responding to natural disasters such as hurricanes. They are well-suited for providing information for damage estimation and localization (DEL); i.e., characterizing the type and spatial extent of damage following a disaster. Despite recent advances in sensing and unmanned aerial systems technology, much of post-disaster aerial imagery is still taken by handheld DSLR cameras from small, manned, fixed-wing aircraft. However, these handheld cameras lack IMU information, and images are taken opportunistically post-event by operators. As such, DEL from such imagery is still a highly manual and time-consuming process. We propose an approach to both detect damage in aerial images and localize it in world coordinates, with specific focus on detecting and localizing flooding. The approach is based on using structure from motion to relate image coordinates to world coordinates via a projective transformation, using class activation mapping to detect the extent of damage in an image, and applying the projective transformation to localize damage in world coordinates. We evaluate the performance of our approach on post-event data from the 2016 Louisiana floods, and find that our approach achieves a precision of 88%. Given this high precision using limited data, we argue that this approach is currently viable for fast and effective DEL from handheld aerial imagery for disaster response.
ER  - 


TY  - CONF
TI  - Exploiting dual-polarization technique in weather radar for civil aircrafts to mitigate risk in adverse conditions
T2  - 2015 1st URSI Atlantic Radio Science Conference (URSI AT-RASC)
SP  - 1
EP  - 2
AU  - F. Cuccoli
AU  - A. Lupidi
AU  - P. Bernabò
AU  - E. Barcaroli
AU  - L. Facheris
AU  - L. Baldini
PY  - 2015
KW  - Meteorology
KW  - Meteorological radar
KW  - Aircraft
KW  - Doppler radar
KW  - Airborne radar
KW  - Aerospace electronics
DO  - 10.1109/URSI-AT-RASC.2015.7303071
JO  - 2015 1st URSI Atlantic Radio Science Conference (URSI AT-RASC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 1st URSI Atlantic Radio Science Conference (URSI AT-RASC)
Y1  - 16-24 May 2015
AB  - Trajectory of civil aircrafts is typically optimized off-board to optimize fuel consumption, using also information available from weather services. Changes to the set route are decided by the pilot based on METAR and NOTAM updates and unexpected adverse weather conditions detected by the weather radar installed on the nose of the aircraft [1]. Typically, weather radars of most civil aircrafts are single-polarization X-band systems (only larger airplanes use C-band) with 3° beam-width flat antenna, following the specifications set by the ARINC 708A standard. Notoriously, attenuation due to propagation through a precipitation filled medium is not negligible at X-band and in the presence of cluster of convective cells, the nearer cells masks or weakens returns from farther cells, ultimately determining a wrong input to the pilot's decision on optimal trajectory. Unfortunately, attenuation correction techniques applicable to single polarization radar are notoriously unreliable and strongly affected by radar calibration bias. Conversely, dual polarization technologies in ground based weather radar have demonstrated the capability of mitigating X-band attenuation based on differential phase shift measurements [2] and therefore could be successfully exploited for civil aviation weather radars. Current systems show to the pilot precipitation returns according to a few levels of reflectivity (the correspondence between colors and levels of reflectivity is not shown) and, within a shorter range, also information on turbulence detected from radar Doppler spectrum width. Meteorological interpretation of such images is largely left to the pilot's experience. Dual polarization radar provides more information arising from the sensitivity their measurements to microphysical properties of particles exploited in hydrometeor classification products [3]. On the other hand, dealing with more information yields increased workload for pilot and therefore, to keep simple and effective the information shown to the pilot, an automated software to process dual-polarization measurements along with trajectory information to support the pilot in decision making is essential. The European Union, through the Clean Sky framework funded several projects to improve airborne weather radars and to optimally use them to optimize flight route. The project KLEAN aimed at using output of the Selex ES Weather Radar Post-Processor software (WRPP) inside an EFB (Electronic Flight Bag) to produce weather classification maps and related binary risk maps as the final radar product to be shown to the pilots or to be used by a trajectory optimizer.
ER  - 


TY  - CONF
TI  - A root-music algorithm for high resolution ISAR imaging
T2  - 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)
SP  - 522
EP  - 526
AU  - A. R. Koushik
AU  - B. S. Shruthi
AU  - R. Rajesh
AU  - R. Sharma
PY  - 2016
KW  - Image resolution
KW  - Marine vehicles
KW  - Radar imaging
KW  - Signal resolution
KW  - Doppler effect
KW  - Imaging
KW  - Multiple signal classification
KW  - ISAR
KW  - Root-MUSIC
KW  - Super Resolution
KW  - Spectral Analysis
DO  - 10.1109/RTEICT.2016.7807876
JO  - 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)
Y1  - 20-21 May 2016
AB  - Inverse Synthetic Aperture Radar (ISAR) is used to image objects such as aircrafts and ships by the virtue of their rotation. The high resolution in range is obtained by large bandwidth pulses and similarly in the azimuth, by the rotational motion of the object. The rotational motion produces a Doppler shift, measured by the phase shift produced on a sequence of coherent pulses. Hence the azimuth resolution is determined by the Doppler discrimination using spectral analysis. In this paper, we propose an analysis technique for classification of ships, based on root-Multiple Signal Classification (MUSIC) algorithm, which betters the performance provided by the conventional Fast Fourier Transform (FFT) technique for ISAR imaging. This technique belongs to a class of super-resolution algorithms which also provides a resolution improved target classification.
ER  - 


TY  - CONF
TI  - A Kind of Research on the Visual Inspection Device of Gantry Aircraft Parts
T2  - 2023 IEEE 16th International Conference on Electronic Measurement & Instruments (ICEMI)
SP  - 98
EP  - 102
AU  - S. Zhi
AU  - J. Chen
AU  - Z. Huang
AU  - S. Qu
AU  - Y. Zhu
PY  - 2023
KW  - Visualization
KW  - Machine vision
KW  - Inspection
KW  - Visual systems
KW  - Robot sensing systems
KW  - Finite element analysis
KW  - Structural engineering
KW  - aircraft parts
KW  - vision measurement
KW  - ball screw
KW  - finite element analysis
DO  - 10.1109/ICEMI59194.2023.10270479
JO  - 2023 IEEE 16th International Conference on Electronic Measurement & Instruments (ICEMI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 16th International Conference on Electronic Measurement & Instruments (ICEMI)
Y1  - 9-11 Aug. 2023
AB  - In response to the demand for rapid and high-precision measurement of aircraft parts, this article proposes an aircraft part detection device based on machine vision measurement, it device has been selected and arranged for the ball screw slide used for measurement. The overall system of the sliding platform and stage has been designed, using machine vision detection technology to collect data and images, feature extraction and part classification are performed on the edges of parts, achieving fast and high-precision measurement of aircraft parts. This article discusses the main composition, working principle, and implementation steps of the visual inspection device for gantry aircraft parts, the structural parameter design and strength verification of the ball screw slide and measurement area, as well as the selection of visual systems. On this basis, through finite element analysis, the key component models are effectively analyzed and validated, proving that the visual inspection device for gantry aircraft parts can meet the design requirements.
ER  - 


TY  - CONF
TI  - A whole scheme for aircraft target identification in HRRP
T2  - 2008 International Conference on Communications, Circuits and Systems
SP  - 871
EP  - 875
AU  - Haitao Jia
AU  - Wei Zhang
AU  - Ke Zhang
AU  - Chi Zhang
AU  - Chunyang Dai
PY  - 2008
KW  - Radar
KW  - Aircraft
KW  - Target recognition
KW  - Airborne radar
KW  - Classification algorithms
KW  - Data mining
KW  - Radar imaging
DO  - 10.1109/ICCCAS.2008.4657908
JO  - 2008 International Conference on Communications, Circuits and Systems
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2008 International Conference on Communications, Circuits and Systems
Y1  - 25-27 May 2008
AB  - this paper describes the authors’ efforts in practical high resolution range profile target automatic recognition. The practical high resolution range profile has more difference to simulation results, so this paper presents a novel algorithm to detect and extract the target range profile data and also create a whole scheme to deal with this difference. First it takes OS CAFR and binary accumulation algorithm to detect target. Here we lower self-adapt threshold to get more targets which maybe be false target. Then we use target trace management to remove the illusion target The novel algorithm, called as endpoint detection based on moving window, can get more valuable data to classification. Practical data has more noise and low PSN, so K-mean algorithm can remove the bad data. Due to variation of target high resolution range profile we build a suffice sample set, which require us to use PCA algorithm to reduce the computer time and memory. Classification is N near classifier, which can get a nonlinear classification effect. The test results prove that this scheme can get better recognition results.
ER  - 


TY  - CONF
TI  - Comparing Deep Learning Performance for Aircraft Detection in Satellite Imagery
T2  - NAECON 2023 - IEEE National Aerospace and Electronics Conference
SP  - 86
EP  - 91
AU  - V. M. Vergara
AU  - J. J. Wojcik
AU  - E. T. Kain
AU  - T. M. Lovelly
PY  - 2023
KW  - YOLO
KW  - Training
KW  - Deep learning
KW  - Analytical models
KW  - Annotations
KW  - Shape
KW  - Computational modeling
DO  - 10.1109/NAECON58068.2023.10366019
JO  - NAECON 2023 - IEEE National Aerospace and Electronics Conference
IS  - 
SN  - 2379-2027
VO  - 
VL  - 
JA  - NAECON 2023 - IEEE National Aerospace and Electronics Conference
Y1  - 28-31 Aug. 2023
AB  - The recent public availability of satellite imagery has pushed forward the field of deep learning allowing the application of novel object detection methods. However, object detection in satellite imagery, such as aircraft detection, presents significant challenges including large image variability due to orientation, scale, and shape. This work makes a summarized comparison of known object detection algorithms including You-Only-Look-Once (YOLO) and RetinaNet-ResNet variants performing aircraft detection on the publicly available RarePlanes dataset. RarePlanes was assessed to be a highly effective training dataset with similar bounding box size distributions across classes. Results indicate that YOLO variants generally provided better performance than RetinaNet-ResNet variants in identical testing scenarios. YOLO variants were especially effective when using high numbers of classes and with classes containing large numbers of annotations.
ER  - 


TY  - CONF
TI  - Signal subspace processing of 4D remote sensing data
T2  - 1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)
SP  - 281
EP  - 288 vol.4
AU  - C. R. Waters
AU  - T. Sommese
AU  - M. Essel
AU  - S. Mark
PY  - 1999
KW  - Signal processing
KW  - Remote sensing
KW  - Sensor arrays
KW  - Hyperspectral sensors
KW  - Multiple signal classification
KW  - Hyperspectral imaging
KW  - Image sensors
KW  - Current measurement
KW  - Signal resolution
KW  - Matched filters
DO  - 10.1109/AERO.1999.792096
JO  - 1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)
IS  - 
SN  - 
VO  - 4
VL  - 4
JA  - 1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)
Y1  - 7-7 March 1999
AB  - The thrust toward high resolution multispectral staring array sensors for a variety of remote sensing applications has produced a need for efficient processing techniques that fully exploit the time, space and spectral correlations of the data. Over the past two decades, the techniques of signal subspace processing have been especially effective in processing 2 and 3 dimensional data. These include approaches such as TMF (Temporal Matched Filter), MUSIC (MUltiple SIgnal Classification), Principal Components and STAP (Space Time Adaptive Processing). This paper extends the basic theory and algorithms to handle 4D (spectral, temporal and two spatial dimensions). Suboptimal formulations are presented that reduce the computational requirements while still taking advantage of the critical correlations. The background estimation and target detection performance of these techniques are demonstrated using (a) high fidelity simulated scene sets corresponding to hyperspectral and multiband imaging sensors (b) measured data sets from a current multiband space sensor The paper concludes with recommended algorithm investigations and measured data set evaluations.
ER  - 


TY  - CONF
TI  - Detection and State Classification of Bolts Based on Faster R-CNN
T2  - 2022 28th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)
SP  - 1
EP  - 5
AU  - Z. Su
AU  - M. Wu
AU  - Z. Zhang
AU  - H. Wen
AU  - Z. Xia
AU  - H. Zheng
PY  - 2022
KW  - Training
KW  - Mechatronics
KW  - Machine vision
KW  - Manuals
KW  - Fasteners
KW  - Maintenance engineering
KW  - Performance analysis
KW  - Bolt State
KW  - Bolt Detection
KW  - State Classification
KW  - Faster R-CNN
DO  - 10.1109/M2VIP55626.2022.10041100
JO  - 2022 28th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 28th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)
Y1  - 16-18 Nov. 2022
AB  - With the increase of the training frequency of aircraft, the frequency of daily maintenance work also increases dramatically. As key connectors in aircraft, the demand for bolts also significantly increases. Hardness is an important performance index of bolts. At present, the manual inspection method cannot meet the requirement for the hardness test of the large number of the bolts. Therefore, there is an urgent need to develop an automated hardness detection system. In this paper, a bolt state detection method based on Faster R-CNN is proposed, which is an important part of an automated hardness detection system and can assist the robot grasping the bolt and placing it on the hardness tester. This method can detect bolts on the pallet and classify their states. The mean average precision (mAP) of this model used in the bolt dataset is 96.44%.
ER  - 


TY  - CONF
TI  - Deep Learning Framework for Macro Marine Litter Classification and Quantification
T2  - 2022 3rd International Conference for Emerging Technology (INCET)
SP  - 1
EP  - 6
AU  - S. Kumar
AU  - A. Gautam
AU  - K. Mehlawat
AU  - K. Rawat
PY  - 2022
KW  - Deep learning
KW  - Government
KW  - Approximation algorithms
KW  - Classification algorithms
KW  - Water resources
KW  - Monitoring
KW  - Macro Marine Litter (MML)
KW  - data augmentation
KW  - Object Detection
KW  - YOLOv3
KW  - line sweep algorithms
DO  - 10.1109/INCET54531.2022.9825334
JO  - 2022 3rd International Conference for Emerging Technology (INCET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 3rd International Conference for Emerging Technology (INCET)
Y1  - 27-29 May 2022
AB  - Marine Litter has become a serious concern in today’s world. It is not only dangerous for marine life, but also has a negative socio-economic impact. Tourism being one of the major contributors to the economy, cleanliness around the water bodies becomes one of the major pointers to attract tourists. With present Governments investing in the marine litter management, it becomes essential to properly utilize the resources for litter management. Inadequate knowledge of quantity and type of litter leads to mismanagement and wastage of resources.Therefore, we propose deep learning-based solution of Marine Litter detection. With the type and quantity of several types of marine waste, better management can be done. Along with this, we also propose a line sweep algorithm to find the approximate area cover of the marine litter using the scale of the images. Knowledge of area cover can help in litter management and monitoring.
ER  - 


TY  - JOUR
TI  - Channel Expansion Convolutional Network for Image Classification
T2  - IEEE Access
SP  - 178414
EP  - 178424
AU  - Y. Yang
AU  - X. Wang
AU  - B. Sun
AU  - Q. Zhao
PY  - 2020
KW  - Convolution
KW  - Sorting
KW  - Task analysis
KW  - Biological system modeling
KW  - Computational modeling
KW  - Visualization
KW  - Adaptation models
KW  - Channel expansion network
KW  - object-level attention CNN
KW  - multi-scale CNN
KW  - image classification
DO  - 10.1109/ACCESS.2020.3027879
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - With the continuous evolution of research on convolutional neural networks, it is an efficient and fashionable method to introduce attention mechanism into the convolutional structure. The channel attention designed in SENet has made a great contribution to the promotion of the attention convolution model. However, our research found that SENet focuses on certain feature channels rather than objects in the channels. It will simultaneously enhance or weaken the target objects and background information in a certain channel. On the basis of the channel attention convolution network, we first perform channel sorting and group convolution on the feature map, and expand each group to β times the original feature channel during the group convolution process to construct a channel expansion convolution network (CENet), where β is an array used to represent the channel expansion coefficient. CENet captures the attention of objects in the feature channel while expanding the proportion of features in the relatively important channel. Furthermore, we improved the structure of CENet and merged it into the intra-layer multi-scale convolutional model to construct an object-level attention multi-scale convolutional neural network (OAMS-CNN). We have conducted a large number of experiments on four data sets, CIFAR-10, CIFAR-100, FGVC-Aircraft and Stanford Cars. The experimental results show that our proposed new object-level attention convolution model has achieved good image classification results.
ER  - 


TY  - CONF
TI  - Object Classification in Thermal Images using Convolutional Neural Networks for Search and Rescue Missions with Unmanned Aerial Systems
T2  - 2018 International Joint Conference on Neural Networks (IJCNN)
SP  - 1
EP  - 8
AU  - C. D. Rodin
AU  - L. N. de Lima
AU  - F. A. de Alcantara Andrade
AU  - D. B. Haddad
AU  - T. A. Johansen
AU  - R. Storvold
PY  - 2018
KW  - Cameras
KW  - Boats
KW  - Pallets
KW  - Sea surface
KW  - Ocean temperature
KW  - Shape
KW  - Search problems
DO  - 10.1109/IJCNN.2018.8489465
JO  - 2018 International Joint Conference on Neural Networks (IJCNN)
IS  - 
SN  - 2161-4407
VO  - 
VL  - 
JA  - 2018 International Joint Conference on Neural Networks (IJCNN)
Y1  - 8-13 July 2018
AB  - In recent years, the use of Unmanned Aerial Systems (UAS) has become commonplace in a wide variety of tasks due to their relatively low cost and ease of operation. In this paper, we explore the use of UAS in maritime Search And Rescue (SAR) missions by using experimental data to detect and classify objects at the sea surface. The objects are chosen as common objects present in maritime SAR missions: a boat, a pallet, a human, and a buoy. The data consists of thermal images and a Gaussian Mixture Model (GMM) is used to discriminate foreground objects from the background. Then, bounding boxes containing the object are defined and used to train a Convolutional Neural Network (CNN). The CNN achieves the average accuracy of 92.5% when evaluating a testing dataset.
ER  - 


TY  - CONF
TI  - Automatic ship classification by superstructure moment invariants and two-stage classifier
T2  - [Proceedings] Singapore ICCS/ISITA `92
SP  - 544
EP  - 547 vol.2
AU  - Qian Zhongliang
AU  - Wang Wenjun
PY  - 1992
KW  - Marine vehicles
KW  - Libraries
KW  - Statistics
KW  - Azimuth
KW  - Object recognition
KW  - Neural networks
KW  - Aircraft
KW  - Shape
KW  - Ear
KW  - Sea measurements
DO  - 10.1109/ICCS.1992.254892
JO  - [Proceedings] Singapore ICCS/ISITA `92
IS  - 
SN  - 
VO  - 
VL  - 
JA  - [Proceedings] Singapore ICCS/ISITA `92
Y1  - 16-20 Nov. 1992
AB  - Moment invariants have been used as descriptive feature in a variety of object recognition applications. A new descriptive scheme called superstructure moment invariants is presented. The authors calculate moment invariants only for the superstructure of a ship, which are different from the general moment invariants for the entire ship. The analysis of the theory and the statistics of the experimental results shows that the classification accuracy using superstructure moment invariants for the ship is higher than that of general ones. A modified 1-NN classifier, two stage classification is presented, which reduces the time expense and has a high classification accuracy.<>
ER  - 


TY  - CONF
TI  - Construction of a Reliability Measurement Scheme for Deep Learning Vision Algorithms based on Failure Modes
T2  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
SP  - 178
EP  - 184
AU  - X. Ai
AU  - P. Wang
AU  - L. Meng
AU  - H. Ren
AU  - Q. Dong
AU  - G. Yang
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Software algorithms
KW  - Failure analysis
KW  - Object detection
KW  - Inference algorithms
KW  - Classification algorithms
KW  - Software reliability
KW  - Image classification
KW  - Fault trees
KW  - deep learning
KW  - computer vision
KW  - failure mode
KW  - reliability
KW  - measurement and evaluation
DO  - 10.1109/ICITES62688.2024.10777423
JO  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Intelligent Technology and Embedded Systems (ICITES)
Y1  - 20-23 Sept. 2024
AB  - With the application of vision algorithms in multitask scenarios such as image classification, object detection and object tracking, the traditional software reliability measurement scheme is difficult to meet the needs of new technologies. Therefore, the research related to the reliability measurement of deep learning vision algorithms has become an urgent need. In this paper, we collect and analyse failure cases in five vision application scenarios, such as image classification, object detection, object tracking, and semantic segmentation, to carry out failure analysis and fault tree analysis. Using the defective causes of the failure cases, ten reliability measurements of deep learning vision algorithms are established, covering four stages: dataset, algorithm training, algorithm inference and algorithm deployment. Through experiments, the influence of each index in the reliability measurement scheme was analysed. The experimental results show that all the measurements in the reliability measurement scheme constructed in this paper have a certain degree of influence, and the three measurements of data balance defect, interference data, and data accuracy defect have the highest influence on the vision task.
ER  - 


TY  - CONF
TI  - ACARS Signal Source Generation and Recognition Based on Convolutional Neural Network
T2  - 2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
SP  - 1
EP  - 5
AU  - G. Wang
AU  - C. Zou
AU  - C. Zhang
AU  - C. Pan
AU  - J. Song
AU  - F. Yang
PY  - 2021
KW  - Training
KW  - Image processing
KW  - Poles and towers
KW  - Transforms
KW  - Receivers
KW  - Software
KW  - Convolutional neural networks
KW  - CNN
KW  - VGG
KW  - ACARS
KW  - Pre-transformation Net
DO  - 10.1109/BMSB53066.2021.9547081
JO  - 2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
IS  - 
SN  - 2155-5052
VO  - 
VL  - 
JA  - 2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
Y1  - 4-6 Aug. 2021
AB  - With the rise of artificial intelligence technology, using convolutional neural network (CNN), which has outstanding effect in other fields, to quickly identify and classify electromagnetic signals has become a subject with practical value. In this paper, the downlink Aircraft Communications Addressing and Reporting System (ACARS) signals of civil aircraft communicating with the airport tower is studied. With a digital receiver used to collect signals, software decoding signals and cross-correlation method adopted to locate and crop signals, a dataset contains thousands of ACARS signals is created. The type of the aircraft sending the signal is taken as the learning target to carry out the classification task. Using a pre-transformation network consisting of several convolutional layers to take full advantage of the information of the signal, the multiscale information and periodic characteristics of the signal are fused. This net transforms the input signal to a 3×224×224 feature map, and then puts it into a classic network like VGG19 and ResNet, the accuracy rate of 92.2% and 88.8% is achieved on two different levels goals respectively.
ER  - 


TY  - CONF
TI  - A Comprehensive Study of Various Techniques for Hostile Drone Detection and Their Classification
T2  - 2021 6th International Conference on Communication and Electronics Systems (ICCES)
SP  - 1538
EP  - 1543
AU  - R. R. Dani
AU  - R. R. Kubde
AU  - M. A. Sadhu
PY  - 2021
KW  - Technical requirements
KW  - Terrorism
KW  - Radar detection
KW  - Imaging
KW  - Radar imaging
KW  - Radar tracking
KW  - Sensor systems
KW  - Unmanned Aerial Vehicle (UAV)
KW  - Drones
KW  - RADAR
KW  - Airspace Classification
KW  - Unique Identification Number (UIN)
KW  - Air Defence Clearance (ADC)
KW  - Flight Information Center (FIC)
DO  - 10.1109/ICCES51350.2021.9489236
JO  - 2021 6th International Conference on Communication and Electronics Systems (ICCES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 6th International Conference on Communication and Electronics Systems (ICCES)
Y1  - 8-10 July 2021
AB  - Today's atmosphere is being increasingly congested due to the proliferation of airborne objects such as small airplanes, helicopters, and unmanned aerial vehicles (drones) of varying forms and sizes. The UAVs can also become a major threat as they can be utilized in smuggling, industrial spy, robbery, and terrorism, besides enabling access to new technologies. Owing to its speed, agility, and altitude, it is hard to defend against a UAV incursion. Aerial defence systems are integrated with a wide range of sensors to efficiently track aircraft. There are several methods and techniques discussed in this paper that are ideal for this specific application. This paper also discusses the paradigm of drone detection using Radar and Imaging techniques and the classification of drones based on their size, features, flying height, and Unique Identification Number (UIN).
ER  - 


TY  - CONF
TI  - On The Application Of Multifrequency Polarimetric Radar Observations To Sea-ice Classification
T2  - [Proceedings] IGARSS '92 International Geoscience and Remote Sensing Symposium
SP  - 576
EP  - 578
AU  - E. Rignot
AU  - M. R. Drinkwater
PY  - 1992
KW  - Radar applications
KW  - Radar polarimetry
KW  - Sea ice
KW  - Backscatter
KW  - Radar imaging
KW  - Frequency
KW  - Polarization
KW  - Aircraft
KW  - Calibration
KW  - Labeling
DO  - 10.1109/IGARSS.1992.576774
JO  - [Proceedings] IGARSS '92 International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - [Proceedings] IGARSS '92 International Geoscience and Remote Sensing Symposium
Y1  - 26-29 May 1992
AB  - 
ER  - 


TY  - CONF
TI  - SAR Target Recognition and Angle Estimation by Using Rotation-Mapping Network
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 3577
EP  - 3580
AU  - Y. Zhou
AU  - W. Wang
AU  - C. Wang
AU  - X. Yang
AU  - J. Shi
AU  - S. Wei
PY  - 2021
KW  - Training
KW  - Radar remote sensing
KW  - Image recognition
KW  - Target recognition
KW  - Estimation
KW  - Apertures
KW  - Feature extraction
KW  - Convolutional Neural Network (CNN)
KW  - synthetic aperture radar (SAR)
KW  - classification
KW  - angle estimation
DO  - 10.1109/IGARSS47720.2021.9554634
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - Convolutional neural network (CNN) has become the mainstream method in the field of image recognition for its excellent ability to feature extraction. Most of the CNNs increase the classification accuracy for the rotational objects by imposing the network with rotation invariance or equivariance property, which causes the loss of the target's orientation information. In this work, a rotation-mapping network (RM-Net) that can achieve objects recognition and angle or orientation estimation simultaneously without additional network training is constructed. Besides, an octagona convolutional kernel is introduced to improve the network's performance. The experiments on the simulation SAR datasets show that the proposed RM-CNN can achieve state-of-the-art results in target recognition and angle estimation.
ER  - 


TY  - JOUR
TI  - Multistatic ISAR autofocus with an image entropy-based technique
T2  - IEEE Aerospace and Electronic Systems Magazine
SP  - 30
EP  - 36
AU  - S. Brisken
AU  - M. Martella
PY  - 2014
KW  - Receivers
KW  - Trajectory
KW  - Radar imaging
KW  - Global Positioning System
KW  - Transmitters
KW  - Electronics packaging
KW  - Targeting
KW  - Missiles
KW  - Surveillance
DO  - 10.1109/MAES.2014.130140
JO  - IEEE Aerospace and Electronic Systems Magazine
IS  - 7
SN  - 1557-959X
VO  - 29
VL  - 29
JA  - IEEE Aerospace and Electronic Systems Magazine
Y1  - July 2014
AB  - A key issue in radar research for many years has been the noncooperative identification (NCI) of moving targets. Such targets can be aircraft, ships, ground vehicles, satellites, or ballistic missiles. Nowadays, identification is mostly based on cooperative methods like the identification friend-foe (IFF) systems in the military sector or automatic dependent surveillance-broadcast (ADS-B) for civil aircraft. However, a number of situations exist in which a cooperative identification is not possible. In conflict situations, hostile targets do not identify themselves. Knowledge of their type by NCI yields a tactical advantage. History has also shown a significant number of cases in which neutral or friendly targets have been destroyed due to their inability to identify themselves. Neutral targets usually do not share IFF codes of conflict parties, and friendly targets may turn off their IFF when performing covert operations. Operations between allied nations have also been a source of possible IFF failure. Furthermore, the desire for NCI is not limited to conflict situations. Unidentified aircraft or ships often are suspected to be terrorists. In all of the above situations, a decision on a possible response is extremely time critical, and possible consequences can be serious. For this reason, NCI remains an important research topic.
ER  - 


TY  - CONF
TI  - A multi/hyper-spectral imaging system for land use/land cover using unmanned aerial systems
T2  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1148
EP  - 1155
AU  - A. Mancini
AU  - E. Frontoni
AU  - P. Zingaretti
PY  - 2016
KW  - Indexes
KW  - Sensors
KW  - Remote sensing
KW  - Payloads
KW  - Imaging
KW  - Spatial resolution
DO  - 10.1109/ICUAS.2016.7502662
JO  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 7-10 June 2016
AB  - During last years the automated classification enabled by Unmanned Aerial System (UASs) captured the interest of many investors and researchers. Several applications could be explored ranging from precision agriculture to Land Use / Land Cover (LU/LC) thematic mapping. The use of UASs is novel when we consider the very high resolution (VHR) multi-hyper spectral sensing of a given area. Remote sensing and LU/LC have a strong intersection from many years even if only in the last period VHR images over several bands are applicable considering the technological progress. Today it is possible to acquire data in Visible (VIS) - Near InfraRed (NIR)-Short Wave InfraRed (SWIR) by using compact and low cost sensors that could be integrated into small size UAS. These sensors overcame the main limitations of classical remote sensed data from satellite increasing the spectral, spatial and temporal resolution also reducing the influence of clouds and water vapor on atmospheric absorption. In particular, in this paper we propose an imaging system to perform analysis from thematics maps derived from hyper-spectral radiometers and multi-spectral cameras mounted on UAS. The high spectral and geometric resolution enhance the level of details of a LU/LC maps. We propose also a novel method to fast classify data by using an improved version of the k-means algorithm. The proposed method significantly reduces the computational time especially for very large high-resolution data-set. Comparison of k-means over regular grid and quadtree decomposition are discussed.
ER  - 


TY  - CONF
TI  - PFNet: A Novel Part Fusion Network for Fine-Grained Image Categorization
T2  - 2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)
SP  - 1
EP  - 6
AU  - J. Guo
AU  - J. Liang
AU  - L. Bai
AU  - S. Lao
PY  - 2018
KW  - Feature extraction
KW  - Proposals
KW  - Training
KW  - Task analysis
KW  - Automobiles
KW  - Fuses
KW  - Object detection
KW  - Fine-grained categorization
KW  - CNN
KW  - image classi-fication
KW  - PFN et
DO  - 10.1109/BigMM.2018.8499094
JO  - 2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)
Y1  - 13-16 Sept. 2018
AB  - Previous work in fine-grained image categorization focuses on integrating multiple deep CNN models or complicated attention mechanism, resulting in increasing cumbersome networks, and requiring expensive expert guidance. In this work, a part fusion network (PFNet) is proposed to fuse image parts for classification, which consists of a Fast R-CNN head to extract part features and a two-level classification network to encode part-level and image-level features simultaneously. Features from These two levels are trained with focal loss and cross-entropy loss respectively. By utilizing the focal loss, PFNet can learn more from hard examples of parts and thus generate more discriminative image features. Furthermore, it does not require extra part annotations and can be trained end to end. Experiments are performed on three challenging datasets in fine-grained image categorization task, namely, CUB-200-2011, Stanford Cars and FGVC-Aircraft, and results show that the PFNnet proposed in this work achieves promising results comparing with the state of the art. In datasets Stanford Cars and FGVC-Aircraft, our PFNet outperforms the best results marginally reported in previous work, and achieves comparable accuracy in datasets CUB-200-2011.
ER  - 


TY  - CONF
TI  - Machine Learning Based Damage Detection in Photovoltaic Arrays Using UAV-Acquired Infrared and Visual Imagery
T2  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 264
EP  - 271
AU  - A. Barrett
AU  - D. Bratanov
AU  - N. Amarasingam
AU  - D. Sera
AU  - F. Gonzalez
PY  - 2024
KW  - Training
KW  - Visualization
KW  - Pipelines
KW  - Manuals
KW  - Maintenance
KW  - Solar panels
KW  - Australia
KW  - deep learning
KW  - drone
KW  - solar panel
KW  - solar energy
KW  - PVarray imaging
DO  - 10.1109/ICUAS60882.2024.10556847
JO  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 4-7 June 2024
AB  - The rapid global expansion of solar panel installations necessitates more efficient and cost-effective methods for performance monitoring and maintenance. The New England Solar Project, Australia's largest solar installation is a prime example of the scale and complexity of modern solar farms, making it increasingly challenging to rely solely on manual ground-based inspections. This paper addresses the challenge by focusing on the integration of unmanned aerial systems (UAS) based imagery and deep learning (DL) techniques to develop a semi-automated pipeline for accurately identifying and classifying photovoltaic (PV) cell surface damage. The study leverages the YOLOv8 and Faster R-CNN models to achieve this goal. Drone based visual and infrared spectrum imagery collected from a solar installation site in Queensland, Australia, during October 2022 form the basis of the dataset, enabling the training and evaluation of these models. Three distinct damage classifications (Single-Cell, Multi-Cell, and Surface-Anomaly) were established with input from a subject matter expert to ensure accurate categorization of damage types. The research results indicate promising outcomes for classifying the distinct damage classes. The YOLOv8s-seg model achieved a mean average precision (mAP) of 87% to segment the solar panels. The YOLOv8m model, trained with a relatively small dataset, achieved a commendable mAP of 76% for solar panel damage detection. The Faster R-CNN model showed potential in detecting damage with high confidence, although a more comprehensive evaluation is needed. This research contributes to the broader goal of enhancing preventive maintenance practices, thereby reducing damage-related losses, and ensuring the long-term sustainability of solar installations.
ER  - 


TY  - CONF
TI  - Unmanned Aircraft Applications in Radiological Surveys
T2  - 2018 IEEE International Symposium on Technologies for Homeland Security (HST)
SP  - 1
EP  - 5
AU  - K. Kochersberger
AU  - J. Peterson
AU  - P. Kumar
AU  - J. Bird
AU  - M. McLean
AU  - W. Czaja
AU  - W. Li
AU  - N. Monson
PY  - 2018
KW  - Aircraft
KW  - Robots
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Detectors
KW  - Data collection
KW  - Uncertainty
KW  - Aerial gamma-ray survey
KW  - Emergency response
KW  - unmanned aircraft
KW  - Non-linear dimension reduction
DO  - 10.1109/THS.2018.8574133
JO  - 2018 IEEE International Symposium on Technologies for Homeland Security (HST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE International Symposium on Technologies for Homeland Security (HST)
Y1  - 23-24 Oct. 2018
AB  - Unmanned vehicles, equipped with radiation detection sensors, can serve as a valuable aid to personnel responding to radiological incidents. The use of tele-operated ground vehicles avoids human exposure to hazardous environments, which in addition to radioactive contamination, might present other risks to personnel. Autonomous unmanned vehicles using algorithms for radioisotope classification, source localization, and efficient exploration allow these vehicles to conduct surveys with reduced human supervision allowing teams to address larger areas in less time. This work presents systems for autonomous radiation search with results presented in several proof-of-concept demonstrations.
ER  - 


TY  - JOUR
TI  - Aircraft detection: a case study in using human similarity measure
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 1404
EP  - 1414
AU  - B. Kamgar-Parsi
AU  - A. K. Jain
AU  - J. E. Dayhoff
PY  - 2001
KW  - Computer aided software engineering
KW  - Humans
KW  - Anthropometry
KW  - Shape measurement
KW  - Military aircraft
KW  - Testing
KW  - Databases
KW  - Infrared imaging
KW  - Euclidean distance
KW  - Infrared detectors
DO  - 10.1109/34.977564
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 12
SN  - 1939-3539
VO  - 23
VL  - 23
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - Dec. 2001
AB  - After the most prominent signal in an infrared image of the sky is extracted, the question is whether the signal corresponds to an aircraft. We present a new approach that avoids metric similarity measures and the use of thresholds, and instead attempts to learn similarity measures like those used by humans. In the absence of sufficient real data, the approach allows one to specifically generate an arbitrarily large number of training exemplars projecting near the classification boundary. Once trained on such a training set, the performance of our neural network-based system is comparable to that of a human expert and far better than a network trained only on the available real data. Furthermore, the results obtained are considerably better than those obtained using an Euclidean discriminator.
ER  - 


TY  - JOUR
TI  - Optimizing Video Analytics Deployment for In-Flight Cabin Readiness Verification
T2  - IEEE Access
SP  - 92985
EP  - 92995
AU  - U. Elordi
AU  - N. Aranjuelo
AU  - L. Unzueta
AU  - J. L. Apellaniz
AU  - I. Arganda-Carreras
PY  - 2023
KW  - Cameras
KW  - Computer vision
KW  - Graphics processing units
KW  - Visual analytics
KW  - Deep learning
KW  - Aircraft
KW  - Pattern recognition
KW  - Aerospace electronics
KW  - Aircraft
KW  - computer vision
KW  - deep learning
KW  - optimal deployment
KW  - pattern recognition
KW  - video analytics
DO  - 10.1109/ACCESS.2023.3309050
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - This paper proposes an approach to optimize the deployment of on-board video analytics for checking the correct positioning of luggage in aircraft cabins. The system consists of embedded cameras installed on top of the cabin and a heterogeneous embedded processor. Each camera covers multiple regions of interest (i.e., multiple seats or aisle sections) to minimize the number of cameras required. Each image region is processed by a separate image classification algorithm trained with the expected kind of visual appearance considering the effect of perspective and lens distortion. They classify these regions as correct or incorrect for cabin readiness by exploiting the hierarchical structure of classes composed of different configurations of passengers’ and objects’ presence or absence and the objects’ location. Our approach leverages semantic distances between classes to guide prototypical neural networks for multi-tasking between the main classification (i.e., correct or incorrect status) and auxiliary attributes (i.e., scene configurations), learning robust features from different data domains (i.e., various cabins, real or synthetic). The processing pipeline optimizes response delay and power consumption by leveraging embedded processors’ computing capabilities. We carried out experiments in a cabin mockup with a Jetson AGX Xavier, efficiently obtaining better-quality descriptive information from the scene to improve the system’s accuracy compared to alternative state-of-the-art methods.
ER  - 


TY  - CONF
TI  - Landing area recognition by image applied to an autonomous control landing of VTOL aircraft
T2  - 2017 18th International Carpathian Control Conference (ICCC)
SP  - 240
EP  - 245
AU  - M. F. Silva
AU  - A. S. Cerqueira
AU  - V. F. Vidal
AU  - L. M. Honório
AU  - M. F. Santos
AU  - E. J. Oliveira
PY  - 2017
KW  - Global Positioning System
KW  - Aircraft
KW  - Pattern recognition
KW  - Sensors
KW  - Boosting
KW  - Feature extraction
KW  - Aerospace control
KW  - Machine learning
KW  - Artificial neural network
KW  - Principal component analysis
KW  - Unmanned aerial vehicles
DO  - 10.1109/CarpathianCC.2017.7970404
JO  - 2017 18th International Carpathian Control Conference (ICCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 18th International Carpathian Control Conference (ICCC)
Y1  - 28-31 May 2017
AB  - The pattern recognition aims to classify objects on different categories based on characteristics analysis. The usage of pattern recognition shows itself more and more frequent and widely used, covering different areas both in industry and research and development of new technologies. With that in mind, this work aims to compare two nonlinear classifiers, the Adaptive Boosting method and the Artificial Neural Network method, applied to the identification of a certain landmark, where the more profitable is inserted in a Vertical Take-Off and Landing (VTOL) aircraft real model to trigger the land action after a demanded mission in the trained pattern presence. It is used as sensing method, computer vision technique, from camera's acquired images the characteristics are extracted by a proceeding based on Viola-Jones technique. To optimize the classification, it is also used the Principal Component Analysis method to uncouple the amount of data in the training stage and optimize the results in both classifiers. To prove the efficiency of the classifier when the aircraft is flying, it is used to test a scenario where it is possible to simulate the landing action with different altitudes. The Adaptive Boosting method proved itself to be more advantageous due to its simple implementation and less computational processing effort, despite the slightly lower performance when it comes to classifying compared to the Artificial Neural Network. The Principal Component Analysis method also shows itself to be a good improvement when applied to both techniques, raising the success rate of the classifiers in all the tested cases. The results obtained in the simulation tests were considered satisfactory as the aircraft lands with great precision over the determined landmark after identifying the landing area used for training.
ER  - 


TY  - CONF
TI  - Comparison of aerial target recognition methods based on ISAR imagery
T2  - 2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )
SP  - 150
EP  - 153
AU  - Y. He
AU  - F. Ma
AU  - Y. Li
PY  - 2022
KW  - Image recognition
KW  - Target recognition
KW  - Classification algorithms
KW  - Bayes methods
KW  - Pattern recognition
KW  - Aircraft
KW  - Regression tree analysis
KW  - ISAR images
KW  - airborne targets
KW  - decision trees
KW  - logistic regression
KW  - random forests
KW  - plain Bayesian
DO  - 10.1109/IAEAC54830.2022.9930013
JO  - 2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )
IS  - 
SN  - 2689-6621
VO  - 
VL  - 
JA  - 2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )
Y1  - 3-5 Oct. 2022
AB  - With the development of science and technology, target recognition has become a hot spot in the field of pattern recognition. In the military field, accurate identification of enemy targets is an important prerequisite for winning in high-tech warfare, and aerial target recognition is one of the typical applications, where higher recognition rates are particularly important due to the greater mobility and flexibility of aerial targets. This paper focuses on finding classifier algorithms with higher recognition rates by comparing the recognition effects of different classifiers on ISAR imaging data of aerial targets.
ER  - 


TY  - CONF
TI  - Radar Target Recognition Using LVQ Network and Majority Voting
T2  - 2008 Congress on Image and Signal Processing
SP  - 184
EP  - 187
AU  - D. -G. Xie
AU  - X. -D. Zhang
AU  - Y. -F. Hu
PY  - 2008
KW  - Radar
KW  - Target recognition
KW  - Voting
KW  - Feature extraction
KW  - Signal resolution
KW  - Data mining
KW  - Physics
KW  - Vector quantization
KW  - Aircraft
KW  - Anechoic chambers
KW  - radar target recognition
KW  - range profile
KW  - feature extraction
KW  - LVQ network
KW  - majority voting
DO  - 10.1109/CISP.2008.364
JO  - 2008 Congress on Image and Signal Processing
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2008 Congress on Image and Signal Processing
Y1  - 27-30 May 2008
AB  - This paper describes a novel method for radar target classification based on high range resolution profile (HRRP). In view of the non-stationary characteristic of radar signal, adaptive Gaussian basis representation (AGR) is utilized to extract features from raw HRRP signatures to fully retain the physics information of target. Then learning vector quantization (LVQ) network is adopted to tackle the classification of single echo (after features extraction) with complicated space distribution. Finally ,a combined classification scheme combining LVQ networks with the majority voting rule is designed to circumvent the sensitivity of HRRP to target aspects based on sequential echoes. A actual example using three scaled aircraft model data collected in microwave anechoic chamber is presented to demonstrate the effectiveness of proposed scheme.
ER  - 


TY  - CONF
TI  - Aircrafts recognition using convolutional neurons network
T2  - International Conference on Radar Systems (Radar 2017)
SP  - 1
EP  - 4
AU  - A. Toumi
AU  - A. E. Housseini
AU  - A. Khenchaf
PY  - 2017
KW  - Target recognition
KW  - Deep learning
KW  - ISAR images
KW  - classification
DO  - 10.1049/cp.2017.0519
JO  - International Conference on Radar Systems (Radar 2017)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - International Conference on Radar Systems (Radar 2017)
Y1  - 23-26 Oct. 2017
AB  - We will present in this paper, a novel approach to achieve the aircraft target recognition task using deep learning (DL) approach based on Inverse Synthetic Aperture Radar (ISAR) images reconstructed using IFFT Inverse Fast Furrier Transform). We will propose mainly in this work the deep learning algorithms based on convolutional neural network (CNN) architecture. In the second step and in order to optimize the convolution of DL steps, we propose to use a convolutional auto-encoder which may be better suited to image processing. To validate our approach, some experimentation results are given and compared. The obtained results show that the proposed approach of DL improves the high accuracy recognition and is efficient to well-known classifiers.
ER  - 


TY  - CONF
TI  - Generalization in Object Recognition from SAR Imagery
T2  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1007
EP  - 1010
AU  - F. Sica
AU  - A. Pulella
AU  - C. V. Lopez
AU  - H. Anglberger
AU  - R. Hänsch
PY  - 2022
KW  - Training
KW  - Deep learning
KW  - Image recognition
KW  - Target recognition
KW  - Training data
KW  - Geoscience and remote sensing
KW  - Image representation
KW  - Synthetic Aperture Radar
KW  - Object Recognition
KW  - Automatic Target Recognition
KW  - Machine Learning
KW  - Deep Learning
DO  - 10.1109/IGARSS46834.2022.9884427
JO  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 17-22 July 2022
AB  - Object recognition in synthetic aperture radar images is a well studied topic that has gained a significant amount of attention within the last decades. Modern approaches are based on machine learning, i.e. deep learning, and often show excellent performance. What is so far missing in the literature is a study dedicated to the generalization capabilities of object recognition approaches, i.e. how well a given system can be transferred to new and previously unseen data. In this paper, the proposed recognition model is trained and tested on a unique dataset of 25 high-resolution TerraSAR-X images (X-band), acquired over four different airports in Staring Spotlight mode. We show how classification performance changes for different application scenarios which require different training and evaluation setups.
ER  - 


TY  - CONF
TI  - A False Data Injection Attack Detection Approach Using Convolutional Neural Networks in Unmanned Aerial Systems
T2  - 2022 IEEE Symposium on Computers and Communications (ISCC)
SP  - 1
EP  - 6
AU  - C. Titouna
AU  - F. Naït-Abdesselam
PY  - 2022
KW  - Measurement
KW  - Computers
KW  - Interpolation
KW  - Computational modeling
KW  - Euclidean distance
KW  - Autonomous aerial vehicles
KW  - Convolutional neural networks
KW  - Unmanned aerial vehicles
KW  - False data injection attack detection
KW  - Convolutional neural network
DO  - 10.1109/ISCC55528.2022.9912761
JO  - 2022 IEEE Symposium on Computers and Communications (ISCC)
IS  - 
SN  - 2642-7389
VO  - 
VL  - 
JA  - 2022 IEEE Symposium on Computers and Communications (ISCC)
Y1  - 30 June-3 July 2022
AB  - With the growing use of Unmanned Aerial Vehicles (UAVs) in military and civilian applications, cyber-attacks are increasing significantly. Therefore, detection of attacks becomes indispensable for such systems. In this paper, we focus on the detection of False Data Injection (FDI) attacks in Unmanned Aerial Systems (UASs). Considered to be the most performed attack, an attacker injects fake data into the system in order to disrupt the final decision. To combat this threat, our proposal is built on image analysis and classification. First, we resize the received image in order to adapt it to feed the classifier using the Nearest Neighbor Interpolation (NNI). Second, we train, validate, and test a Convolutional Neural Network (CNN) to perform the image classification. Finally, we compare each classification result classes to a neighborhood using Euclidean distance. Numerical results on the VisDrone dataset demonstrate the efficiency of our proposal under a set of metrics.
ER  - 


TY  - CONF
TI  - Remote Sensing Image Captioning Using Deep Learning
T2  - 2024 International Conference on Automation and Computation (AUTOCOM)
SP  - 295
EP  - 302
AU  - B. Yamani
AU  - N. Medavarapu
AU  - S. Rakesh
PY  - 2024
KW  - Deep learning
KW  - Recurrent neural networks
KW  - Satellites
KW  - Computational modeling
KW  - Semantics
KW  - Transformers
KW  - Convolutional neural networks
KW  - Remote Sensing Images
KW  - Image Captioning
KW  - Encoder-Decoder Frameworks
KW  - Deep Learning Techniques
KW  - Convolutional Neural Network (CNN)
KW  - Transformers
KW  - Recurrent Neural Network (RNN)
DO  - 10.1109/AUTOCOM60220.2024.10486178
JO  - 2024 International Conference on Automation and Computation (AUTOCOM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Automation and Computation (AUTOCOM)
Y1  - 14-16 March 2024
AB  - Image captioning is a complex task that involves using deep learning techniques to automatically generate sentences describing the images. This idea is expanded upon by remote sensing image captioning, which applies to images obtained from high altitudes, such as those captured by satellites, aircraft, or drones. In the realm of remote sensing image captioning, the prevailing approach predominantly employs encoder-decoder frameworks. In this framework, the image input is encoded by a convolutional neural network (CNN), and a recurrent neural network (RNN) deciphers this encoded information into coherent sentence descriptions. However, recent times have witnessed the emergence of advanced models and solutions that leverage deep learning techniques to enhance performance. To gain a comprehensive understanding of these solutions, extensive research has been conducted, delving into reputable journal publications. This study conducts a thorough examination of the numerous proposed solutions. It delves into their methodologies, meticulously assesses their strengths and weaknesses, and conducts comparative analyses. Through this research, our objective is to provide a thorough review of the subject of remote sensing image captioning, shedding light on the latest developments and their potential implications.
ER  - 


TY  - CONF
TI  - On the efficiency of OLS reduced probabilistic neural networks for aircraft-flare discrimination
T2  - Proceedings of the International Joint Conference on Neural Networks, 2003.
SP  - 2306
EP  - 2311 vol.3
AU  - G. Labonte
PY  - 2003
KW  - Neural networks
KW  - Decision making
KW  - Real time systems
KW  - Infrared imaging
KW  - Target tracking
KW  - Mathematics
KW  - Computer science
KW  - Military aircraft
KW  - Military computing
KW  - Educational institutions
DO  - 10.1109/IJCNN.2003.1223771
JO  - Proceedings of the International Joint Conference on Neural Networks, 2003.
IS  - 
SN  - 1098-7576
VO  - 3
VL  - 3
JA  - Proceedings of the International Joint Conference on Neural Networks, 2003.
Y1  - 20-24 July 2003
AB  - Probabilistic neural networks (PNN) are the instruments of choice when it comes to critical decision making. Indeed, their output is not a simple yes-or-no decision; they are able to produce the probability that the features received as their input correspond to an object of any one of many classes. In work reported elsewhere, we have devised such a network for the discrimination of aircrafts from their decoy flares. It is very efficient, consistently exhibiting a recognition success rate of the order of 98-99%. However, because these neural networks are based on the Parzen-windows method, they must contain a very large number of neurons in order to be efficient. This can represent a serious disadvantage when they are to be incorporated in a real time system. It is thus advantageous to be able to reduce their size, without affecting appreciably their performance. We report in this article on the success we have had with adapting and applying an Orthogonal Least Squares (OLS) reduction method to the probabilistic neural network we built previously. We show that this method allows for a reduction of the number of neurons by as much as 81.9% with a decrease in performance of only 0.6%. Even a drastic reduction of 97.7% of number of neurons still produces a network with a 93.5% success rate. A side benefit of the application of this method to PNNs, is an ordered list of the images that the neural network considers as the best representatives of their class of objects.
ER  - 


TY  - CONF
TI  - Impact of Camera Vibration Frequencies on Image Noise for Unmanned Aerial System Applications
T2  - SoutheastCon 2018
SP  - 1
EP  - 5
AU  - W. -H. Huang
AU  - L. Daniel Otero
AU  - C. E. Otero
AU  - M. Moyou
PY  - 2018
KW  - Vibrations
KW  - Cameras
KW  - PSNR
KW  - Acceleration
KW  - Transportation
KW  - Noise level
KW  - Noise measurement
KW  - Image noise
KW  - Unmanned aerial systems
KW  - Structural inspections
KW  - Image processing
DO  - 10.1109/SECON.2018.8479208
JO  - SoutheastCon 2018
IS  - 
SN  - 1558-058X
VO  - 
VL  - 
JA  - SoutheastCon 2018
Y1  - 19-22 April 2018
AB  - In the transportation systems engineering field, there is a fast growing interest to use small unmanned aerial systems (UAS) for structural inspections. Of particular importance is the development of image processing algorithms to process UAS collected data for transportation infrastructure (e.g., bridges) defect detection/classification purposes. Results from preliminary experiments using image data collected with a UAS have shown the presence of significant image noise, which negatively impacts the efficiency of algorithms to detect/classify structural defects. This research study presents an ongoing effort to understand the relationship between UAS camera vibrations to image noise. Ultimately, the objective is to identify maximum camera vibration frequencies that will result in acceptable image noise levels for effective detection and classification of defects.
ER  - 


TY  - CONF
TI  - Real-time hyperspectral image cube compression combining adaptive classification and partial transform coding
T2  - 2006 8th international Conference on Signal Processing
SP  - 1
EP  - 
AU  - Z. Zhou
AU  - J. Liu
AU  - J. Tian
PY  - 2006
KW  - Hyperspectral imaging
KW  - Image coding
KW  - Transform coding
KW  - Hyperspectral sensors
KW  - Decorrelation
KW  - Principal component analysis
KW  - Independent component analysis
KW  - Satellites
KW  - Remote sensing
KW  - Karhunen-Loeve transforms
DO  - 10.1109/ICOSP.2006.345561
JO  - 2006 8th international Conference on Signal Processing
IS  - 
SN  - 2164-523X
VO  - 2
VL  - 2
JA  - 2006 8th international Conference on Signal Processing
Y1  - 16-20 Nov. 2006
AB  - In using partial transform for the coding of hyperspectral image, it must consider spectral decorrelation between image components, issues that a combined adaptive classification and partial transform algorithm for hyperspectral image compression is presented in this paper. Our method uses a linear prediction based on adaptive classification to decorrelate the spectrum redundancy and a 2D integer reversible DCT-based scheme as spatial compression engine. The classification includes band ordering, band regrouping and reference frame selection. Experimental results on AVIRIS data indicate that the proposed approach is a novel low complexity lossy hyperspectral image compression scheme and exhibits performance better than other comparative methods in quality and fidelity. It can be used for hyperspectral image compression in the embedded processor of the platform on satellite or aircraft
ER  - 


TY  - CONF
TI  - Improvised Power Line Classification Method by Merging the Selective Features of Pre-Trained CNN
T2  - 2019 Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
SP  - 634
EP  - 643
AU  - E. Kavinkartik
AU  - D. D. Venna
AU  - K. Hari Hara Suthan
AU  - G. Suguna
PY  - 2019
KW  - Feature extraction
KW  - Machine learning
KW  - Discrete cosine transforms
KW  - Decision trees
KW  - Training
KW  - Conferences
KW  - Forestry
KW  - Pre trained CNN
KW  - resnet34
KW  - vgg16
KW  - features
KW  - deep learning
DO  - 10.1109/I-SMAC47947.2019.9032636
JO  - 2019 Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
Y1  - 12-14 Dec. 2019
AB  - Power line detection has become a prominent measure to ensure aircraft safety. Various methods have already been implemented and embedded in the flight security system for the detection of the presence of power lines on route. In this paper, we propose a detection method using deep learning techniques. Publicly available dataset for power lines that includes separate set of infrared and visible light images are used. Performance of the machine learning model with selective pre-processing and DCT technique are experimented after which we perform transfer learning and fine-tuning on the pre-trained Convolutional Neural Network models, Resnet34, and VGG16 which provided us with great results especially with the IR images. We designed a model which extracts the features from each of the previously studied models and merges all the features extracted from each model before classification, thus by merging the features before classification we achieved very high testing accuracy for both IR and visible light images. The number of convolutional layers in the pre-trained models was modified based on our system constraints but by concentrating on improving the output accuracy of the trained models. From the analysis, it's understood that the deep learning model performs well compared to the machine learning model. Deep learning networks being dense with many layers and having the ability to extract imperceptible features efficiently has outshone with maximum amount of accuracy in our methods.
ER  - 


TY  - CONF
TI  - An overview on Change Detection and a Case Study Using Multi-temporal Satellite Imagery
T2  - 2019 International Conference on Computational Intelligence in Data Science (ICCIDS)
SP  - 1
EP  - 6
AU  - N. Anusha
AU  - B. Bharathi
PY  - 2019
KW  - Remote sensing
KW  - Earth
KW  - Artificial satellites
KW  - Satellites
KW  - Urban areas
KW  - Image segmentation
KW  - Buildings
KW  - Change detection
KW  - Remote sensing
KW  - Urbanization
KW  - Region of Interest (ROI)
KW  - Filtering
DO  - 10.1109/ICCIDS.2019.8862160
JO  - 2019 International Conference on Computational Intelligence in Data Science (ICCIDS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 International Conference on Computational Intelligence in Data Science (ICCIDS)
Y1  - 21-23 Feb. 2019
AB  - Satellite imagery based change detection plays an important role in analyzing the after effects of natural disasters, detecting the changes in city limits due to rapid urbanization, updating the map database, monitoring the factors impacting agriculture, etc., The remote sensors mounted on satellites or aircrafts absorb the light reflected by the earth's surface. The output of these sensors will be a digital image which represents the scene being perceived. In order to extract the useful information from these images, various image processing techniques need to be employed. In this paper, a detailed outline of the steps and various techniques used for detecting the changes in multi temporal remote sensing images is discussed and a case study is done by taking multi-temporal Landsat-8 images covering Hyderabad city. Image differencing method is applied in order to find the changes in the Hyderabad city limits over 2013December and 2017 December time periods.
ER  - 


TY  - CONF
TI  - Critical Zone Recognition: Classification vs. regression
T2  - 2014 International Conference on Prognostics and Health Management
SP  - 1
EP  - 5
AU  - Z. Bluvband
AU  - S. Porotsky
AU  - S. Tropper
PY  - 2014
KW  - Magnetic resonance imaging
KW  - Erbium
KW  - Noise measurement
KW  - Hafnium
KW  - Kuiper belt
KW  - Cross-Entropy
KW  - Cross-Validation
KW  - Prognostics
KW  - Remaining Useful Life
KW  - RUL Estimation
KW  - SVC
KW  - SVR
DO  - 10.1109/ICPHM.2014.7036386
JO  - 2014 International Conference on Prognostics and Health Management
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2014 International Conference on Prognostics and Health Management
Y1  - 22-25 June 2014
AB  - The article describes the Classification and Regression procedures, developed and successfully used for Critical Zone Recognition. One of the main tasks of the Prognostics and Health Management is the Failure Prognostics, specifically to provide predictive information regarding Remaining Useful Life (RUL) of device using prognostic systems. But sometimes it is necessary to get inflexible answer for closed type question: Is current device within critical zone or not? In other words, is RUL of device less than pre-defined Critical Value or not? To solve this problem, two approaches may be considered: · Regression Approach: to predict RUL value and compare results with critical value · Classification Approach: to recognize directly entering the critical zone In general, Classification Approach is more preferred for recognition tasks, but some aspects of the approach prevent to get an evident answer. Two models, based on modifications of the SVM method - SVC (Support Vector Classification) and SVR (Support Vector Regression) are proposed for consideration. Suggested methodology and algorithms were verified on the NASA Aircraft Engine database (http://ti.arc.nasa.gov/tech/dash/pcoe/prognostic-data-repository/). Numerical examples, based on this database, have been also considered.
ER  - 


TY  - JOUR
TI  - An analytical and experimental study of the performance of Markov random fields applied to textured images using small samples
T2  - IEEE Transactions on Image Processing
SP  - 447
EP  - 458
AU  - A. Speis
AU  - G. Healey
PY  - 1996
KW  - Performance analysis
KW  - Markov random fields
KW  - Autoregressive processes
KW  - Image coding
KW  - Maximum likelihood estimation
KW  - Image texture analysis
KW  - Least squares approximation
KW  - Surface texture
KW  - Image analysis
KW  - Image processing
DO  - 10.1109/83.491318
JO  - IEEE Transactions on Image Processing
IS  - 3
SN  - 1941-0042
VO  - 5
VL  - 5
JA  - IEEE Transactions on Image Processing
Y1  - March 1996
AB  - We investigate to what extent textures can be distinguished using conditional Markov fields and small samples. We establish that the least square (LS) estimator is the only reasonable choice for this task, and we prove its asymptotic consistency and normality for a general class of random fields that includes Gaussian Markov fields as a special case. The performance of this estimator when applied to textured images of real surfaces is poor if small boxes are used (20/spl times/20 or less). We investigate the nature of this problem by comparing the behavior predicted by the rigorous theory to the one that has been experimentally observed. Our analysis reveals that 20/spl times/20 samples contain enough information to distinguish between the textures in our experiments and that the poor performance mentioned above should be attributed to the fact that conditional Markov fields do not provide accurate models for textured images of many real surfaces. A more general model that exploits more efficiently the information contained in small samples is also suggested.
ER  - 


TY  - CONF
TI  - An improved particle filter method on unmanned multi-rotor aircraft platform
T2  - 2016 5th International Conference on Computer Science and Network Technology (ICCSNT)
SP  - 842
EP  - 845
AU  - W. Gang
AU  - Z. Xiaoqin
PY  - 2016
KW  - Object tracking
KW  - Machine learning
KW  - Computer science
KW  - Encoding
KW  - Particle filters
KW  - Particle filter
KW  - object tracking
KW  - dictionary learning
KW  - sparse representation
DO  - 10.1109/ICCSNT.2016.8070278
JO  - 2016 5th International Conference on Computer Science and Network Technology (ICCSNT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 5th International Conference on Computer Science and Network Technology (ICCSNT)
Y1  - 10-11 Dec. 2016
AB  - Aiming at difficulties for vehicle tracking on unmanned multi-rotor aircraft platform, an improved particle filter method is proposed in this project. Vehicle tracking process is posed as binary classification issue, where the utmost priority is to distinguish foreground vehicle region from the background image. In the preprocessing stage, target images are collected as positive samples, and background images as negative samples. Compared with raw image features, it is easier to separate the vehicle region from the background image with low-rank sparse and dictionary learning. Possible appearance variations of the target vehicle are captured by the specific templates in the dictionary set. A small number of these templates are required to reliably represent the observation of each candidate particle. The improved algorithm is applied to track selected vehicle in the urban road, demonstrate the performance of our method on the process of vehicle tracking.
ER  - 


TY  - CONF
TI  - Application of the polarimetric matched image filter to the assessment of SAR data from the Mississippi flood region
T2  - Proceedings of IGARSS '94 - 1994 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1368
EP  - 1370 vol.3
AU  - J. G. Teti
AU  - F. J. Ilsemann
AU  - J. S. Verdi
AU  - W. . -M. Boerner
AU  - S. K. Krasznay
PY  - 1994
KW  - Matched filters
KW  - Levee
KW  - Floods
KW  - Radar scattering
KW  - Synthetic aperture radar
KW  - Sediments
KW  - L-band
KW  - Radiometry
KW  - Image segmentation
KW  - Data mining
DO  - 10.1109/IGARSS.1994.399441
JO  - Proceedings of IGARSS '94 - 1994 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - Proceedings of IGARSS '94 - 1994 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 8-12 Aug. 1994
AB  - The optimal use of polarimetric scattering information provided by multiband synthetic aperture radar (SAR) is examined for its utility to improve the assessment of damage caused by the Mississippi flood of 1993. This natural flood disaster submerged or saturated large geographical regions including extensive farmland, and deposited large amounts of sandy sediment. A polarimetric matched image filter technique based on eigenanalysis is applied to enhance image contrast in selected regions for improved detection and classification of flooded regions. A subset of L-Band SAR data is first radiometrically and polarimetrically calibrated, and then coarsely segmented for the extraction of spatial statistics useful for filter application. The polarimetric matched image filter is applied to increase the contrast between dry and flooded land regions that may include stressed embankments (dikes) and levees, while also reducing speckle. The processing procedure is described, and the potential utility of the results is examined. The results are compared to the span image and verified with ground truth information.<>
ER  - 


TY  - JOUR
TI  - One-Dimensional Frequency-Domain Features for Aircraft Recognition from Radar Range Profiles
T2  - IEEE Transactions on Aerospace and Electronic Systems
SP  - 1880
EP  - 1892
AU  - Z. Guo
AU  - S. Li
PY  - 2010
KW  - Aircraft
KW  - Aerospace electronics
KW  - Target recognition
KW  - Aircraft propulsion
KW  - Airborne radar
KW  - Radar imaging
DO  - 10.1109/TAES.2010.5595601
JO  - IEEE Transactions on Aerospace and Electronic Systems
IS  - 4
SN  - 1557-9603
VO  - 46
VL  - 46
JA  - IEEE Transactions on Aerospace and Electronic Systems
Y1  - Oct. 2010
AB  - To extract effective one-dimensional frequency-domain features from high-resolution radar range profiles, the differential power spectrum (DPS) and the product spectrum, which were originally proposed for the speech signal processing, are introduced to the radar target recognition community. Through differentiating the power spectrum with respect to frequency, we obtained the DPS, which is translation invariant. The DPS can preserve the spectral information contained in the range profiles. The product spectrum is defined as the product of the power spectrum and the group delay function. Thus, it can combine the information contained in the magnitude spectrum and phase spectrum of the range profiles and then carry more details about the shape of the aircrafts. In the classification phase, an optimal choice can be determined by implementing six different training algorithms of multilayered feed-forward neural network. The range profiles were measured by using the two-dimensional backscatters distribution data of four different scaled aircraft models. Simulations were demonstrated to evaluate the classification performance with the DPS and the product spectrum-based features. The simulation results have shown that both DPS and product spectrum-based features are effective for the automatic target recognition (ATR) of aircrafts.
ER  - 


TY  - CONF
TI  - Real-time scene understanding for UAV imagery based on deep convolutional neural networks
T2  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
SP  - 2243
EP  - 2246
AU  - C. Sheppard
AU  - M. Rahnemoonfar
PY  - 2017
KW  - Economic indicators
DO  - 10.1109/IGARSS.2017.8127435
JO  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
Y1  - 23-28 July 2017
AB  - Real-time scene understanding is important for many applications of Unmanned Aerial Vehicles (UAVs) such as reconnaissance, surveillance, mapping, and infrastructure inspection. With the recent growth of computation power, it is feasible to use Deep Learning for real-time applications. Deep Convolutional Neural Networks (CNNs) have emerged as a powerful model for classifying image content, and are widely considered in the computer vision community to be the de facto standard approach for most problems. Current Deep learning approaches for image classification and object detection are designed and evaluated on lab setting human-centric photographs taken horizontally from a height of 1-2 meters. UAV images are taken vertically in high altitude; therefore the objects of interest are relatively small with a skewed vantage point which creates a real challenge in detection and classification of such images. Here we present a deep convolutional approach for classification of Aerial imagery taken by UAV. We applied our network on optical imagery taken with UAV RS-16 from Port Mansfield, TX. Experimental results in comparison with ground-truth show 93.6 % accuracy for UAV image classification.
ER  - 


TY  - CONF
TI  - Unsupervised multiscale segmentation of multispectral imagery
T2  - [1992] Proceedings of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis
SP  - 547
EP  - 550
AU  - R. A. Fernandes
AU  - M. E. Jernigan
PY  - 1992
KW  - Image segmentation
KW  - Multispectral imaging
KW  - Spatial resolution
KW  - Smoothing methods
KW  - Image resolution
KW  - Forestry
KW  - Aircraft
KW  - Aggregates
KW  - Frequency
KW  - Labeling
DO  - 10.1109/TFTSA.1992.274120
JO  - [1992] Proceedings of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis
IS  - 
SN  - 
VO  - 
VL  - 
JA  - [1992] Proceedings of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis
Y1  - 4-6 Oct. 1992
AB  - A method for segmenting high resolution multispectral forestry images acquired from aircraft is described. This method makes use of a hierarchical smoothing network to aggregate pixels. The aggregation process is guided by a nonorthogonal multiscale spatial/spatial frequency texture representation. Texture and spectral similarity measures between and within network levels are used to inhibit smoothing between land cover classes at five different resolutions. Segmentation performance is evaluated in terms of classification accuracy using independent and dependent samples for labeling emergent classes. The hypothesis that the accuracy of the network as it approaches steady state drops when interlayer connections are eliminated or when the texture information is removed is supported. The hypothesis that the segmentation network is more accurate than fuzzy clustering and unsupervised segmentation is verified.<>
ER  - 


TY  - CONF
TI  - Bryza-1RM/Bis — A multimission polish navy plane with SAR sensor dedicated to sea and ground monitoring
T2  - 2011 3rd International Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
SP  - 1
EP  - 4
AU  - G. Andrzej
AU  - J. Anna
AU  - S. Maciej
AU  - K. Krzysztof
AU  - M. Mateusz
AU  - M. Jacek
AU  - S. Piotr
PY  - 2011
KW  - Radar imaging
KW  - Synthetic aperture radar
KW  - Image color analysis
KW  - Radar detection
KW  - Imaging
KW  - History
KW  - SAR
KW  - real-time SAR imaging
KW  - ISAR
KW  - survailance radar
DO  - 
JO  - 2011 3rd International Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2011 3rd International Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
Y1  - 26-30 Sept. 2011
AB  - The paper presents the results of real-life measurements using an ARS-800 SAR sensor installed on the maritime patrol aircraft BRYZA-1RM/Bis. In the present day there is a large need for small — and medium-sized reliable sensors for remote imaging of the area of interest. Among many others, medium and high resolution Synthetic Aperture Radars (SAR), working in different modes (SAR, ISAR and others) play a special role. The main task for such radars is to provide images of the Earth's surface, and detailed imaging of selected targets (e.g. ships, buildings, etc.).
ER  - 


TY  - CONF
TI  - Non co-operative air target identification using radar imagery: identification rate as a function of signal bandwidth
T2  - Record of the IEEE 2000 International Radar Conference [Cat. No. 00CH37037]
SP  - 305
EP  - 309
AU  - K. Rosenbach
AU  - J. Schiller
PY  - 2000
KW  - Radar imaging
KW  - Bandwidth
KW  - Radar tracking
KW  - Signal processing
KW  - Military aircraft
KW  - Image resolution
KW  - Radar scattering
KW  - Optical scattering
KW  - Airborne radar
KW  - Target tracking
DO  - 10.1109/RADAR.2000.851851
JO  - Record of the IEEE 2000 International Radar Conference [Cat. No. 00CH37037]
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Record of the IEEE 2000 International Radar Conference [Cat. No. 00CH37037]
Y1  - 12-12 May 2000
AB  - Non-cooperative identification of (air) targets is still an unsolved problem though being of high relevance in the context of reliable friend-foe identification. In the past many methods have been proposed and investigated, e.g., by using the infrared, acoustical, optical or radar signatures of the targets. Some of these methods are passive ones, having the advantage of not alerting the observed object, but with some other shortcomings such as very limited observation ranges or marginal resolutions. The approach presented in this paper is based on exploitation of radar images of aircraft. Considered are one-dimensional radar images (so-called high-range resolution profiles) and 2D ISAR images.
ER  - 


TY  - JOUR
TI  - Guided Zoom: Zooming into Network Evidence to Refine Fine-Grained Model Decisions
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 4196
EP  - 4202
AU  - S. A. Bargal
AU  - A. Zunino
AU  - V. Petsiuk
AU  - J. Zhang
AU  - K. Saenko
AU  - V. Murino
AU  - S. Sclaroff
PY  - 2021
KW  - Grounding
KW  - Training
KW  - Predictive models
KW  - Annotations
KW  - Location awareness
KW  - Correlation
KW  - Visualization
KW  - Explainable AI
KW  - grounding
KW  - saliency
KW  - fine-grained image classification
KW  - classification refinement
KW  - convolutional neural networks
DO  - 10.1109/TPAMI.2021.3054303
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
IS  - 11
SN  - 1939-3539
VO  - 43
VL  - 43
JA  - IEEE Transactions on Pattern Analysis and Machine Intelligence
Y1  - 1 Nov. 2021
AB  - In state-of-the-art deep single-label classification models, the top-$k$k $(k=2,3,4, \dots)$(k=2,3,4,⋯) accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top $k$k predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainability could be used to improve model performance. We do so by making sure the model has “the right reasons” for a prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top-$k$k predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model's classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets. Our code is available at https://github.com/andreazuna89/Guided-Zoom.
ER  - 


TY  - CONF
TI  - Shape feature extraction from object corners
T2  - Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation
SP  - 160
EP  - 165
AU  - K. K. Rao
AU  - R. Krishnan
PY  - 1994
KW  - Shape
KW  - Feature extraction
KW  - Data mining
KW  - Image edge detection
KW  - Aircraft
KW  - Computer vision
KW  - Target recognition
KW  - Detectors
KW  - Pixel
KW  - Libraries
DO  - 10.1109/IAI.1994.336665
JO  - Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation
Y1  - 21-24 April 1994
AB  - A method to extract shape features based on corners is described. Corners contain most of the shape information. Extraction of shape features which are invariant to scaling, rotation and translation is an important problem in computer vision and automatic target recognition systems. A Canny (1986) edge detector which is capable of producing single pixel wide edges is used for obtaining the contour from an image. Using this closed contour as input, the arch height function is computed at each point. The local maxima's correspond to the corner points in the shape. A set of efficient one dimensional moments which are invariant under rotation, translation and scale change is computed. These are the corresponding shape features. Classification is achieved by comparing the extracted features with the shape feature library. In order to validate the concept the following experiments were performed. Ten dissimilar aircrafts and ten similar aircrafts were used as inputs. Contour based moments performed better than the geometric moments in both the data sets. Rotation invariance of two very similar aircrafts showed that contour based moments performed better. The procedure described provides an elegant approach for extracting shape features. These features can also be used as inputs for training and recognizing shapes using neural networks.<>
ER  - 


TY  - CONF
TI  - Multi-Frequency Feature Enhancement for Multi-Granularity Visual Classification
T2  - 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)
SP  - 484
EP  - 489
AU  - M. Fu
AU  - Y. Zheng
AU  - D. Chang
AU  - W. Li
AU  - Z. Ma
PY  - 2023
KW  - Representation learning
KW  - Visualization
KW  - Image recognition
KW  - Convolution
KW  - Asia
KW  - Information processing
KW  - Feature extraction
DO  - 10.1109/APSIPAASC58517.2023.10317566
JO  - 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)
IS  - 
SN  - 2640-0103
VO  - 
VL  - 
JA  - 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)
Y1  - 31 Oct.-3 Nov. 2023
AB  - Multi-granularity visual classification is a challenging task derived from traditional image recognition. Previous methods commonly use the features from the final convolutional layer to perform multi-granularity visual classification. However, the features required for different granularity label classification are not consistent. The finer the granularity of the label, the more detailed the features are needed. So, the key to multi-granularity visual classification is to extract effective features for different granularity levels. Generally, the high-frequency parts of natural images usually encode detailed information, while low-frequency parts often encode global structures. Therefore, mapping the output features of convolutional layers into high-frequency and low-frequency parts may enhance feature learning with multi-granularity. In this paper, we decompose the output features from convolutional layers into high-frequency and low-frequency counterparts, and use the combinations of the high-frequency and low-frequency features from various convolutional layers to learn different granularity labels. We evaluate the proposed method on the three public available datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets. The experimental results demonstrate the effectiveness of the proposed method.
ER  - 


TY  - CONF
TI  - Unmanned Aircraft Swarm Detection Method Based on RF Features and Unsupervised Learning
T2  - 2023 3rd International Conference on Communication Technology and Information Technology (ICCTIT)
SP  - 117
EP  - 120
AU  - B. Gao
PY  - 2023
KW  - Radio frequency
KW  - Dimensionality reduction
KW  - Wavelet transforms
KW  - Clustering algorithms
KW  - Autonomous aerial vehicles
KW  - Feature extraction
KW  - Signal to noise ratio
KW  - Unmanned aircraft swarm
KW  - RF signals
KW  - Unsupervised learning
KW  - Dimensionality reduction processing
KW  - Cluster estimation
DO  - 10.1109/ICCTIT60726.2023.10435965
JO  - 2023 3rd International Conference on Communication Technology and Information Technology (ICCTIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Communication Technology and Information Technology (ICCTIT)
Y1  - 24-26 Nov. 2023
AB  - Aiming at the current problem that UAV swarms are susceptible to environmental interference during detection and classification based on image recognition, and that traditional signal recognition makes it difficult to accurately extract features and has poor real-time performance, a UAV swarm detection method based on RF features and unsupervised learning is proposed. Firstly, the detected UAV RF signal is processed by a combination of the wavelet transform, independent component analysis (ICA) on linear dimensionality reduction algorithm, and unified manifold approximation and projection (UMAP) of nonlinear dimensionality reduction algorithm to obtain the RF signal spectrum; then, based on X-means algorithm (X-means), the number of clusters is estimated and the detection and identification of UAV swarms are carried out; finally, the proposed method can effectively detect and identify UAV swarms, as shown by simulation verification. The method can effectively detect and recognize UAV swarms with a recognition accuracy of up to 95%, and the recognition accuracy reaches 90% when the signal-to-noise ratio (SNR) is greater than 20 dB. Compared with traditional methods such as convolutional neural network (CNN) and support vector machine (SVM), the recognition accuracy and robustness of the proposed method are improved by 5% and 30%, which provides a certain reference value for the detection and recognition of UAV clusters in flight test.
ER  - 


TY  - CONF
TI  - Classification of Textures Distorted by WaterWaves
T2  - 18th International Conference on Pattern Recognition (ICPR'06)
SP  - 421
EP  - 424
AU  - A. Donate
AU  - G. Dahme
AU  - E. Ribeiro
PY  - 2006
KW  - Optical surface waves
KW  - Surface waves
KW  - Optical refraction
KW  - Distortion measurement
KW  - Histograms
KW  - Cameras
KW  - Geometry
KW  - Data mining
KW  - Laboratories
KW  - Aircraft
DO  - 10.1109/ICPR.2006.371
JO  - 18th International Conference on Pattern Recognition (ICPR'06)
IS  - 
SN  - 1051-4651
VO  - 2
VL  - 2
JA  - 18th International Conference on Pattern Recognition (ICPR'06)
Y1  - 20-24 Aug. 2006
AB  - In this paper, we approach the novel problem of classifying images of underwater textures as observed from outside the water. Our main contribution is to combine a geometric distortion removal algorithm with a texture classification method to solve the problem of classifying images of submerged textures when the water is disturbed by waves. We show that by modeling the separate types of distortion, we can extract enough texture information to correctly classify textures using spatial statistical measurements on the texton representations. We evaluate our algorithm on both natural and artificial textures acquired in our laboratory. Results are promising and show the feasibility of our algorithm.
ER  - 


TY  - CONF
TI  - The HICLASS software system: a manufacturing expert system shell
T2  - [1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications
SP  - 256
EP  - 261
AU  - D. Liu
PY  - 1988
KW  - Software systems
KW  - Manufacturing
KW  - Expert systems
KW  - Application software
KW  - Aircraft propulsion
KW  - Aircraft manufacture
KW  - Data systems
KW  - Aerospace engineering
KW  - Data engineering
KW  - Design engineering
DO  - 10.1109/CAIA.1988.196112
JO  - [1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications
IS  - 
SN  - 
VO  - 
VL  - 
JA  - [1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications
Y1  - 14-18 March 1988
AB  - Several expert system applications are being used at the Hughes Aircraft Company's Electro-Optical and Data Systems Group to aid in the engineering-to-manufacturing cycle of circuit board design and production. The author focuses on the evolution of the Hughes Integrated Classification Software System and its applications. He describes the various versions of the system and the auxiliary support modules; these are a rule-driven graphics generator, a rule-driven file generator, and a route sheet generator.<>
ER  - 


TY  - CONF
TI  - Advances in vegetation management for power line corridor monitoring using aerial remote sensing techniques
T2  - 2010 1st International Conference on Applied Robotics for the Power Industry
SP  - 1
EP  - 6
AU  - Z. Li
AU  - R. Walker
AU  - R. Hayward
AU  - L. Mejias
PY  - 2010
KW  - Laser radar
KW  - Vegetation mapping
KW  - Remote sensing
KW  - Classification algorithms
KW  - Classification tree analysis
KW  - Monitoring
KW  - Image segmentation
KW  - Power Line Corridor Monitoring
KW  - Vegetation Management
KW  - Unmanned Aerial System (UAS)
KW  - Remote Sensing
KW  - LiDAR
KW  - Multi-spectral Imagery
KW  - Power Line Detection
KW  - Tree Crown Segmentation
KW  - Tree Species Classification
DO  - 10.1109/CARPI.2010.5624431
JO  - 2010 1st International Conference on Applied Robotics for the Power Industry
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2010 1st International Conference on Applied Robotics for the Power Industry
Y1  - 5-7 Oct. 2010
AB  - This paper presents a comprehensive discussion of vegetation management approaches in power line corridors based on aerial remote sensing techniques. We address three issues 1) strategies for risk management in power line corridors, 2) selection of suitable platforms and sensor suite for data collection and 3) the progress in automated data processing techniques for vegetation management. We present initial results from a series of experiments and, challenges and lessons learnt from our project.
ER  - 


TY  - CONF
TI  - LiDAR-guided analysis of airborne hyperspectral data
T2  - 2009 First Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing
SP  - 1
EP  - 4
AU  - K. O. Niemann
AU  - G. Frazer
AU  - R. Loos
AU  - F. Visintini
PY  - 2009
KW  - Hyperspectral imaging
KW  - Laser radar
KW  - Hyperspectral sensors
KW  - Spatial resolution
KW  - Spectroscopy
KW  - Surface topography
KW  - Aircraft
KW  - Optical imaging
KW  - Nanometers
KW  - Bandwidth
KW  - Hyperspectral
KW  - LiDAR
KW  - data fusion
DO  - 10.1109/WHISPERS.2009.5289029
JO  - 2009 First Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing
IS  - 
SN  - 2158-6276
VO  - 
VL  - 
JA  - 2009 First Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing
Y1  - 26-28 Aug. 2009
AB  - This paper describes a new framework to the collection and fusion of multisensor airborne LiDAR and hyperspectral data. We describe a data fusion philosophy that provides a spatially precise positioning of hyperspectral data based on discrete first and last return LiDAR data. Three dimensional objects defined by the LiDAR data are then used to sample optimal spectra for subsequent analysis. The sampled spectra retain their positioning metadata and so can be mapped back into geographic space for further analysis. While the paper presents this philosophy within the context of a species classification, other analytical analysis can be performed.
ER  - 


TY  - CONF
TI  - Pose Aware Fine-Grained Visual Classification Using Pose Experts
T2  - 2018 25th IEEE International Conference on Image Processing (ICIP)
SP  - 2381
EP  - 2385
AU  - K. Mahajan
AU  - T. Khurana
AU  - A. Chopra
AU  - I. Gupta
AU  - C. Arora
AU  - A. Rai
PY  - 2018
KW  - Footwear
KW  - Benchmark testing
KW  - Detectors
KW  - Visualization
KW  - Training
KW  - Feature extraction
KW  - Object recognition
KW  - Fine Grained Visual Classification
KW  - CNN Ensemble
KW  - Pose Experts
DO  - 10.1109/ICIP.2018.8451257
JO  - 2018 25th IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2018 25th IEEE International Conference on Image Processing (ICIP)
Y1  - 7-10 Oct. 2018
AB  - We focus on the problem of fine-grained visual classification (FGVC). We posit that unreasonable effectiveness of the state-of-the-art in this area is because of similar object categories present in the ImageNet dataset, which allows such models to be pretrained on a much larger set of samples and learn generic features for those object categories. We observe an important and often ignored additional structure present in an FGVC problem: the objects are captured from a small set of viewing angles only. We notice that subtle differences between object categories are difficult to pick from an arbitrary angle but easier to identify from a similar pose. We show in this paper that training specialized pose experts, focusing on classification from a single, fixed pose, and combining them in an ensemble style framework successfully exploits the structure in the problem. We demonstrate the effectiveness of the proposed approach on the benchmark Stanford Cars, FGVC-Aircrafts, and DeepFashion datasets. To highlight the contribution when the target category features may not be available in a pretrained network, we test on footwear class. We contribute a new 1000 object, 12 category footwear dataset, each object captured from 4 different poses and show significant improvement on this dataset.
ER  - 


TY  - CONF
TI  - Classification of Drones in Disaster Management
T2  - 2024 IEEE 7th International Conference and Workshop Óbuda on Electrical and Power Engineering (CANDO-EPE)
SP  - 301
EP  - 306
AU  - P. M. Hell
AU  - P. J. Varga
PY  - 2024
KW  - Power engineering
KW  - Disasters
KW  - Imaging
KW  - Disaster management
KW  - Standardization
KW  - Thermal management
KW  - Drones
KW  - drones
KW  - disaster management
KW  - classification
DO  - 10.1109/CANDO-EPE65072.2024.10772960
JO  - 2024 IEEE 7th International Conference and Workshop Óbuda on Electrical and Power Engineering (CANDO-EPE)
IS  - 
SN  - 2831-4506
VO  - 
VL  - 
JA  - 2024 IEEE 7th International Conference and Workshop Óbuda on Electrical and Power Engineering (CANDO-EPE)
Y1  - 17-18 Oct. 2024
AB  - The application of drones in disaster management has received significant attention in recent years due to their capability to provide rapid and efficient support in handling various types of disasters. Aerial images, thermal imaging, and air composition data provided by drones significantly contribute to the effective performance of disaster management professionals. This article focuses on the classification and potential applications of drones in different phases of disaster management tasks. During the research, 32 named disasters and their relationship diagrams were analyzed, highlighting the relevance and efficiency of drone operations for various disaster types. The research findings indicate that the use of drones in the periods before, during, and after disasters offers significant advantages.
ER  - 


TY  - CONF
TI  - Non-cooperative classification of helicopters using millimetre wave radar and ISAR processing
T2  - 2008 Tyrrhenian International Workshop on Digital Communications - Enhanced Surveillance of Aircraft and Vehicles
SP  - 1
EP  - 6
AU  - H. Essen
AU  - M. Hagelen
AU  - A. Wahlen
AU  - K. . -H. Bers
AU  - M. Jager
AU  - M. Hebel
PY  - 2008
KW  - Radar
KW  - Helicopters
KW  - Scattering
KW  - Radar imaging
KW  - Radar tracking
KW  - Image resolution
KW  - Antenna measurements
KW  - Millimetre Wave Radar
KW  - ISAR Imaging
KW  - High Resolution
DO  - 10.1109/TIWDC.2008.4649040
JO  - 2008 Tyrrhenian International Workshop on Digital Communications - Enhanced Surveillance of Aircraft and Vehicles
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2008 Tyrrhenian International Workshop on Digital Communications - Enhanced Surveillance of Aircraft and Vehicles
Y1  - 3-5 Sept. 2008
AB  - Light weight and compact radars with high resolution capability can be built at millimeter wave frequencies. This has been demonstrated for a long period of time for missile seeker application for military use and as automotive radars in a civilian application. The technological advantages of this type of radar can be adapted to security applications in air traffic management at short and medium range as well as on the ground. The application discussed in this paper, focuses on the derivation of high resolution signatures of flying helicopters for non-cooperative classification schemes.
ER  - 


TY  - CONF
TI  - Automatic target recognition method based on polsar images with circular polarimetric basis conversion
T2  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
SP  - 3243
EP  - 3246
AU  - S. Ohno
AU  - S. Kidera
AU  - T. Kirimoto
PY  - 2015
KW  - Azimuth
KW  - Synthetic aperture radar
KW  - Target recognition
KW  - Polarization
KW  - Accuracy
KW  - Measurement
KW  - Aircraft
KW  - Synthetic aperture radar(SAR)
KW  - Polari-metric SAR
KW  - Automatic Target Recognition(ATR)
KW  - Circular Polarization Basis
DO  - 10.1109/IGARSS.2015.7326509
JO  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
Y1  - 26-31 July 2015
AB  - Satellite-borne or aircraft-borne synthetic aperture radar (SAR) technique is useful for high resolution imaging analysis for terrain surface monitoring or surveillance, even in optically harsh environment. For surveillance application, there are various approaches for automatic target recognition (ATR) of SAR images aiming at monitoring unidentified ships or aircrafts. In addition, various types of analyses using full polarimetric data have been developed recently because it can provide significant information to identify structure of targets, such as vegetation, urban, sea surface areas. In this paper, the circular polarization basis conversion is adopted to improve the robustness especially to variation of target rotation angles. The experimental data, assuming the 1/100 scale model of X-band radar, demonstrate that our proposed method significantly improves an accuracy of target area extraction and classification, even in noisy or angular fluctuated situations.
ER  - 


TY  - JOUR
TI  - A Curvature-Based Saliency Method for Ship Detection in SAR Images
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 1590
EP  - 1594
AU  - M. Yang
AU  - C. Guo
AU  - H. Zhong
AU  - H. Yin
PY  - 2021
KW  - Sensitivity
KW  - Optimization
KW  - Mathematical model
KW  - Taylor series
KW  - Probability distribution
KW  - Aircraft
KW  - Connectors
KW  - Curvature
KW  - Gamma manifold
KW  - information geometry
KW  - synthetic aperture radar (SAR) image
KW  - ship detection
DO  - 10.1109/LGRS.2020.3005197
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 9
SN  - 1558-0571
VO  - 18
VL  - 18
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - Sept. 2021
AB  - According to the theory and interpretation method of information geometry, this work presents a novel curvature-based saliency method for ship detection in synthetic aperture radar (SAR) images. First, the nonlinear anisotropic diffusive process has been adopted to eliminate clutter while preserving the local ship target structure in SAR images. Then, a novel curvature-based saliency method for super-pixels in the filtered image is presented, which is used to exploit the microstructure feature of statistical manifold. Finally, a statistical classification method is used to realize the location of targets. The experimental results on real SAR images show that the proposed method can achieve good performance.
ER  - 


TY  - CONF
TI  - Passive radar imaging capabilities using space-borne commercial illuminators in surveillance applications
T2  - 2015 Signal Processing Symposium (SPSympo)
SP  - 1
EP  - 5
AU  - J. . -L. Bárcena-Humanes
AU  - N. del-Rey-Maestre
AU  - M. P. Jarabo-Amores
AU  - D. Mata-Moya
AU  - P. Gomez-del-Hoyo
PY  - 2015
KW  - Satellites
KW  - Global Positioning System
KW  - Satellite broadcasting
KW  - Passive radar
KW  - Imaging
KW  - Trajectory
KW  - Lighting
DO  - 10.1109/SPS.2015.7168314
JO  - 2015 Signal Processing Symposium (SPSympo)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 Signal Processing Symposium (SPSympo)
Y1  - 10-12 June 2015
AB  - Space-borne commercial Opportunity Illuminators present interesting characteristics (global coverage and availability). An analysis of these illuminators' imaging capabilities has been carried out. Resolution requirements for classification purposes has been considered and the impact of the desired target size and dynamics studied. Small-medium ships, light aircrafts and UAVs have been considered in a maritime surveillance scenario. Geostationary communication satellites and Global Positioning Systems have been analysed, whose central frequency, signal bandwidth and orbits are quite different. Results show the feasibility of both illuminators for the considered application.
ER  - 


TY  - CONF
TI  - Through Thick and Thin: Imaging Through Obscurant using SPAD array
T2  - 2020 IEEE SENSORS
SP  - 1
EP  - 4
AU  - J. Mau
AU  - V. Devrelis
AU  - G. Day
AU  - G. Nash
AU  - J. Trumpf
AU  - D. Delic
PY  - 2020
KW  - Single-photon avalanche diodes
KW  - Laser radar
KW  - Photonics
KW  - Imaging
KW  - Measurement by laser beam
KW  - Histograms
KW  - Generators
DO  - 10.1109/SENSORS47125.2020.9278706
JO  - 2020 IEEE SENSORS
IS  - 
SN  - 2168-9229
VO  - 
VL  - 
JA  - 2020 IEEE SENSORS
Y1  - 25-28 Oct. 2020
AB  - Preliminary work on 3D image collection and classification of targets in the presence of obscurant using a Flash LiDAR system is discussed in this paper. The system is based around a DST designed 32 x 32 Single Photon Avalanche Diode (SPAD) array to image either targets or silhouettes of targets. The collected data included military targets that were obscured either by camouflage nets or fog. For camouflage net, the target was detected using an algorithm implemented on the Nvidia Jetson TX2. Targets obscured by fog are detected and classified where the classification accuracy is 100% for fog visibility down to 17.3m and 89.5% for 14.1m. This algorithm was not implemented on the TX2 but its simplicity shows potential for it in the future. This initial approach opens the road to eventually operate SPAD based systems for real-time classification through dust or smoke.
ER  - 


TY  - JOUR
TI  - K-Means Clustering Approach to UAS Classification via Graphical Signal Representation of Radio Frequency Signals for Air Traffic Early Warning
T2  - IEEE Transactions on Intelligent Transportation Systems
SP  - 24957
EP  - 24965
AU  - C. J. Swinney
AU  - J. C. Woods
PY  - 2022
KW  - Interference
KW  - Spectrogram
KW  - Wireless fidelity
KW  - Bluetooth
KW  - Signal representation
KW  - Costs
KW  - Transfer learning
KW  - Unmanned aerial systems
KW  - UAS
KW  - unmanned aerial vehicles
KW  - UAV
KW  - drones
KW  - detection
KW  - classification
KW  - security
DO  - 10.1109/TITS.2022.3202011
JO  - IEEE Transactions on Intelligent Transportation Systems
IS  - 12
SN  - 1558-0016
VO  - 23
VL  - 23
JA  - IEEE Transactions on Intelligent Transportation Systems
Y1  - Dec. 2022
AB  - Small Unmanned Aerial Systems (UASs) provide significant benefits to economies across the world on a daily basis but increased usage brings a number of security challenges. For example, identifying small UASs operating with malicious intent in a restricted airspace. Supervised learning techniques applied to radio frequency (RF) signals have been considered for the classification of UAS type with high accuracy but due to labelled data assume the UAS signal is already known. Unsupervised learning algorithms such as K-means clustering provide a potential for identifying small UAS signals which have not been seen before. The use of transfer learning and CNN feature extraction (FE) with spectrogram graphical signal representations have been successfully used in a supervised manner for medical diagnosis and audio classification. This research is the first application of transfer learning and CNN FE as a pre-cursor to an unsupervised learning algorithm. This paper shows that clustering graphical representations of the signal and utilising CNN FE with transfer learning produces the highest v-measure score 0.814 but at a cost of 6s in time. Small UASs can travel at speeds of 45 mph so timely detection is essential in many use cases. A decrease in 0.2 v-measure score using PSD graphical image representations of the RF signal and PCA initialisation allows the clustering time to complete in under 0.3s even in environments with active interference in the same band. This timely result could provide effective early warning with the cueing of a secondary sensor or supervised algorithm with higher classification accuracy.
ER  - 


TY  - CONF
TI  - Application of Histogram of Oriented Gradients and Support Vector Machine on Detection of Far-side Corrosion
T2  - 2022 International Conference on Advanced Technologies for Communications (ATC)
SP  - 69
EP  - 74
AU  - M. Le
AU  - V. S. Luong
AU  - D. K. Nguyen
AU  - T. T. Trinh
AU  - P. T. Vu
AU  - T. H. Nguyen
AU  - H. H. Thi Vu
AU  - J. Lee
PY  - 2022
KW  - Support vector machines
KW  - Histograms
KW  - Magnetic multilayers
KW  - Corrosion
KW  - Magnetic resonance imaging
KW  - Machine learning
KW  - Feature extraction
KW  - Far-side corrosion
KW  - Aircraft
KW  - Electromagnetic testing
KW  - HOG
KW  - SVM
DO  - 10.1109/ATC55345.2022.9943002
JO  - 2022 International Conference on Advanced Technologies for Communications (ATC)
IS  - 
SN  - 2162-1039
VO  - 
VL  - 
JA  - 2022 International Conference on Advanced Technologies for Communications (ATC)
Y1  - 20-22 Oct. 2022
AB  - Nondestructive testing (NDT) of far-side corrosion in a multilayer structure of aircraft is an important task to ensure the integrity and safety of the aircraft. Among the NDT methods, electromagnetic testing (ET) is powerful in detecting far-side corrosion. However, the far-side corrosion usually appears at the riveting site, making the ET signal complicated and challenging to recognize the presence of the small corrosion. In this paper, a histogram of gradients will be applied to extract features of the magnetic image, and a support vector machine will be used to detect the far-side corrosion. The proposed method helps to improve the accuracy of the detection significantly.
ER  - 


TY  - CONF
TI  - UAV Embedded Real-Time Object Detection by a DCNN Model Trained on Synthetic Dataset
T2  - 2023 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 580
EP  - 585
AU  - R. M. Bernardo
AU  - L. Claudio Batista da Silva
AU  - P. F. Ferreira Rosa
PY  - 2023
KW  - Training
KW  - Runtime
KW  - Surveillance
KW  - Object detection
KW  - Autonomous aerial vehicles
KW  - Cameras
KW  - Real-time systems
DO  - 10.1109/ICUAS57906.2023.10156134
JO  - 2023 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2023 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 6-9 June 2023
AB  - The utilization of unmanned aerial vehicles (UAVs) in civilian and military applications has significantly increased in recent years. A common task associated with these applications is detecting objects of interest in images captured by onboard UAV cameras. The ongoing development of advanced deep convolutional neural network (DCNN) algorithms has substantially improved the accuracy of general image segmentation and classification. However, applying these techniques to images obtained from UAVs requires a representative dataset for enhanced performance. This paper presents a method for DCNN-based object detection, utilizing resources embedded in a 1.5kg quadrotor-type UAV. To address the lack of representative datasets for our target scope, we employed a DCNN model trained on a self-generated synthetic dataset. The proposed method has been validated through real experiments, and the results demonstrate this approach’s feasibility for real-time surveillance with fully onboard processing. Furthermore, this offers a stand-alone, portable, and cost-effective solution for surveillance tasks using a small UAV.
ER  - 


TY  - CONF
TI  - Geospatial Object Detection Using Machine Learning-Aviation Case Study
T2  - 2021 Integrated Communications Navigation and Surveillance Conference (ICNS)
SP  - 1
EP  - 8
AU  - D. P. Dhulipudi
AU  - R. KS
PY  - 2021
KW  - Image sensors
KW  - Training
KW  - Visualization
KW  - Instruments
KW  - Surveillance
KW  - Sensor systems
KW  - Sensors
KW  - auto-land
KW  - air vehicle
KW  - UAV
KW  - UAS
KW  - Machine Learning
KW  - Computer Vision
KW  - landing
KW  - taxi
KW  - Runways
KW  - aerodrome surface markings
DO  - 10.1109/ICNS52807.2021.9441566
JO  - 2021 Integrated Communications Navigation and Surveillance Conference (ICNS)
IS  - 
SN  - 2155-4951
VO  - 
VL  - 
JA  - 2021 Integrated Communications Navigation and Surveillance Conference (ICNS)
Y1  - 19-23 April 2021
AB  - This paper presents the application of computer vision and machine learning to autonomous approach and landing and taxiing for an air vehicle. Recently, there has been growing interest in developing unmanned aircraft systems (UAS). We present a system and method that uses pattern recognition which aids the landing of a UAS and enhances the human-crewed air vehicle landing. Auto-landing systems based on the Instrument Landing System (ILS) have already proven their importance through decades. The auto-land systems work in conjunction with a radio altimeter, ILS, MLS, or GNSS. Closer to the runway, both under VFR and IFR, pilots are expected to rely on visual references for landing. Modern systems like HUD or CVS allow a trained pilot to manually fly the aircraft using guidance cues from the flight guidance system.Notwithstanding the type of landing and instruments used, typically, Pilots are expected to have the runway threshold markings, aiming point, displacement arrows, and touch down markings/lights insight before Minimum Decision Altitude (MDA). Imaging sensors are the essential standard equipment in crewed and crewless aerial vehicles that are widely used during the landing maneuver. In this method, a dataset of visual objects from satellite images is subjected to pattern recognition training. This trained system learns and then identifies and locates important visual references from imaging sensors and could help in landing and taxiing.
ER  - 


TY  - JOUR
TI  - A Semisupervised Aircraft Fuselage Defect Detection Network With Dynamic Attention and Class-Aware Adaptive Pseudolabel Assignment
T2  - IEEE Transactions on Artificial Intelligence
SP  - 3551
EP  - 3563
AU  - X. Zhang
AU  - J. Zhang
AU  - J. Chen
AU  - R. Guo
AU  - J. Wu
PY  - 2024
KW  - Aircraft
KW  - Defect detection
KW  - Training
KW  - Detectors
KW  - Object detection
KW  - Semisupervised learning
KW  - Defect detection
KW  - pseudolabel assignment
KW  - semisupervised learning
DO  - 10.1109/TAI.2024.3372474
JO  - IEEE Transactions on Artificial Intelligence
IS  - 7
SN  - 2691-4581
VO  - 5
VL  - 5
JA  - IEEE Transactions on Artificial Intelligence
Y1  - July 2024
AB  - To track the problem of aircraft fuselage defect detection in complex environments and reduce aviation safety hazards such as careless observation and delayed reporting due to objective factors, a semisupervised aircraft fuselage defect detection network was proposed. First, we constructed a new baseline model that extends one-stage detector with dynamic head and partial convolution named as dynamic decoupled detector, which enhances the representation capability of the model and improves the detection accuracy of small defects. Second, to address the issue of inconsistent pseudolabel distribution in semisupervised learning, we propose a class-aware adaptive pseudolabel assignment strategy that adaptively obtains the pseudolabel filtering threshold during the training iteration to further optimize the pseudolabel assignment process. Finally, to validate the effectiveness of the proposed model, we construct a dataset for aircraft fuselage defect detection for semisupervised training. Experimental results show that the proposed semisupervised aircraft fuselage defect detection network outperforms the current state-of-the-art semisupervised object detection framework on the aircraft fuselage defect dataset. At the same time, the proposed model has better generalization performance and provides more reliable support for real-time visualization of aircraft fuselage defects.
ER  - 


TY  - CONF
TI  - Object recognition by saccadic parts verification
T2  - Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)
SP  - 4247
EP  - 4252 vol.7
AU  - O. G. Jakubowicz
PY  - 1994
KW  - Object recognition
KW  - Lighting
KW  - Layout
KW  - Image recognition
KW  - Control systems
KW  - Pixel
KW  - Retina
KW  - Neural networks
KW  - Mathematical model
KW  - Computer interfaces
DO  - 10.1109/ICNN.1994.374948
JO  - Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)
IS  - 
SN  - 
VO  - 7
VL  - 7
JA  - Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)
Y1  - 28 June-2 July 1994
AB  - A model for the sequential processing of image 'parts' alias subobjects for the purpose of identifying visual objects is presented. The implementation uses three different coherently working neural networks to accomplish the task. One is for coarse resolution hypothesizing, one for verification via fine resolution subobject identification, and the third is a sequentially processing saccade generation net. The mathematical/architectural model is implemented on a UNIX computer with MOTIF interface. In addition a comprehensive set of positional, scale and rotational invariance (PSRI) conditions are tested using CCD camera inputs of real world objects. The data collected from experiments describe the system to have 100% PSRI for real photographs. Samples over a wide range of other real world conditions such as varied illumination and cluttered scenes are also correctly recognized.<>
ER  - 


TY  - CONF
TI  - A Novel Approach of Automatic Target Recognition using High Resolution Radar Range Profiles
T2  - 2007 International Symposium on Microwave, Antenna, Propagation and EMC Technologies for Wireless Communications
SP  - 852
EP  - 855
AU  - S. Luo
AU  - S. Li
PY  - 2007
KW  - Target recognition
KW  - Kernel
KW  - Aircraft
KW  - Radar imaging
KW  - Pattern recognition
KW  - Radar scattering
KW  - Radar antennas
KW  - Extraterrestrial measurements
KW  - Goniometers
KW  - Aerospace electronics
KW  - high resolution range profile
KW  - generalized information cut
KW  - Cauchy-schwarz inequality
KW  - automatic target recognition
DO  - 10.1109/MAPE.2007.4393760
JO  - 2007 International Symposium on Microwave, Antenna, Propagation and EMC Technologies for Wireless Communications
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2007 International Symposium on Microwave, Antenna, Propagation and EMC Technologies for Wireless Communications
Y1  - 16-17 Aug. 2007
AB  - Generalized information cut (GIC) based on the nonlinear feature mapping using the kernel function is invariant for the linear transformation and the coordinates shift. By comparison and analysis of the angle similarity measurement and generalized information cut, the criterion of generalized information cut is proposed to solve the automatic target recognition using high resolution radar range profile. Feature mapping and inner product is synthesized in the feature space. The range profile data established from four different scaled aircraft models in unclassifiable low dimensional space is mapped into classifiable high dimensional space via the kernel function, and then the linear classifiable pattern transformed from nonlinear unclassifiable pattern is accomplished. Experimental results suggest that the target recognition technique based on the proposed method achieves better anti-noise performance, and is much more efficient in the improvement of recognition ratio of four different aircrafts than using the conventional angle similarity coefficient (the vectorial angle cosine).
ER  - 


TY  - JOUR
TI  - Double-Attentive Principle Component Analysis
T2  - IEEE Signal Processing Letters
SP  - 1814
EP  - 1818
AU  - D. Wu
AU  - H. Zhang
AU  - F. Nie
AU  - R. Wang
AU  - C. Yang
AU  - X. Jia
AU  - X. Li
PY  - 2020
KW  - Principal component analysis
KW  - Analytical models
KW  - Image reconstruction
KW  - Signal processing algorithms
KW  - Convergence
KW  - Robustness
KW  - Principle component analysis
KW  - robust learning
KW  - attentive mechanism
KW  - image reconstruction
DO  - 10.1109/LSP.2020.3027462
JO  - IEEE Signal Processing Letters
IS  - 
SN  - 1558-2361
VO  - 27
VL  - 27
JA  - IEEE Signal Processing Letters
Y1  - 2020
AB  - This letter proposes a double-attentive principle component analysis (DA-PCA) model for image processing. Compared to the previous PCA-based works that cannot deal with normal images and outliers effectively, the proposed DA-PCA model performs a double-attentive mechanism to sever the connections with outliers and hold the effectiveness of normal images. To solve the proposed DA-PCA model, we propose an efficiently iterative algorithm and provide strict convergence analysis for it. Moreover, in the simulations, we conduct the reconstruction and classification experiments on several real datasets and the experimental results demonstrate the superb performance of our proposal.
ER  - 


TY  - CONF
TI  - Spatial pyramid co-occurrence for image classification
T2  - 2011 International Conference on Computer Vision
SP  - 1465
EP  - 1472
AU  - Yi Yang
AU  - S. Newsam
PY  - 2011
KW  - Visualization
KW  - Kernel
KW  - Histograms
KW  - Dictionaries
KW  - Spatial resolution
KW  - Feature extraction
DO  - 10.1109/ICCV.2011.6126403
JO  - 2011 International Conference on Computer Vision
IS  - 
SN  - 2380-7504
VO  - 
VL  - 
JA  - 2011 International Conference on Computer Vision
Y1  - 6-13 Nov. 2011
AB  - We describe a novel image representation termed spatial pyramid co-occurrence which characterizes both the photometric and geometric aspects of an image. Specifically, the co-occurrences of visual words are computed with respect to spatial predicates over a hierarchical spatial partitioning of an image. The representation captures both the absolute and relative spatial arrangement of the words and, through the choice and combination of the predicates, can characterize a variety of spatial relationships. Our representation is motivated by the analysis of overhead imagery such as from satellites or aircraft. This imagery generally does not have an absolute reference frame and thus the relative spatial arrangement of the image elements often becomes the key discriminating feature. We validate this hypothesis using a challenging ground truth image dataset of 21 land-use classes manually extracted from high-resolution aerial imagery. Our approach is shown to result in higher classification rates than a non-spatial bagof- visual-words approach as well as a popular approach for characterizing the absolute spatial arrangement of visual words, the spatial pyramid representation of Lazebnik et al. [7]. While our primary objective is analyzing overhead imagery, we demonstrate that our approach achieves state-of-the-art performance on the Graz-01 object class dataset and performs competitively on the 15 Scene dataset.
ER  - 


TY  - JOUR
TI  - Aircraft Skin Countersink Primitive Extraction From 3-D Measurement Point Clouds via Deep Clustering and Fitting
T2  - IEEE Transactions on Instrumentation and Measurement
SP  - 1
EP  - 11
AU  - M. Chen
AU  - L. Zhou
AU  - Y. Zhang
AU  - H. Chen
AU  - J. Wang
PY  - 2024
KW  - Image edge detection
KW  - Fitting
KW  - Aircraft
KW  - Point cloud compression
KW  - Skin
KW  - Inspection
KW  - Surface fitting
KW  - 3-D measurement point cloud
KW  - aircraft skin countersink
KW  - deep learning
KW  - primitive parameters extraction
KW  - similarity clustering (CLU)
KW  - weighted least squares
DO  - 10.1109/TIM.2024.3427820
JO  - IEEE Transactions on Instrumentation and Measurement
IS  - 
SN  - 1557-9662
VO  - 73
VL  - 73
JA  - IEEE Transactions on Instrumentation and Measurement
Y1  - 2024
AB  - To ensure the quality of aircraft assembly, precise 3-D inspection of countersinks in the aircraft skin is crucial. We propose CPE-Net, a multitask countersink primitive extraction network designed for this purpose. CPE-Net employs a 3-D point cloud deep learning network to predict countersink edge points, which are then clustered into paired large and small circles, effectively segmenting each countersink into two circular structures. To overcome the influence of measurement noise and sampling irregularity, we employ a learning-based weighted least squares method to adaptively fit circle parameters. Unlike conventional methods, CPE-Net co-trains the classification (CLA), clustering (CLU), and fitting (FIT) modules using a comprehensive loss function that incorporates edge detection error, CLU error, and circle FIT error. This holistic training approach enhances the quality of the extracted countersinks. The extracted countersink primitive parameters are utilized for geometry calculations, resulting in 3-D quality metric values. Our method undergoes testing on both virtual point cloud data and raw-scan data. Experimental results demonstrate the superior accuracy of our approach compared with the existing extraction methods. Furthermore, through a comparative analysis with detection results from contact measurement methods on practical test workpieces, our countersink extraction method showcases its capability and practicality to achieve precise quality inspection.
ER  - 


TY  - CONF
TI  - Aerial Target classification using micro-Doppler spectrum based on PCT and ResNet34
T2  - 2024 9th International Conference on Signal and Image Processing (ICSIP)
SP  - 575
EP  - 579
AU  - M. Zheng
AU  - S. Li
AU  - P. Huang
AU  - W. Li
AU  - B. Tian
AU  - S. Xu
PY  - 2024
KW  - Time-frequency analysis
KW  - Accuracy
KW  - Target recognition
KW  - Noise
KW  - Machine learning
KW  - Transforms
KW  - Robustness
KW  - RATR
KW  - micro-Doppler
KW  - PCT
KW  - ResNet
DO  - 10.1109/ICSIP61881.2024.10671406
JO  - 2024 9th International Conference on Signal and Image Processing (ICSIP)
IS  - 
SN  - 2642-6471
VO  - 
VL  - 
JA  - 2024 9th International Conference on Signal and Image Processing (ICSIP)
Y1  - 12-14 July 2024
AB  - The application of deep neural networks has dramatically improved the accuracy of Radar Automatic Target Recognition (RATR). In most involved studies, however, the recognition method based on micro-Doppler (m-D) features has not considered the effects of real noise and clutter,making it difficult to apply in paractical engineering. In this paper, a robust aerial classification method using m-D signatures based on Polynomial Chirplet Transform (PCT) and ResNet is proposed. The Doppler spectrum of the signal is extracted after PCT processing and then classified via ResNet Experiments show that PCT processing improves the differentiability of the Doppler spectrum, and the accuracy of PCT-ResNet is higher than that of ResNet and other machine learning methods.
ER  - 


TY  - JOUR
TI  - Complemental Attention Multi-Feature Fusion Network for Fine-Grained Classification
T2  - IEEE Signal Processing Letters
SP  - 1983
EP  - 1987
AU  - Z. Miao
AU  - X. Zhao
AU  - J. Wang
AU  - Y. Li
AU  - H. Li
PY  - 2021
KW  - Feature extraction
KW  - Transformers
KW  - Automobiles
KW  - Task analysis
KW  - Dogs
KW  - Training
KW  - Loss measurement
KW  - Attention
KW  - fine-grained classification
KW  - transformer
DO  - 10.1109/LSP.2021.3114622
JO  - IEEE Signal Processing Letters
IS  - 
SN  - 1558-2361
VO  - 28
VL  - 28
JA  - IEEE Signal Processing Letters
Y1  - 2021
AB  - Transformer-based architecture network has shown excellent performance in the coarse-grained image classification. However, it remains a challenge for the fine-grained image classification task, which needs more significant regional information. As one of the attention mechanisms, transformer pays attention to the most significant region while neglecting other sub-significant regions. To use more regional information, in this letter, we propose a complemental attention multi-feature fusion network (CAMF), which extracts multiple attention features to obtain more effective features. In CAMF, we propose two novel modules: (i) a complemental attention module (CAM) that extracts the most salient attention feature and the complemental attention feature. (ii) a multi-feature fusion module (MFM) that uses different branches to extract multiple regional discriminative features. Furthermore, a new feature similarity loss is proposed to measure the diversity of inter-class features. Experiments were conducted on four public fine-grained classification datasets. Our CAMF achieves 91.2%, 92.8%, 93.3%, 95.3% on CUB-200-2011, Stanford Dogs, FGVC-Aircraft, and Stanford Cars. The ablation study verified that CAM and MFM can focus on more local discriminative regions and improve fine-grained classification performance.
ER  - 


TY  - JOUR
TI  - Methodology of Detection and Classification of Selected Aviation Obstacles Based on UAV Dense Image Matching
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 1869
EP  - 1883
AU  - M. Lalak
AU  - D. Wierzbicki
PY  - 2022
KW  - Point cloud compression
KW  - Three-dimensional displays
KW  - Airports
KW  - Buildings
KW  - Aircraft
KW  - Safety
KW  - Feature extraction
KW  - Accuracy
KW  - air traffic control
KW  - image processing
KW  - remote sensing
KW  - unmanned aerial vehicles (UAVs)
DO  - 10.1109/JSTARS.2022.3149105
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 15
VL  - 15
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2022
AB  - Currently, more and more accurate data provided by UAVs make it possible to analyze land cover, which requires the detection of objects and their individual elements. Object detection and determination of their geometric features is possible thanks to dense point clouds generated based on imagery obtained from low altitudes. 3D data from UAVs turn out to be extremely useful for ensuring safety in the airspace in the close vicinity of the airport. This article presents the methodology of automatic aviation obstacle detection based on low altitude data (UAV). The research was carried out on a dense 3D point cloud. The developed methodology for detecting aviation obstacles consists of three main stages. The first is point cloud filtration based on height–preliminary identification of aviation obstacles, followed by 3D point cloud segmentation using a modified RANSAC algorithm, supplemented with two-dimensional vector data of aviation obstacles to improve the accuracy of the segmentation process. The last stage is the classification of aviation obstacles according to the adopted height and cross-section criterion. The proposed method of detecting aviation obstacles is characterized by high accuracy. The mean error of fitting the point cloud to the obstacle database ranged from ± 0.04 m to ± 0.07 m.
ER  - 


TY  - CONF
TI  - Improving classification boundaries by exemplar generation for visual pattern discrimination
T2  - IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)
SP  - 2969
EP  - 2974 vol.4
AU  - B. Kamgar-Parsi
AU  - B. Kamgar-Parsi
AU  - J. E. Dayhoff
AU  - A. K. Jain
PY  - 2001
KW  - Humans
KW  - Training data
KW  - Pattern recognition
KW  - Neural networks
KW  - Testing
KW  - Information technology
KW  - Laboratories
KW  - Silver
KW  - Springs
KW  - Computer science
DO  - 10.1109/IJCNN.2001.938850
JO  - IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)
IS  - 
SN  - 1098-7576
VO  - 4
VL  - 4
JA  - IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)
Y1  - 15-19 July 2001
AB  - In many applications for visual pattern discrimination, a major drawback is insufficient training data. Often the data contains too few example images, and those images are not distributed along the boundary between the alternative classifications. In this paper we present an approach that develops realistic synthetic data along the boundary between two different discrimination classes, where exemplars are needed the most. An application of this technique to a real life object recognition problem shows a performance comparable to that of a human expert, and far better than a network trained only on the available real data. Furthermore, the results are considerably better than those obtained using non-network discriminators such as Euclidean.
ER  - 


TY  - CONF
TI  - Objects Recognition Methods Estimation in Application to Runway Pictures Taken in Poor Visibility Conditions
T2  - 2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
SP  - 1141
EP  - 1145
AU  - D. S. Andreev
AU  - N. V. Lysenko
PY  - 2019
KW  - Object recognition
KW  - Aircraft
KW  - Image recognition
KW  - Support vector machines
KW  - Task analysis
KW  - Aerospace electronics
KW  - flight vision systems
KW  - objects recognition
KW  - classification
DO  - 10.1109/EIConRus.2019.8656649
JO  - 2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
IS  - 
SN  - 2376-6565
VO  - 
VL  - 
JA  - 2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
Y1  - 28-31 Jan. 2019
AB  - The paper covers the methods of objects recognition, which might be used in Enhanced Flight Vision Systems. Modern Flight Vision Systems designs focus on all-weather vision. Covered methods are estimated in application to runway pictures, taken in poor visibility conditions above and below the decision height, considering various characteristics. Results of this work are relevant to the tasks of avionics, computer vision and image processing.
ER  - 


TY  - JOUR
TI  - Digital image-processing activities in remote sensing for earth resources
T2  - Proceedings of the IEEE
SP  - 1177
EP  - 1200
AU  - G. Nagy
PY  - 1972
KW  - Digital images
KW  - Remote sensing
KW  - Earth
KW  - State-space methods
KW  - Moon
KW  - Planets
KW  - Space technology
KW  - Economic forecasting
KW  - Artificial satellites
KW  - Cameras
DO  - 10.1109/PROC.1972.8879
JO  - Proceedings of the IEEE
IS  - 10
SN  - 1558-2256
VO  - 60
VL  - 60
JA  - Proceedings of the IEEE
Y1  - Oct. 1972
AB  - The United States space program is in the throes of a major shift in emphasis from exploration of the moon and nearby planets to the application of remote sensing technology toward increased scientific understanding and economic exploitation of the earth itself. Over one hundred potential applications have already been identified. Since data from the unmanned Earth Resources Technology Satellites and the manned Earth Resources Observation Satellites are not yet available, the experimentation required to realize the ambitious goals of these projects is carried out through approximation of the expected characteristics of the data by means of images derived from weather satellite vidicon and spin-scan cameras, Gemini and Apollo photographs, and the comprehensive sensor complement of the NASA earth resources observation aircraft. The extensive and varied work currently underway is reviewed in terms of the special purpose scan and display equipment and efficient data manipulation routines required for high-resolution images; the essential role of interactive processing; the application of supervised classification methods to crop and timber forecasts, geological exploration, and hydrological surveys; the need for nonsupervised classification techniques for video compaction and for more efficient utilization of ground-control samples; and the outstanding problem of mapping accurately the collected data on a standard coordinate system. An attempt is made to identify among the welter of "promising" results areas of tangible achievement as well as likely bottlenecks, and to assess the contribution to be expected of digital image-processing methods in both operational and experimental utilization of the forthcoming torrent of data.
ER  - 


TY  - CONF
TI  - Weed Identification using K-Means Clustering with Color Spaces Features in Multi-Spectral Images Taken by UAV
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 7047
EP  - 7050
AU  - R. Agarwal
AU  - S. Hariharan
AU  - M. Nagabhushana Rao
AU  - A. Agarwal
PY  - 2021
KW  - Measurement
KW  - Image color analysis
KW  - Supervised learning
KW  - Sociology
KW  - Vegetation mapping
KW  - Production
KW  - Tools
KW  - Weed
KW  - Color spaces
KW  - UAV
KW  - Maize
KW  - K-means Clustering
KW  - NDVI
KW  - VARI
KW  - TGI
DO  - 10.1109/IGARSS47720.2021.9554097
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - Food security is a pertinent global challenge that has plagued the nations over time immemorial. Maize is one of the world's most significant consumption crop, based on production volume. Maize supply can vary according to the cultivation area, climatic condition, and disease. Modern precision weed management relies on site-specific management tactics to maximize resource use efficiency and yield, while reducing unintended environmental impacts caused by herbicides. Recent advancements in Unmanned Aircraft Systems (UAS)-based tools and geospatial information technology have created enormous applications for efficient and economical assessment of weed infestations as well as site-specific weed management. This paper explores the possibility of extracting features from color spaces and combining them with vegetation indices (NDVI, VARI, and TGI) to be clustered using K-means classifier to identify the weed population from a multispectral imagery. The results give a clear indication that the NDVI performance is better than VARI; It also shows that TGI is not acceptable for the classification.
ER  - 


TY  - CONF
TI  - Comparison of Time-Frequency Classification Methods for Intelligent Automatic Jettisoning Device of Helmet- Mounted Display Systems
T2  - 2007 IEEE/SP 14th Workshop on Statistical Signal Processing
SP  - 730
EP  - 734
AU  - H. F. Alqadah
AU  - H. H. Fan
AU  - J. A. Plaga
PY  - 2007
KW  - Time frequency analysis
KW  - Displays
KW  - Kernel
KW  - Computer crashes
KW  - Acceleration
KW  - Medical signal detection
KW  - Interference
KW  - Autocorrelation
KW  - Buffer storage
KW  - Laboratories
KW  - Time-Frequency Representation
KW  - Signal Classification
KW  - Wigner-Ville Distribution
KW  - Kernel
KW  - Pattern Recognition
DO  - 10.1109/SSP.2007.4301355
JO  - 2007 IEEE/SP 14th Workshop on Statistical Signal Processing
IS  - 
SN  - 2373-0803
VO  - 
VL  - 
JA  - 2007 IEEE/SP 14th Workshop on Statistical Signal Processing
Y1  - 26-29 Aug. 2007
AB  - Helmet-Mounted Display Systems (HMDS) improve the situational awareness of an air force pilot in combat; however, they can increase the probability of neck injury to a pilot during a crash or ejection due to their added weight and center of gravity shift. Attempts with simple mechanical force/acceleration release systems to release the HMDS during an ejection event have been unsatisfactory since helmet accelerations during normal air combat maneuvering (ACM) can be near peak accelerations seen during a crash or ejection. The HMDS acceleration responses indicated being non-stationary signals, which resulted in use of time-frequency classification methods to differentiate between the crash/ejection events and those of normal aircraft maneuvering. Parametric and non-parametric approaches for the optimization of the time-frequency representations (TFR) with the goal of classifications between these two environments were compared. The non-parametric approach was determined to be superior.
ER  - 


TY  - CONF
TI  - Transformer Enhanced YOLO Network for Ship Detection in Optical Remote Sensing Images
T2  - 2024 7th International Conference on Information Communication and Signal Processing (ICICSP)
SP  - 845
EP  - 852
AU  - D. Chen
AU  - R. Ju
AU  - X. Liu
AU  - J. Liu
PY  - 2024
KW  - YOLO
KW  - Measurement
KW  - Integrated optics
KW  - Optical fiber networks
KW  - Transformers
KW  - Optical imaging
KW  - Optical noise
KW  - Marine vehicles
KW  - Optical signal processing
KW  - Remote sensing
KW  - Optical remote sensing images
KW  - YOLO
KW  - Swin-Transformer
KW  - Wise-IoU
KW  - Ship detection
DO  - 10.1109/ICICSP62589.2024.10809310
JO  - 2024 7th International Conference on Information Communication and Signal Processing (ICICSP)
IS  - 
SN  - 2770-792X
VO  - 
VL  - 
JA  - 2024 7th International Conference on Information Communication and Signal Processing (ICICSP)
Y1  - 21-23 Sept. 2024
AB  - Ship detection in optical remote sensing images poses challenges such as complex scenes, large target scale differences, and imperfect fine-grained classification. Deep learning methods like YOLO have provided effective solutions. With emerging detection approaches like Transformers achieving new breakthroughs in accuracy, this study proposes an enhanced detection model by incorporating Transformer into YOLO network. The model replaces the Backbone of YOLOv5s with SwinTransformer-Tiny and refines the Loss function to Wise-IoU, tailored for remote sensing images and ship targets. Experimental verification on self-curated single-category dataset LEVIR-ship and multi-category dataset HRSC-4 shows that the comprehensive metric mAP50 values have increased from 93.8% and 96.2% to 95.2% and 96.5%, respectively. Both precision (P) and recall (R) values have significantly improved. There is also some enhancement in fine-grained classification metrics on the HRSC-4 dataset. The newly improved model demonstrates good performance.
ER  - 


TY  - JOUR
TI  - Generalized Ridge Regression-Based Channelwise Feature Map Weighted Reconstruction Network for Fine-Grained Few-Shot Ship Classification
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 10
AU  - Y. Li
AU  - C. Bian
AU  - H. Chen
PY  - 2023
KW  - Image reconstruction
KW  - Marine vehicles
KW  - Learning systems
KW  - Feature extraction
KW  - Task analysis
KW  - Aerospace electronics
KW  - Neural networks
KW  - Channelwise
KW  - feature map weighted reconstruction
KW  - few-shot learning
KW  - generalized ridge regression
KW  - ship classification
DO  - 10.1109/TGRS.2023.3235747
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 61
VL  - 61
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2023
AB  - Fine-grained ship classification (FGSCR) has many applications in military and civilian fields. In recent years, deep learning has been widely used for classification tasks, and its success is inseparable from that of big data. However, ship images are valuable, with only a few images of a specific category being obtained, leading to the fine-grained few-shot ship classification problem. In addition, feature map channels contain distinct characteristics and discriminative details, which significantly influence FGSCR. Intuitively, channels with distinct characteristics should be assigned larger weights for classification, but most few-shot learning methods treat the channels equally. Therefore, we propose a generalized ridge-regression-based channelwise feature map weighted reconstruction network to address these issues. First, we reconstruct the query feature map by assigning different weights to the support feature map channels using the generalized ridge regression method. The channels with large discriminative details contribute more toward reconstruction. Second, we propose a support channel weight module to calculate the channel weight matrix used in the generalized ridge regression method. Finally, based on the reconstructed query feature map, we can calculate the reconstruction error. The reconstruction error is adopted as the distance metric. Our proposed method achieves excellent performance on the fine-grained ship, bird, aircraft, and WHU-RS19 datasets compared with other representative few-shot learning methods. Considering the limited studies on the fine-grained few-shot ship classification problem, we believe that our work is of great significance.
ER  - 


TY  - CONF
TI  - Bistatic Image Processing for a Hybrid SAR Experiment Between TerraSAR-X and PAMIR
T2  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
SP  - 1934
EP  - 1937
AU  - I. Walterscheid
AU  - J. Ender
AU  - J. Klare
AU  - A. Brenner
AU  - O. Loffeld
PY  - 2006
KW  - Image processing
KW  - Spaceborne radar
KW  - Synthetic aperture radar
KW  - Satellites
KW  - Phased arrays
KW  - Transmitters
KW  - Layout
KW  - Frequency
KW  - Radar imaging
KW  - Airplanes
DO  - 10.1109/IGARSS.2006.500
JO  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2006 IEEE International Symposium on Geoscience and Remote Sensing
Y1  - 31 July-4 Aug. 2006
AB  - Future bi- and multistatic SAR systems could support the scientific community and the commercial market with an additional and powerful tool for imaging and exploration of interesting areas on earth. With the diversity of geometries between transmitter and receivers one can achieve for instance improvements in scene classification, extractions of particular features, and cost reduction. New experiments are necessary to investigate the advantages as well as the problems of bi- and multistatic SAR systems. This paper describes the bistatic use of the spaceborne SAR system TerraSAR-X for future space- borne/airborne SAR exploration. TerraSAR-X will illuminate a particular scene while the receiver, the airborne SAR system PAMIR, will collect the reflected signals on board of an aircraft.
ER  - 


TY  - CONF
TI  - Deep Learning-based approach for detection and classification of Micro/Mini drones
T2  - 2020 4th International Conference on Advanced Systems and Emergent Technologies (IC_ASET)
SP  - 332
EP  - 337
AU  - T. Delleji
AU  - H. Fekih
AU  - Z. Chtourou
PY  - 2020
KW  - Drones
KW  - Object detection
KW  - Detectors
KW  - Annotations
KW  - Training
KW  - Task analysis
KW  - Computational modeling
KW  - micro/mini drone detection
KW  - Deep Learning
KW  - classification
KW  - restricted area
DO  - 10.1109/IC_ASET49463.2020.9318281
JO  - 2020 4th International Conference on Advanced Systems and Emergent Technologies (IC_ASET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 4th International Conference on Advanced Systems and Emergent Technologies (IC_ASET)
Y1  - 15-18 Dec. 2020
AB  - In the recent years, the micro/mini drones' industry has witnessed an explosive growth, making these flying objects become highly accessible to Terrorist groups. This phenomenon has caused specific security concerns due to the fact that these suspicious flying gadgets can cause serious hazards. To protect the sensitive locations and restricted areas, we suggest, in this paper, a drone detection method that integrates deeplearning-based classification and localization tasks. Specially, we selected a family of fast and accurate one-stage object detector: YOLOv3. So, we use and improve YOLOv3 deep learning neural network, by upgrading its architecture and fine-tuning its parameters to better accommodate small object detection such as micro/mini drone. Furthermore, to train our algorithm to classify the detected drone, we have constructed a multi-class drone dataset consisting of drones' images that may fly in Tunisian airspace and among which some may be a possible threat.
ER  - 


TY  - CONF
TI  - Globally-scalable Automated Target Recognition (GATR)
T2  - 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
SP  - 1
EP  - 7
AU  - G. Chern
AU  - A. Groener
AU  - M. Harner
AU  - T. Kuhns
AU  - A. Lam
AU  - S. O’Neill
AU  - M. Pritt
PY  - 2019
KW  - Artificial intelligence (AI)
KW  - automatic target recognition (ATR)
KW  - classification
KW  - computer vision
KW  - deep learning
KW  - image interpretation
KW  - machine learning (ML)
KW  - neural networks
DO  - 10.1109/AIPR47015.2019.9174585
JO  - 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
IS  - 
SN  - 2332-5615
VO  - 
VL  - 
JA  - 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
Y1  - 15-17 Oct. 2019
AB  - GATR (Globally-scalable Automated Target Recognition) is a Lockheed Martin software system for real-time object detection and classification in satellite imagery on a worldwide basis. GATR uses GPU-accelerated deep learning software to quickly search large geographic regions. On a single GPU it processes imagery at a rate of over 16 km2/sec (or more than 10 Mpixels/sec), and it requires only two hours to search the entire state of Pennsylvania for gas fracking wells. The search time scales linearly with the geographic area, and the processing rate scales linearly with the number of GPUs. GATR has a modular, cloud-based architecture that uses Maxar's GBDX platform and provides an ATR analytic as a service. Applications include broad area search, watch boxes for monitoring ports and airfields, and site characterization. ATR is performed by deep learning models including RetinaNet and Faster R-CNN. Results are presented for the detection of aircraft and fracking wells and show that the recalls exceed 90% even in geographic regions never seen before. GATR is extensible to new targets, such as cars and ships, and it also handles radar and infrared imagery.
ER  - 


TY  - CONF
TI  - GRAD-CAM Guided Channel-Spatial Attention Module for Fine-Grained Visual Classification
T2  - 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP)
SP  - 1
EP  - 6
AU  - S. Xu
AU  - D. Chang
AU  - J. Xie
AU  - Z. Ma
PY  - 2021
KW  - Location awareness
KW  - Visualization
KW  - Annotations
KW  - Semantics
KW  - Machine learning
KW  - Detectors
KW  - Signal processing
KW  - Fine-grained visual classification
KW  - gradient-weighted class activation mapping
KW  - channel-spatial attention mechanism
DO  - 10.1109/MLSP52302.2021.9596481
JO  - 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP)
IS  - 
SN  - 1551-2541
VO  - 
VL  - 
JA  - 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP)
Y1  - 25-28 Oct. 2021
AB  - Fine-grained visual classification (FGVC) is becoming an important research field, due to its wide applications and the rapid development of computer vision technologies. The current state-of-the-art (SOTA) methods in the FGVC usually employ attention mechanisms to first capture the semantic parts and then discover their subtle differences between distinct classes. The existing attention modules have significantly improved the classification performance but they are poorly guided since part-based detectors in the FGVC depend on the network learning ability without the supervision of part annotations. As obtaining such part annotations is labor-expensive, some visual localization and explanation methods, such as gradient-weighted class activation mapping (Grad-CAM), can be utilized for supervising the attention mechanism. In this paper, we propose a Grad-CAM guided channel-spatial attention module for the FGVC, which employs the Grad-CAM to supervise and constrain the attention weights by generating the coarse localization maps. To demonstrate the effectiveness of the proposed method, we conduct comprehensive experiments on three popular FGVC datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets. The proposed method outperforms the SOTA attention modules in the FGVC task. In addition, visualizations of the feature maps demonstrate the superiority of the proposed method against the SOTA approaches.
ER  - 


TY  - CONF
TI  - Classification of Sea Ice Types with Radar
T2  - 1992 22nd European Microwave Conference
SP  - 957
EP  - 962
AU  - M. Hallikainen
AU  - M. Toikka
PY  - 1992
KW  - Sea ice
KW  - Spaceborne radar
KW  - Synthetic aperture radar
KW  - Radar imaging
KW  - Aircraft navigation
KW  - Satellite navigation systems
KW  - Marine vehicles
KW  - Radar measurements
KW  - Radar scattering
KW  - Image sensors
DO  - 10.1109/EUMA.1992.335828
JO  - 1992 22nd European Microwave Conference
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 1992 22nd European Microwave Conference
Y1  - 5-9 Sept. 1992
AB  - Using a helicopter-borne multichannel scatterometer (accurately calibrated non-imaging radar), the feasibility of using synthetic aperture radar (SAR) images from present and near-future spaceborne sensors for assisting winter navigation in the Baltic Sea was investigated. Due to cloud cover and short days in the winter, visible/infrared satellite data cannot be used for Baltic ice mapping. In order to decrease the operating cost of icebreakers, real-time radar-derived sea ice charts are needed to optimize the route selection of icebreakers assisting ships to Finnish and Swedish harbors. For ship navigation, information on (1) new ice and open water (preferred route), (2) level ice (second choice) and (3) ice ridges and rubble fields (should be avoided; may stop an icebreaker) is of primary importance. Four helicopter-borne radar campaigns were organized in 1988-1992 in order to experimentally study the radar response to different Baltic ice types. Measurements for six ice categories and open water were conducted under various weather conditions (wet and dry ice and snow surfaces). Based on the experimental data set, (1) the optimum radar parameters for Baltic ice mapping were determined, followed by three additional recommended systems, and (2) the feasibility of using synthetic aperture radar (SAR) images from present (ESA ERS-1 satellite) and near-future (ESA ERS-2 satellite, U.S. SIR-C Shuttle mission and Canadian Radarsat satellite) missions for discriminating sea ice categories was evaluated.
ER  - 


TY  - JOUR
TI  - Sensor Fusion for Aircraft Detection at Airport Ramps Using Conditional Random Fields
T2  - IEEE Transactions on Intelligent Transportation Systems
SP  - 18100
EP  - 18112
AU  - S. Lee
AU  - S. -W. Seo
PY  - 2022
KW  - Airports
KW  - Cameras
KW  - Point cloud compression
KW  - Radar
KW  - Airplanes
KW  - Laser radar
KW  - Ground support
KW  - Aircraft detection
KW  - sensor fusion
KW  - self-driving at airport ramps
KW  - point cloud segmentation
DO  - 10.1109/TITS.2022.3157809
JO  - IEEE Transactions on Intelligent Transportation Systems
IS  - 10
SN  - 1558-0016
VO  - 23
VL  - 23
JA  - IEEE Transactions on Intelligent Transportation Systems
Y1  - Oct. 2022
AB  - Self-driving baggage tractors on airport ramps or aprons enable better airport operation procedures and support the expansion of the aviation market. Airport ramps have unique mobility requirements in terms of layout, population, demand, and patterns. Avoiding aircraft movement on an airport apron is a top priority because of critical security and safety issues. Existing aircraft detection approaches use remote-sensing images or surveillance cameras. However, these are not compatible with sensors for low-height equipment at airport ramps. Similarly, public road-based self-driving studies have not considered detecting the massive size and concave contours of movable objects. Camera sensors cannot accurately measure the distance of concave contours, whereas a lidar sensor cannot easily cluster or classify an object among point cloud data. In this paper, we present the fusion of cameras and lidar sensors for aircraft and object detection at airport ramps. We use parallel detection from lidar and camera sensors and then integrate both detection results to compensate for any issues. Using the proposed energy optimization model by adapting a conditional random field, we can handle over- and under-segmentation of the point cloud objects caused by the sparse point cloud generated by the aircraft. Our algorithm achieves 31.1% improvement on tracking and 5.5% improvement on classification over other fusion algorithms when applied to a dataset acquired from the Cincinnati and Northern Kentucky airport.
ER  - 


TY  - JOUR
TI  - A finite mixtures algorithm for finding proportions in SAR images
T2  - IEEE Transactions on Image Processing
SP  - 1182
EP  - 1186
AU  - R. Samadani
PY  - 1995
KW  - Histograms
KW  - Pixel
KW  - Ice
KW  - Displays
KW  - Maximum likelihood estimation
KW  - Equations
KW  - Layout
KW  - Distribution functions
KW  - Solids
KW  - Iterative algorithms
DO  - 10.1109/83.403427
JO  - IEEE Transactions on Image Processing
IS  - 8
SN  - 1941-0042
VO  - 4
VL  - 4
JA  - IEEE Transactions on Image Processing
Y1  - Aug. 1995
AB  - This correspondence describes an algorithm for estimating the proportions of classes in an SAR image by first assuming that an image consists of a mixture of a known number of different pixel types. A maximum likelihood estimate of the parameters of the resulting mixture distribution is then evaluated using the EM algorithm. An advantage of the finite mixtures approach is that the quantities of interest, the proportions, are directly estimated. The technique is applied to aircraft synthetic aperture radar (SAR) images of sea ice. In addition to finding the proportions of the classes, knowledge of the mixture components allows image displays tailored to a user's requirements.<>
ER  - 


TY  - CONF
TI  - Invariant pattern recognition using higher-order neural networks
T2  - Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)
SP  - 1273
EP  - 1276 vol.2
AU  - J. Wu
AU  - Jyh-Yeong Chang
PY  - 1993
KW  - Pattern recognition
KW  - Neural networks
KW  - Feature extraction
KW  - Rotation measurement
KW  - Lifting equipment
KW  - Control engineering
KW  - Associative memory
KW  - Aircraft
KW  - Image classification
KW  - Computer networks
DO  - 10.1109/IJCNN.1993.716777
JO  - Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)
Y1  - 25-29 Oct. 1993
AB  - This paper explores the use of a higher-order neural networks to implement a pattern recognition system that is insensitive to transformations, i.e., translation, rotation, and scaling. The proposed implementation of the invariant system consists of a feature extractor (a third-order neural network) and a trainable classifier (a single-layer linear associative memory). A single parameter, sphericity, which represents the similarity of two triangles, is introduced into the third-order neural network structures, from which the invariant feature vector is extracted. In this way, the invariant pattern recognition problem can be formulated and the invariance property can be proven under the assumption that the input pattern is continuous. The vast storage requirement usually encountered in higher-order networks is overcome, since only the activated pixels have to be processed in our scheme. Translation invariance is guaranteed by our invariant structure for the grid transformation of the binary image. Simulation results for typed numerals with different feature vector lengths show that the invariant system achieves 100% recognition accuracy for rotated and scaled patterns, respectively. Accuracy up to 95.11% is achieved for the random combination of rotated and scaled patterns. A 99.60% success rate for combined transformation is achieved for the recognition of various aircraft figures.
ER  - 


TY  - CONF
TI  - Intelligent missions for MAVs: visual contexts for control, tracking and recognition
T2  - IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004
SP  - 1640
EP  - 1645 Vol.2
AU  - S. Todorovic
AU  - M. C. Nechyba
PY  - 2004
KW  - Object detection
KW  - Machine vision
KW  - Image segmentation
KW  - Aircraft
KW  - Aerospace control
KW  - Stability
KW  - Control systems
KW  - Object recognition
KW  - Real time systems
KW  - Feature extraction
DO  - 10.1109/ROBOT.2004.1308059
JO  - IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004
IS  - 
SN  - 1050-4729
VO  - 2
VL  - 2
JA  - IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004
Y1  - 26 April-1 May 2004
AB  - In this paper, we develop a unified vision system for small-scale aircraft that not only addresses basic flight stability and control, but also enables more intelligent missions, such as ground object recognition and moving-object tracking. The proposed system defines a framework for real-time image feature extraction, horizon detection and sky/ground segmentation, and contextual ground object detection. Multiscale Linear Discriminant Analysis (MLDA) defines the first stage of the vision system, and generates a multiscale description of images, incorporating both color and texture through a dynamic representation of image details. This representation is ideally suited for horizon detection and sky/ground segmentation of images, which we accomplish through the probabilistic representation of tree-structured belief networks (TSBN). Specifically, we propose incomplete meta TSBNs (IMTSBN) to accommodate the properties of our MLDA representation and to enhance the descriptive component of these statistical models. In the last stage of the vision processing, we seamlessly extend this probabilistic framework to perform computationally efficient detection and recognition of objects in the segmented ground region, through the idea of visual contexts. By exploiting visual contexts, we can quickly focus on candidate regions where objects of interest may be found, and then perform additional analysis for those regions only. Throughout, our approach is heavily influenced by real-time constraints and robustness to transient video noise.
ER  - 


TY  - CONF
TI  - Signal processing and object recognition in 3-D laser imagery on a Connection Machine
T2  - Sixth Multidimensional Signal Processing Workshop,
SP  - 645
EP  - 
AU  - D. E. Larch
AU  - S. F. Miller
PY  - 1989
KW  - Signal processing
KW  - Object recognition
KW  - Reflectivity
KW  - Aircraft
KW  - Image sensors
KW  - Vehicles
KW  - Layout
KW  - Signal resolution
KW  - Pixel
KW  - Supercomputers
DO  - 10.1109/MDSP.1989.97142
JO  - Sixth Multidimensional Signal Processing Workshop,
IS  - 
SN  - 
VO  - 
VL  - 
JA  - Sixth Multidimensional Signal Processing Workshop,
Y1  - 6-8 Sept. 1989
AB  - Summary form only given. Imagery from an aircraft laser sensor is being processed to find camouflaged and uncamouflaged vehicles in varying backgrounds. The laser sensor operates at 0.85 mu m and measures, using cross-track scanning, both range from the sensor and reflectivity of the scene at a resolution of 6 in. For a typical application, the range and reflectance imagery is produced at a rate of 4 million pixels/s. The processing, done on a Connection Machine, a fine-grained, massively parallel supercomputer, includes two steps: removing artifacts in the raw signal data and automatically recognizing objects using a hybrid statistical classification and model-matching approach. The artifact removed is a sawtooth pattern superimposed on the range information caused by the 10-m range ambiguity of the sensor. Object recognition occurs as a two-step process: statistical classification for rapid image screening and object detection, and 3-D model matching for object recognition. A highly parallel model matching scheme is used to complete the recognition process.<>
ER  - 


TY  - JOUR
TI  - A Lightweight Spatial and Temporal Multi-Feature Fusion Network for Defect Detection
T2  - IEEE Transactions on Image Processing
SP  - 472
EP  - 486
AU  - B. Hu
AU  - B. Gao
AU  - W. L. Woo
AU  - L. Ruan
AU  - J. Jin
AU  - Y. Yang
AU  - Y. Yu
PY  - 2021
KW  - Image segmentation
KW  - Feature extraction
KW  - Semantics
KW  - Convolution
KW  - Task analysis
KW  - Principal component analysis
KW  - Data mining
KW  - Image segmentation
KW  - sequence-PCA
KW  - attention
KW  - model compression
KW  - defect detection
DO  - 10.1109/TIP.2020.3036770
JO  - IEEE Transactions on Image Processing
IS  - 
SN  - 1941-0042
VO  - 30
VL  - 30
JA  - IEEE Transactions on Image Processing
Y1  - 2021
AB  - This article proposes a hybrid multi-dimensional features fusion structure of spatial and temporal segmentation model for automated thermography defects detection. In addition, the newly designed attention block encourages local interaction among the neighboring pixels to recalibrate the feature maps adaptively. A Sequence-PCA layer is embedded in the network to provide enhanced semantic information. The final model results in a lightweight structure with smaller number of parameters and yet yields uncompromising performance after model compression. The proposed model allows better capture of the semantic information to improve the detection rate in an end-to-end procedure. Compared with current state-of-the-art deep semantic segmentation algorithms, the proposed model presents more accurate and robust results. In addition, the proposed attention module has led to improved performance on two classification tasks compared with other prevalent attention blocks. In order to verify the effectiveness and robustness of the proposed model, experimental studies have been carried out for defects detection on four different datasets. The demo code of the proposed method can be linked soon: http://faculty.uestc.edu.cn/gaobin/zh_CN/lwcg/153392/list/index.htm
ER  - 


TY  - CONF
TI  - The generalized classification of Unmanned Air Vehicles
T2  - 2013 IEEE 2nd International Conference Actual Problems of Unmanned Air Vehicles Developments Proceedings (APUAVD)
SP  - 28
EP  - 34
AU  - A. G. Korchenko
AU  - O. S. Illyash
PY  - 2013
KW  - Engines
KW  - Unmanned aerial vehicles
KW  - Aircraft
KW  - Fuels
KW  - Standards
KW  - Aircraft propulsion
KW  - Aerospace electronics
KW  - Unmanned Air Vehicle
KW  - Unmanned Air Vehicle System
KW  - classification of UAV
KW  - fundamental features of UAV
DO  - 10.1109/APUAVD.2013.6705275
JO  - 2013 IEEE 2nd International Conference Actual Problems of Unmanned Air Vehicles Developments Proceedings (APUAVD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2013 IEEE 2nd International Conference Actual Problems of Unmanned Air Vehicles Developments Proceedings (APUAVD)
Y1  - 15-17 Oct. 2013
AB  - According to a generalization of common classifications and performance characteristics of existing Unmanned Air Vehicles, this article calls attention to its classification which is based on 16 the fundamental features: 1) aircraft applications; 2) type of a control system; 3) flight rules; 4) airspace classification; 5) aircraft types; 6) wing types; 7) takeoff/landing direction; 8) types of take-off/landing; 9) aircraft engine types; 10) fuel system; 11) fuel tank types; 12) number of exploitations; 13) category (according to the weight and maximum range of flight UAV); 14) flight radius; 15) flight altitude; 16) Aircraft Functions.
ER  - 


TY  - CONF
TI  - A Method of Identifying Pitot Tube Cover Based on Color Histogram Features and SVM
T2  - 2020 11th International Conference on Prognostics and System Health Management (PHM-2020 Jinan)
SP  - 34
EP  - 39
AU  - Z. Zhong
AU  - J. Guo
AU  - H. Zuo
AU  - J. Xu
PY  - 2020
KW  - Image color analysis
KW  - Electron tubes
KW  - Histograms
KW  - Feature extraction
KW  - Aircraft
KW  - Support vector machines
KW  - Object detection
KW  - PHM
KW  - digital image processing
KW  - pitot tube cover
KW  - color histogram features
KW  - SVM
DO  - 10.1109/PHM-Jinan48558.2020.00014
JO  - 2020 11th International Conference on Prognostics and System Health Management (PHM-2020 Jinan)
IS  - 
SN  - 2166-5656
VO  - 
VL  - 
JA  - 2020 11th International Conference on Prognostics and System Health Management (PHM-2020 Jinan)
Y1  - 23-25 Oct. 2020
AB  - The pitot tube cover is a ground protection device used to avoid the error of airspeed indication caused by foreign objects entering the pitot tube during the parking period and thus threatening the aviation safety. At the same time, the aircraft taking off without taking off the pitot tube cover is also a serious threat to aviation safety, so a method for identifying this kind of production defects is proposed. The existing 36 pitot tube on-site images (with and without covers) were preprocessed using the improved sliding window object detection algorithm and the image enhancement method based on the grayscale transformation. The color histogram features of the processed images was extracted. Using SVM for fault recognition, and the parameters of SVM are optimized by K-CV method. After optimization, the cross validated identification rate can up to 96%. The proposed method can be applied to the inspection of UAV around the aircraft, which is conducive to improving the PHM capability of civil aircraft pitot-static pressure system.
ER  - 


TY  - CONF
TI  - Texture Classification of Very High Resolution UAS Imagery Using a Graphics Processing Unit
T2  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
SP  - 6476
EP  - 6479
AU  - S. Samiappan
AU  - L. Casagrande
AU  - G. M. Machado
AU  - G. Turnage
AU  - L. Hathcock
AU  - R. Moorhead
AU  - J. Ball
PY  - 2018
KW  - Graphics processing units
KW  - Feature extraction
KW  - Remote sensing
KW  - Random access memory
KW  - Optimization
KW  - Support vector machines
KW  - Parallel processing
KW  - graphics processing
KW  - texture
KW  - invasive species
KW  - UAS
KW  - remote sensing
KW  - probabilistic neural network
DO  - 10.1109/IGARSS.2018.8519298
JO  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2018
AB  - The high computational demands of classifying very high-resolution (VHR) imagery calls for efficient implementations such as parallel processing, in particular, for computing on Graphics Processing Units (GPUs). In this work, a problem of classifying invasive Phragmites australis from other vegetation is considered. This study uses VHR imagery with a ground sampling distance of 25 centimeters acquired using a small-unmanned aerial system. We studied the use of Compute Unified Device Architecture for two feature extraction methods with a probabilistic neural network classifier. The texture extraction and parameter optimization on GPUs uses a divide and conquer strategy. The results of the study show the use of GPUs significantly increased speed when compared to a traditional Central Processing Unit based classification setup.
ER  - 


TY  - CONF
TI  - The study of the detection of pedestrian and bicycle using image processing
T2  - Proceedings of the 2003 IEEE International Conference on Intelligent Transportation Systems
SP  - 340
EP  - 345 vol.1
AU  - Zhijun Qui
AU  - Danya Yao
AU  - Yi Zhang
AU  - Daosong Ma
AU  - Xinyu Liu
PY  - 2003
KW  - Bicycles
KW  - Image processing
KW  - Object detection
KW  - Computer vision
KW  - Target tracking
KW  - Feature extraction
KW  - Communication system traffic control
KW  - Roads
KW  - Monitoring
KW  - Cameras
DO  - 10.1109/ITSC.2003.1251974
JO  - Proceedings of the 2003 IEEE International Conference on Intelligent Transportation Systems
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - Proceedings of the 2003 IEEE International Conference on Intelligent Transportation Systems
Y1  - 12-15 Oct. 2003
AB  - Besides vehicle both pedestrian and bicycle are two primary components in urban traffic flow. Video object tracking is an important method of information detection in ITS. This system implements an approach for detecting pedestrians and bicycles in urban traffic scenes by means of feature-based reason on visual data. In the video traffic monitoring system, matching method is often used to find the position of the moving object in the image sequence. This paper tried to propose an improved algorithm of corner feature extraction, an algorithm of position matching using motion matching and a method for traffic object classification. Experiment results were followed for verification.
ER  - 


TY  - JOUR
TI  - Object Detection for Unmanned Aerial Vehicle Camera via Convolutional Neural Networks
T2  - IEEE Journal on Miniaturization for Air and Space Systems
SP  - 98
EP  - 103
AU  - I. V. Saetchnikov
AU  - E. A. Tcherniavskaia
AU  - V. V. Skakun
PY  - 2021
KW  - Feature extraction
KW  - Computer architecture
KW  - Object detection
KW  - Neural networks
KW  - Convolutional neural networks
KW  - Task analysis
KW  - Proposals
KW  - Image segmentation
KW  - neural network architecture
KW  - object detection
KW  - pattern recognition
DO  - 10.1109/JMASS.2020.3040976
JO  - IEEE Journal on Miniaturization for Air and Space Systems
IS  - 2
SN  - 2576-3164
VO  - 2
VL  - 2
JA  - IEEE Journal on Miniaturization for Air and Space Systems
Y1  - June 2021
AB  - The object tracking alongside the image segmentation have recently become a particular significance in satellite and aerial imagery. The latest achievements in this field are closely related to the application of the deep-learning algorithms and, particularly, convolutional neural networks (CNNs). Supplemented by the sufficient amount of the training data, CNNs provide the advantageous performance in comparison to the classical methods based on Viola-Jones or support vector machines. However, the application of CNNs for the object detection on the aerial images faces several general issues that cause classification error. The first one is related to the limited camera shooting angle and spatial resolution. The second one arises from the restricted dataset for specific classes of objects that rarely appear in the captured data. This article represents a comparative study on the effectiveness of different deep neural networks for detection of the objects with similar patterns on the images within a limited amount of the pretrained datasets. It has been revealed that YOLO ver. 3 network enables better accuracy and faster analysis than region convolution neural network (R-CNN), Fast R-CNN, Faster R-CNN, and SSD architectures. This has been demonstrated on the example of “Stanford dataset,” “DOTA v-1.5,” and “xView 2018 Detection” datasets. The following metrics on the accuracy have been obtained for the YOLO ver. 3 network: 89.12 mAP (Stanford dataset), 80.20 mAP (DOTA v-1.5), and 78.29 (xView 2018) for testing; and 85.51 mAP (Stanford dataset), 79.28 (DOTA v-1.5), and 79.92 (xView 2018) on validation with the analysis speed of 26.82 frames/s.
ER  - 


TY  - CONF
TI  - Building change detection based on 3D reconstruction
T2  - 2015 IEEE International Conference on Image Processing (ICIP)
SP  - 4126
EP  - 4130
AU  - B. Chen
AU  - L. Deng
AU  - Y. Duan
AU  - S. Huang
AU  - J. Zhou
PY  - 2015
KW  - Three-dimensional displays
KW  - Buildings
KW  - Image reconstruction
KW  - Cameras
KW  - Solid modeling
KW  - Correlation
KW  - Geometry
KW  - building change detection
KW  - 3D reconstruction
KW  - 2D-3D registration
KW  - UAV aerial image
DO  - 10.1109/ICIP.2015.7351582
JO  - 2015 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE International Conference on Image Processing (ICIP)
Y1  - 27-30 Sept. 2015
AB  - Automatic building change detection at different periods is very important for city monitoring, disaster assessment, map updating, etc. Some existing data sources could be used in this task such as 3D geometry model (e.g. Digital Surface Model, Geographic Information System) and radiometric images from satellites or special aircrafts. However, it is too expensive for timely change detection by using these above methods. With the rapid development of UAV technique, capturing the city building images with high resolution camera at a low altitude becomes cheaper and cheaper. Using these easily acquired aerial images, we proposed a novel change detection framework based on RGB-D map generated by 3D reconstruction, which can overcome the large illumination changes. Firstly, an image-based 3D reconstruction is applied to retrieve two point clouds and related camera poses from two aerial image sets captured at different periods. Then an RGB-D map could be generated from each 3D model, followed by a 2D-3D registration procedure to align the two reconstructed 3D point clouds together. At last, a difference depth map could be generated and from which we can use random forest classification and component connectivity analysis techniques to segment the changed building areas out. Experimental results have illustrated the effectiveness and applicability of the proposed framework.
ER  - 


TY  - CONF
TI  - Aerial Landscape Recognition via Multi-Input Neural Network
T2  - 2021 International Conference on Military Technologies (ICMT)
SP  - 1
EP  - 5
AU  - L. Kopečný
AU  - J. Hnidka
PY  - 2021
KW  - Histograms
KW  - Military computing
KW  - Neural networks
KW  - Software algorithms
KW  - Unmanned aerial vehicles
KW  - Real-time systems
KW  - Software
KW  - Unmanned Aerial Vehicles
KW  - Multi-input neural networks
KW  - aerial landscape recognition
KW  - histogram
KW  - Principal Component Analysis
KW  - Gabor Filter
DO  - 10.1109/ICMT52455.2021.9502749
JO  - 2021 International Conference on Military Technologies (ICMT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Military Technologies (ICMT)
Y1  - 8-11 June 2021
AB  - Throughout the last decade, the advancements in the hardware allow use for wider applications of the unmanned aerial vehicles (UAV). UAVs feature significant advantages in autonomous aerial landscape mapping and recognition (ALR) over traditional methods due to their high level of operationality and mission repeatability, along with a simple alteration of e.g., on board remote sensors. ALR system based on convolutional neural networks is proposed. The system is designed with real-time capabilities. Data classification based on histogram and Gabor filter is explored on commercially available aerial images. The research roadmap designed to offload the dependency of the process on flight testing to improve the cost-efficiency of the development is proposed as well.
ER  - 


TY  - CONF
TI  - Detection of Atta Capiguara Ant Nests from aerial images using Random Forest Classifiers
T2  - 2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics (SBR), and 2023 Workshop on Robotics in Education (WRE)
SP  - 59
EP  - 64
AU  - T. B. De Almeida
AU  - A. C. Hernandes
PY  - 2023
KW  - Image segmentation
KW  - Computer vision
KW  - Machine learning algorithms
KW  - Pest control
KW  - Cameras
KW  - Data models
KW  - Classification algorithms
KW  - Computer vision
KW  - Random Forest
KW  - Ant Nest
KW  - Unmanned Aerial Vehicle
DO  - 10.1109/LARS/SBR/WRE59448.2023.10333031
JO  - 2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics (SBR), and 2023 Workshop on Robotics in Education (WRE)
IS  - 
SN  - 2643-685X
VO  - 
VL  - 
JA  - 2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics (SBR), and 2023 Workshop on Robotics in Education (WRE)
Y1  - 9-11 Oct. 2023
AB  - In the last 40 years, Brazilian agricultural production and productivity have had significant increases. Agribusiness has been strongly recognised in the growth of the Brazilian Gross Domestic Product, which has resulted in investments and technological advances in the agricultural sector. With these technological advances came precision agriculture. As part of the advances in precision agriculture, Unmanned Aircraft are gaining more and more space, given their flexibility of use and diversity of applications. At the same time, the use of computer vision and machine learning brings analysis technologies that further optimise processes, such as pasture mapping and pest detection. And when it comes to crop pests, leaf-cutter ants come to mind. The direct and indirect damage that leaf-cutting ants can cause amounts to billions of dollars worldwide. The leaf-cutting ant, known as Sauva, is one of the insects that causes most damage to pastures in southeastern Brazil, even competing with cattle for grass. Pest control is of paramount importance for the growth of agricultural production, therefore, this article presents the proposal for the development of an algorithm capable of detecting anthills. Based on recent technological advances and cost reduction, the proposed detection was performed using an unmanned aerial vehicle with an integrated conventional RGB camera and an algorithm, based on computer vision and machine learning. The proposed classification algorithm was Random Forest and it presented an accuracy of approximately 70%. The application of computer vision and quadrotors in the detection of ant nests represented a good opportunity to boost precision agriculture and pest control in the field.
ER  - 


TY  - CONF
TI  - Feature extraction and classification using a hierarchical neural network topology
T2  - IJCNN-91-Seattle International Joint Conference on Neural Networks
SP  - 906 vol.2
EP  - 
AU  - K. L. Priddy
AU  - S. K. Rogers
AU  - M. Kabrisky
AU  - B. Welsh
AU  - D. W. Ruck
AU  - J. Jones
PY  - 1991
KW  - Feature extraction
KW  - Neural networks
KW  - Network topology
KW  - Aircraft
KW  - Bonding
KW  - Feedforward neural networks
KW  - Skin
KW  - Ultrasonic transducers
KW  - Artificial neural networks
KW  - Image recognition
DO  - 10.1109/IJCNN.1991.155494
JO  - IJCNN-91-Seattle International Joint Conference on Neural Networks
IS  - 
SN  - 
VO  - ii
VL  - ii
JA  - IJCNN-91-Seattle International Joint Conference on Neural Networks
Y1  - 8-12 July 1991
AB  - Summary form only given as follows. The current trend in pattern recognition of objects of interest in an image is to use templating techniques via matched filters. The authors explore the use of a hierarchical neural network utilizing Gabor functions for feature extraction and feedforward networks for classification. The proposed combination of Gabor functions and feedforward neural networks in a hierarchical system is shown to be a viable concept for pattern recognition.<>
ER  - 


TY  - CONF
TI  - Classification of Harmful Noise Signals for Hearing Aid Applications using Spectrogram Images and Convolutional Neural Networks
T2  - 2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)
SP  - 1
EP  - 9
AU  - K. Zaman
AU  - M. şah
AU  - C. Direkoğlu
PY  - 2020
KW  - Storms
KW  - Low-pass filters
KW  - Adaptive filters
KW  - Auditory system
KW  - White noise
KW  - Hearing aids
KW  - Spectrogram
KW  - Hearing Aids
KW  - Convolutional Neural Networks
KW  - Spectrogram Images
KW  - Speech Classification
DO  - 10.1109/ISMSIT50672.2020.9254451
JO  - 2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)
Y1  - 22-24 Oct. 2020
AB  - Hearing aids are important for impaired people suffering from hearing loss. Generally, digital hearing aids are preferred since digital systems are flexible and programmable for different hearing loss conditions. Hearing aid applications usually use low-pass filters, adaptive filters and spectral analysis to remove harmful noise and amplify incoming speech signals. However, existing methods in digital hearing aids may not remove all harmful noises. To overcome this problem, first there is a need to understand whether the speech signal is harmful or harmless. If the signal is harmful, noise type should be identified, and then according to the noise type the filter will be selected. In this paper, we particularly focus on classification of speech signals into different classes. One of these classes is the clean speech signal class, and the others are harmful noisy speech signal classes that are generated with respect to five different noise types. We propose to use spectrogram images of incoming speech signals and a Convolution Neural Networks (CNN) for accurate classification. We create a dataset that consists of speech signals corrupted by different noise types such as white noise, jet aircrafts noise, storm noise, running tape noise and high-frequency noise. We compare the performance of different CNN architectures, and also conduct evaluation for computation time. Combination of spectrogram images with CNNs is a new approach for harmful speech classification. Results show that the proposed method can classify speech signals very accurately for different noise types.
ER  - 


TY  - CONF
TI  - Image Processing for Drones Detection
T2  - 2019 5th International Conference on Engineering, Applied Sciences and Technology (ICEAST)
SP  - 1
EP  - 4
AU  - R. Hamatapa
AU  - C. Vongchumyen
PY  - 2019
KW  - Drones
KW  - Image color analysis
KW  - Cameras
KW  - Machine learning
KW  - Libraries
KW  - Tools
KW  - Open Source Computer Vision (Open CV)
KW  - Library
KW  - Drones Detection
DO  - 10.1109/ICEAST.2019.8802578
JO  - 2019 5th International Conference on Engineering, Applied Sciences and Technology (ICEAST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 5th International Conference on Engineering, Applied Sciences and Technology (ICEAST)
Y1  - 2-5 July 2019
AB  - Presently, Drones Quadcopter has caused tremendous problems threatening the military security boundary and the area under Thai Army's surveillance. This article presents a conceptual framework for detecting and tracking unmanned aircraft by applying image processing. Study and analysis Compare advantages and disadvantages in order to develop and provide suitable equipment and tools Replacement of imported high-priced With limiting factors, namely distance, time, terrain And interference resistance The researchers used the camera USB 3.0 Cameras and Open Source Computer Vision (OpenCV) as a software development library on Linux operating systems to automatically record movies. In order to bring the image to analyze and distinguish the object classification by using Machine Learning through a sample of correct information and some incorrect information. Which when detecting, the installed device will calculate the coordinates detected, lock the target, track the movement Voice notification and report. The experiment was conducted by using Anova to test various factors affecting the detection, such as speed, light, color and size in 350 feet with the equipment we installed. The results of the experiment concluded that there was only a speed that had an effect at 22.65.
ER  - 


TY  - CONF
TI  - Neural networks for planar shape classification
T2  - ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing
SP  - 936
EP  - 939 vol.2
AU  - L. Gupta
AU  - M. R. Sayeh
PY  - 1988
KW  - Neural networks
KW  - Shape
KW  - Noise shaping
KW  - Military aircraft
KW  - Aerospace industry
KW  - Defense industry
KW  - Robustness
KW  - Biomedical imaging
KW  - Robotic assembly
KW  - Sequential analysis
DO  - 10.1109/ICASSP.1988.196744
JO  - ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing
IS  - 
SN  - 1520-6149
VO  - 
VL  - 
JA  - ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing
Y1  - 11-14 April 1988
AB  - A neural network approach is presented for the classification of closed planar shapes. The neural net classifier developed is robust and invariant to translation, rotation, and scaling. The primary foci are the development of an effective representation for planar shapes and the selection of a suitable neural network structure. In particular, planar shapes are represented by an ordered sequence that represents the Euclidean distance between the centroid and all contour pixels of the shape. It is also shown that for this classification problem and the representation derived, the three-layer perceptron with backpropagation training is an appropriate neural network configuration.<>
ER  - 


TY  - CONF
TI  - Deep learning based doppler radar for micro UAS detection and classification
T2  - MILCOM 2016 - 2016 IEEE Military Communications Conference
SP  - 924
EP  - 929
AU  - G. J. Mendis
AU  - T. Randeny
AU  - Jin Wei
AU  - A. Madanayake
PY  - 2016
KW  - Radar imaging
KW  - Aircraft
KW  - Correlation
KW  - Birds
KW  - Drones
KW  - Radio frequency
KW  - UAS
KW  - doppler
KW  - radar
KW  - spectral correlation function
KW  - cyclostationary signals
KW  - deep belief network
KW  - deep learning
DO  - 10.1109/MILCOM.2016.7795448
JO  - MILCOM 2016 - 2016 IEEE Military Communications Conference
IS  - 
SN  - 2155-7586
VO  - 
VL  - 
JA  - MILCOM 2016 - 2016 IEEE Military Communications Conference
Y1  - 1-3 Nov. 2016
AB  - In this paper, a radar sensor is proposed for the automated detection and classification of micro unmanned aerial systems (UASs), using Doppler signatures and their spectral correlation functions (SCFs). Our proposed system effectively detects and identifies UASs (within the radar beam width) by employing a Deep Belief Network (DBN) to classify the SCF signature patterns. The proposed system is experimentally verified using 3 UASs sensed with a 2.4 GHz continuous-wave doppler radar, which is set up in a laboratory environment. The experiment results show that a Doppler radar sensor is able to detect and classify UASs with an accuracy above 90% based on the automated classification of the radar signature SCF using DBN-based classifier.
ER  - 


TY  - CONF
TI  - Selective Sparse Sampling for Fine-Grained Image Recognition
T2  - 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
SP  - 6598
EP  - 6607
AU  - Y. Ding
AU  - Y. Zhou
AU  - Y. Zhu
AU  - Q. Ye
AU  - J. Jiao
PY  - 2019
KW  - Visualization
KW  - Image recognition
KW  - Feature extraction
KW  - Birds
KW  - Task analysis
KW  - Agriculture
KW  - Automobiles
DO  - 10.1109/ICCV.2019.00670
JO  - 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
IS  - 
SN  - 2380-7504
VO  - 
VL  - 
JA  - 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
Y1  - 27 Oct.-2 Nov. 2019
AB  - Fine-grained recognition poses the unique challenge of capturing subtle inter-class differences under considerable intra-class variances (e.g., beaks for bird species). Conventional approaches crop local regions and learn detailed representation from those regions, but suffer from the fixed number of parts and missing of surrounding context. In this paper, we propose a simple yet effective framework, called Selective Sparse Sampling, to capture diverse and fine-grained details. The framework is implemented using Convolutional Neural Networks, referred to as Selective Sparse Sampling Networks (S3Ns). With image-level supervision, S3Ns collect peaks, i.e., local maximums, from class response maps to estimate informative, receptive fields and learn a set of sparse attention for capturing fine-detailed visual evidence as well as preserving context. The evidence is selectively sampled to extract discriminative and complementary features, which significantly enrich the learned representation and guide the network to discover more subtle cues. Extensive experiments and ablation studies show that the proposed method consistently outperforms the state-of-the-art methods on challenging benchmarks including CUB-200-2011, FGVC-Aircraft, and Stanford Cars.
ER  - 


TY  - JOUR
TI  - Multi-Level Metric Learning Network for Fine-Grained Classification
T2  - IEEE Access
SP  - 166390
EP  - 166397
AU  - J. Wang
AU  - Y. Li
AU  - Z. Miao
AU  - X. Zhao
AU  - Z. Rui
PY  - 2019
KW  - Measurement
KW  - Feature extraction
KW  - Detectors
KW  - Automobiles
KW  - Deep learning
KW  - Computational efficiency
KW  - Fine-grained recognition
KW  - metric learning
KW  - multi-level objectives
KW  - classification
DO  - 10.1109/ACCESS.2019.2953957
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 7
VL  - 7
JA  - IEEE Access
Y1  - 2019
AB  - The application of fine-grained image classification can be problematic due to subtle differences between classes. The existing global feature-based methods have worse accuracies than regional feature-based methods, because regional feature-based methods focus on the determination of differentiated features within local regions. To learn more discriminative global features, in this paper, we proposed the use of L2 normalization to tackle a neglected conflict between the widely used metric loss (triplet loss) and classification loss (softmax loss) in global feature-based methods. Furthermore, a multi-level metric learning network (MMLN) is proposed for fine-grained image classification based on global features. In the MMLN, multi-level metric learning objectives and classification objectives are present at multiple high-level layers. The multi-level metric learning objectives work together to supervise the network in order to learn highly discriminative features. In addition, a new probability aggregation strategy (PAS) is proposed to produce a fused prediction by combining the multi-level predictive probabilities. Experiments were conducted on three standard fine-grained classification datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft). Results demonstrated that our MMLN achieved accuracies of 88.0%, 94.6% and 92.4% respectively and outperformed state-of-the-art methods, substantially improving fine-grained classification tasks. Besides, gradient-weighted class activation mapping (Grad-CAM) shows that the MMLN is able to pay more attention to the discriminative local regions due to the application of multi-level metric learning.
ER  - 


TY  - CONF
TI  - Roof Damage Assessment using Deep Learning
T2  - 2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
SP  - 6403
EP  - 6408
AU  - M. M. Hezaveh
AU  - C. Kanan
AU  - C. Salvaggio
PY  - 2017
KW  - Feature extraction
KW  - Machine learning
KW  - Inspection
KW  - Training
KW  - Insurance
KW  - Storms
KW  - Spatial resolution
DO  - 10.1109/AIPR.2017.8457946
JO  - 2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
IS  - 
SN  - 2332-5615
VO  - 
VL  - 
JA  - 2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
Y1  - 10-12 Oct. 2017
AB  - Industrial procedures can be inefficient in terms of time, money and consumer satisfaction. the rivalry among businesses' gradually encourages them to exploit intelligent systems to achieve such goals as increasing profits, market share, and higher productivity. The property casualty insurance industry is not an exception. The inspection of a roof's condition is a preliminary stage of the damage claim processing performed by insurance adjusters. When insurance adjusters inspect a roof, it is a time consuming and potentially dangerous endeavor. In this paper, we propose to automate this assessment using RGB imagery of rooftops that have been inflicted with damage from hail impact collected using small unmanned aircraft systems (sUAS) along with deep learning to infer the extent of roof damage (see Fig. I). We assess multiple convolutional neural networks on our unique rooftop damage dataset that was gathered using a sUAS. Our experiments show that we can accurately identify hail damage automatically using our techniques.
ER  - 


TY  - CONF
TI  - First results from trials on active-passive SAR imaging
T2  - 2023 XXXVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS)
SP  - 1
EP  - 4
AU  - D. Gromek
AU  - J. Drozdowicz
AU  - P. Samczyński
AU  - A. Gromek
PY  - 2023
KW  - Passive radar
KW  - C-band
KW  - Surface waves
KW  - Radar
KW  - UHF measurements
KW  - Radar imaging
KW  - Radar scattering
DO  - 10.23919/URSIGASS57860.2023.10265652
JO  - 2023 XXXVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS)
IS  - 
SN  - 2642-4339
VO  - 
VL  - 
JA  - 2023 XXXVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS)
Y1  - 19-26 Aug. 2023
AB  - This paper presents the preliminary results of synthetic aperture radar (SAR) imaging of the Earth’s surface obtained using both passive and active radar sensors at the same time. The measurement was carried out during a dedicated measurement campaign held in Poland. As part of the exercise, selected region(s) of the Earth’s surface were imaged using the C-band frequency modulation continuous wave (FMCW) active radar, and the DVB-T-based passive radar at the same time. Both radars were mounted on the light manned aircraft. The main goal of this measurement was to test a fusion possibility of the data acquired from both the active and passive SAR imaging taken in different frequency bands. Thanks to various properties of these radar systems such as foliage penetration capabilities of the passive radar operating at the low UHF frequency and/or high resolution of the C-band active radar, and based on different scattering phenomena (forward/backward) - the fusion of both technologies might provide new yet unexplored possibilities for target recognition and classification. What is more, thanks to the different reflection properties of the targets at different frequencies, different waveforms and different geometries used (a monostatic one in the case of active radars, and a bistatic-in the case of passive radars), such a combined configuration may open new opportunities for measuring other target features than in the case of the individual systems operated separately.
ER  - 


TY  - JOUR
TI  - Oil Spill Detection in Radarsat and Envisat SAR Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 746
EP  - 755
AU  - A. H. S. Solberg
AU  - C. Brekke
AU  - P. O. Husoy
PY  - 2007
KW  - Petroleum
KW  - Radar detection
KW  - Synthetic aperture radar
KW  - Feature extraction
KW  - Pollution
KW  - Sea surface
KW  - Clustering algorithms
KW  - Oceans
KW  - Cleaning
KW  - Aircraft
KW  - Classification
KW  - feature extraction
KW  - oil spill detection
KW  - synthetic aperture radar (SAR)
DO  - 10.1109/TGRS.2006.887019
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 3
SN  - 1558-0644
VO  - 45
VL  - 45
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - March 2007
AB  - We present algorithms for automatic detection of oil spills in synthetic aperture radar (SAR) images. The algorithms consist of three main parts, namely: 1) detection of dark spots; 2) feature extraction from the dark spot candidates; and 3) classification of dark spots as oil spills or look-alikes. The algorithms have been trained on a large number of Radarsat and Envisat Advanced Synthetic Aperture Radar (ASAR) images. The performance of the algorithm is compared to manual and semiautomatic approaches in a benchmark study using 59 Radarsat and Envisat images. The algorithms can be considered to be a good alternative to manual inspection when large ocean areas are to be inspected
ER  - 


TY  - CONF
TI  - Results and analysis of hybrid bistatic SAR experiments with spaceborne, airborne and stationary sensors
T2  - 2009 IEEE International Geoscience and Remote Sensing Symposium
SP  - II-238
EP  - II-241
AU  - I. Walterscheid
AU  - T. Espeter
AU  - C. Gierull
AU  - J. Klare
AU  - A. R. Brenner
AU  - J. H. G. Ender
PY  - 2009
KW  - Transmitters
KW  - Image analysis
KW  - Layout
KW  - Shadow mapping
KW  - Geometry
KW  - Satellites
KW  - Aircraft
KW  - Poles and towers
KW  - Scattering
KW  - Image resolution
KW  - Bistatic Synthetic Aperture Radar
KW  - SAR
DO  - 10.1109/IGARSS.2009.5418052
JO  - 2009 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 2
VL  - 2
JA  - 2009 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 12-17 July 2009
AB  - Bistatic SAR is a promising and useful supplement to a classical monostatic SAR system. Since transmitter and receiver are spatially separated, additional information of a scene may be provided. Further, as shadowing, layover, and foreshortening depend on the bistatic geometry, which can be quite different to the monostatic case, they can contribute to image analysis and classification. The transmitter and receiver are located on different platforms, which may either be close together or hundreds of kilometers apart. Typical platforms are satellites, UAVs, aircrafts, and towers. This paper presents recent bistatic SAR experiments with spaceborne, airborne, and stationary sensors, which have been conducted at FHR or in cooperation with Defence Research & Development Canada. Image results are presented and analyzed with respect to scattering behavior and resolution and compared to monostatic images.
ER  - 


TY  - CONF
TI  - Identity Re recognition Technology Based on Improved Lightweight ConvNetx Model
T2  - 2023 42nd Chinese Control Conference (CCC)
SP  - 7531
EP  - 7535
AU  - P. Siting
AU  - L. Lei
AU  - H. Zhu
AU  - R. Jun
AU  - S. Zhichao
AU  - J. Yang
PY  - 2023
KW  - Target tracking
KW  - Image recognition
KW  - Target recognition
KW  - Estimation
KW  - Reconnaissance
KW  - Feature extraction
KW  - Sensors
KW  - ConvNeXt network model
KW  - Re recognition
KW  - similarity is calculation
KW  - accurate tracking
DO  - 10.23919/CCC58697.2023.10239855
JO  - 2023 42nd Chinese Control Conference (CCC)
IS  - 
SN  - 1934-1768
VO  - 
VL  - 
JA  - 2023 42nd Chinese Control Conference (CCC)
Y1  - 24-26 July 2023
AB  - In the engineering field, there is a requirement for re recognition, including the identity recognition of cooperative and non-cooperative targets, that is, different sensors shoot targets in the same scene, and the identity recognition of targets from two perspectives is required, also known as target accurate retrieval. This method is widely used in fields such as automatic driving and military strikes. In the actual application scenario, deploying the re-recognition algorithm on the embedded processor needs to ensure the real-time computing speed and accuracy, so the complexity of the deep learning algorithm must be reduced, and pruning and lightweight processing are required. In the engineering field, when the image acquired by the sensor is subject to natural interference such as cloud and fog weather or fire and smoke, the difficulty of re-recognition increases. This paper adopts the lightweight improvement of ConvNeXt network model, and fine-tuned it on the basis of its original feature extraction function, so that it can successfully migrate to infrared image classification and also complete the extraction of infrared image target detail distinguishing features, and at the same time introduce the alignment of similar targets in infrared cross-view recognition. In the original similarity calculation, the average value is directly obtained from the spatial dimension of the output feature layer, which is replaced by the weight of the position in each space for estimation. The feature information of the multi-view marine ship target is extracted from the multi-sensor marine image, and its similarity is calculated, so as to determine the identity, thus reducing the support conditions and reconnaissance costs, and improving the accurate detection and tracking ability under the diverse dynamic reconnaissance conditions.
ER  - 


TY  - CONF
TI  - Shape-based defect classification for non destructive testing
T2  - 2015 IEEE Metrology for Aerospace (MetroAeroSpace)
SP  - 406
EP  - 410
AU  - G. D'Angelo
AU  - S. Rampone
PY  - 2015
KW  - Shape
KW  - Image retrieval
KW  - Impedance
KW  - Feature extraction
KW  - Testing
KW  - Conferences
KW  - Eddy currents
KW  - Non-destructive testing (NDT)
KW  - learning algorithm
KW  - signature-based classifier
KW  - content-based image retrieval (CBIR)
KW  - shape geometric descriptor (SGD)
KW  - eddy current testing (ECT)
DO  - 10.1109/MetroAeroSpace.2015.7180691
JO  - 2015 IEEE Metrology for Aerospace (MetroAeroSpace)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 IEEE Metrology for Aerospace (MetroAeroSpace)
Y1  - 4-5 June 2015
AB  - The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors.
ER  - 


TY  - CONF
TI  - Real-Time Embedded HPC Based Earthquake Damage Mapping Using 3D LiDAR Point Clouds
T2  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
SP  - 8241
EP  - 8244
AU  - P. Talreja
AU  - S. S. Durbha
AU  - R. C. Shinde
AU  - A. V. Potnis
PY  - 2021
KW  - Deep learning
KW  - Performance evaluation
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Satellites
KW  - Power demand
KW  - Earthquakes
KW  - LiDAR
KW  - High Performance Computing
KW  - Deep Learning
KW  - Damage Mapping
KW  - Earthquake
DO  - 10.1109/IGARSS47720.2021.9554481
JO  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS
Y1  - 11-16 July 2021
AB  - In the early hours following the earthquake, supporting humanitarian actions like rescue operations and relief distribution is the primary objective of the rescue managers. The damage mapping can be performed using reliable data that can be obtained from high-resolution satellite imagery but obtaining satellite imagery can be challenging for some days post disaster due to revisit time. Considering the disaster response timing, Unmanned Aerial Vehicles (UAV) are used because ground transportation systems are ineffective due to road blockage. In this work, we make use of Light Detection and Ranging (LiDAR) 3D point cloud data obtained for Haiti Earthquake. The focus of our work is to develop and implement an approach for LiDAR data classification to enable Earthquake damage mapping and detection. This is obtained by running our deep learning network on NVIDIA Jetson Nano embedded supercomputing platform. This approach takes the advantage of embedded High-Performance computing and low power consumption capabilities of Jetson Nano which enhances the classification and promotes rapid response which is the key to manage post-disaster activities. Jetson Nano is a feasible option which provides a GPU architecture that is optimized for running energy-aware deep learning models and which generates the results in real or near-real time. We envisage that our work could be extended to perform near real-time classification of LiDAR point clouds in a post earthquake scenario.
ER  - 


TY  - JOUR
TI  - RRCNet: Rivet Region Classification Network for Rivet Flush Measurement Based on 3-D Point Cloud
T2  - IEEE Transactions on Instrumentation and Measurement
SP  - 1
EP  - 12
AU  - Q. Xie
AU  - D. Lu
AU  - A. Huang
AU  - J. Yang
AU  - D. Li
AU  - Y. Zhang
AU  - J. Wang
PY  - 2021
KW  - Three-dimensional displays
KW  - Aircraft
KW  - Two dimensional displays
KW  - Skin
KW  - Deep learning
KW  - Inspection
KW  - Feature extraction
KW  - 3-D deep learning
KW  - attention mechanism
KW  - point cloud processing
KW  - rivet flush measurement
DO  - 10.1109/TIM.2020.3028399
JO  - IEEE Transactions on Instrumentation and Measurement
IS  - 
SN  - 1557-9662
VO  - 70
VL  - 70
JA  - IEEE Transactions on Instrumentation and Measurement
Y1  - 2021
AB  - In the aircraft manufacturing industry, rivet inspection is a vital task for the aircraft structure stability and aerodynamic performance. In this article, we propose a novel framework for fully automated rivet flush measurement, which is the key step in rivet inspection task. To efficiently perform rivet flush measurement, we first develop a mobile 3-D scanning system to automatically capture the 3-D point cloud of the aircraft skin surface. Subsequently, rivet regions are extracted through point cloud processing techniques. Instead of relying on handcrafted features, we propose a novel data-driven approach for rivet point extraction via a deep-learning-based technique. Our algorithm takes a scanned point cloud of the aircraft skin surface as input and produces a dense point cloud label result for each point, distinguishing as rivet point or not. To achieve this, we propose a rivet region classification network (RRCNet) that can input the 2-D representations of a point and output a binary label indicating that the point is rivet or nonrivet point. Moreover, we design a field attention unit (FAU) to assign adaptive weights to different forms of 2-D representations via the attention mechanism in convolutional neural networks. The extracted rivet regions can then be used to perform rivet flush measurement. The abovementioned components result in a fully automatic contactless measurement framework of aircraft skin rivet flush. Several experiments are performed to demonstrate the priority of the proposed RRCNet and the effectiveness of the presented rivet flush measurement framework.
ER  - 


TY  - CONF
TI  - Vision-based Human Identification with Face and Nametape Recognition in Aerial Casualty Monitoring System
T2  - 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
SP  - 1655
EP  - 1660
AU  - J. Lee
AU  - E. Quist
AU  - J. Chambers
AU  - J. Peel
AU  - K. Roman
AU  - N. Fisher
PY  - 2023
KW  - Training
KW  - Image recognition
KW  - Face recognition
KW  - Transfer learning
KW  - Text detection
KW  - Medical treatment
KW  - Personnel
KW  - Aerial image processing
KW  - Neural network
KW  - Face and text recognitions
KW  - Synthetic text data
KW  - Human detection
KW  - identification
KW  - and monitoring
KW  - Unmanned Aerial Systems.
DO  - 10.1109/RO-MAN57019.2023.10309606
JO  - 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
IS  - 
SN  - 1944-9437
VO  - 
VL  - 
JA  - 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
Y1  - 28-31 Aug. 2023
AB  - In emergency rescue scenarios, rapid identification of human casualties is a critical first step in enhancing emergency medical response. This task can be limited by the physical and cognitive capacity of rescue personnel, who are exposed to significant risk. The use of small unmanned aerial systems (sUAS) equipped with autonomous casualty assessment abilities can reduce these limitations and risks by enabling remote casualty detection, identification, and vitals assessment, providing standoff protection, and eliminating the need for human personnel to access the potentially hazardous scene. This paper presents a vision-based casualty assessment framework and specifically discusses our casualty identification software, which is designed to recognize the faces of casualties and identify their nametapes in images captured by sUAS under realistic conditions. Our approach addresses the limitations of the sUAS-captured long-distance images to enable accurate identification in challenging casualty monitoring situations. The face and nametape recognition algorithms will be integrated into the larger casualty perception framework and embedded into sUAS platforms to assist with emergency rescue operations. The total casualty perception system will detect, identify, and evaluate the condition of casualties from a remote location, providing standoff protection to first responders and rapid information to inform a suitable medical treatment plan.
ER  - 


TY  - CONF
TI  - An enhanced deep convolutional neural network for densely packed objects detection in remote sensing images
T2  - 2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)
SP  - 1
EP  - 4
AU  - Z. Deng
AU  - L. Lei
AU  - H. Sun
AU  - H. Zou
AU  - S. Zhou
AU  - J. Zhao
PY  - 2017
KW  - Proposals
KW  - Object detection
KW  - Feature extraction
KW  - Remote sensing
KW  - Neural networks
KW  - Pipelines
KW  - object detection
KW  - remote sensing images
KW  - convolutional neural network
KW  - densely packed
DO  - 10.1109/RSIP.2017.7958800
JO  - 2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)
Y1  - 18-21 May 2017
AB  - Faster Region based convolutional neural networks (FRCN) has shown great success in object detection in recent years. However, its performance will degrade on densely packed objects in real remote sensing applications. To address this problem, an enhanced deep CNN based method is developed in this paper. Following the common pipeline of “CNN feature extraction + region proposal + Region classification”, our method is primarily based on the latest Residual Networks (ResNets) and consists of two sub-networks: an object proposal network and an object detection network. For detecting densely packed objects, the output of multi-scale layers are combined together to enhance the resolution of the feature maps. Our method is trained on the VHR-10 data set with limited samples and successfully tested on large-scale google earth images, such as aircraft boneyard or tank farm, containing a substantial number of densely packed objects.
ER  - 


TY  - JOUR
TI  - Feature extraction for texture discrimination via random field models with random spatial interaction
T2  - IEEE Transactions on Image Processing
SP  - 635
EP  - 645
AU  - A. Speis
AU  - G. Healey
PY  - 1996
KW  - Feature extraction
KW  - Surface texture
KW  - Image analysis
KW  - Algorithm design and analysis
KW  - Data analysis
KW  - Autocorrelation
KW  - Parameter estimation
KW  - Image restoration
KW  - Image coding
KW  - Image texture analysis
DO  - 10.1109/83.491339
JO  - IEEE Transactions on Image Processing
IS  - 4
SN  - 1941-0042
VO  - 5
VL  - 5
JA  - IEEE Transactions on Image Processing
Y1  - April 1996
AB  - In this paper, we attack the problem of distinguishing textured images of real surfaces using small samples. We first analyze experimental data that results from applying ordinary conditional Markov fields. In the face of the disappointing performance of these models, we introduce a random field with spatial interaction that is itself a random variable (usually referred to as a random field in a random environment). For this class of models, we establish the power spectrum and the autocorrelation function as well-defined quantities, and we devise a scheme for the estimation of related parameters. The new set of features that resulted from this approach was applied to real images. Accurate discrimination was observed even for boxes of size 10/spl times/16.
ER  - 


TY  - CONF
TI  - Smart Farming Based Early Classification of Paddy Blast Disease Using Adaptive Deep Learning Algorithm
T2  - 2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT)
SP  - 49
EP  - 53
AU  - A. A
AU  - G. Preemi
AU  - S. R. Sriram
AU  - D. F. Deva Shahila
AU  - M. A. Merlin Suji
AU  - D. V. S. Sudeep Reddy
PY  - 2024
KW  - Smart agriculture
KW  - Deep learning
KW  - Plant diseases
KW  - Adaptation models
KW  - Adaptive systems
KW  - Accuracy
KW  - Crops
KW  - Adaptive Deep Neural Network
KW  - Agriculture
KW  - Dmey wavelet filter
KW  - Image processing
KW  - Plant infections
KW  - Rice blast disease
KW  - Soft computing
DO  - 10.1109/ICCPCT61902.2024.10673091
JO  - 2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 7th International Conference on Circuit Power and Computing Technologies (ICCPCT)
Y1  - 8-9 Aug. 2024
AB  - The backbone of the Indian economy, agriculture is essential to Indians' standard way of life. Plant disease monitoring by hand typically involves a tremendous labor and takes more time to perform. The primary goal of this research is to help farmers, particularly those who are dealing with blast disease in rice crops, by quickly and reliably detecting plant diseases. The primary goal of the research is to catalog the diseases that affect rice and identify crop infections early on. Therefore, the development of soft computing and image processing technologies for smart agriculture applications is the subject of this research study, with a special emphasis on applications in innovative design for the detection of diseases in rice. The key methods of analysis used include preprocessing, classification, feature extraction, and picture acquisition. The lands used for agriculture are the source of input images. Dmey wavelet filtering is the first step in the pre-processing procedure which removes noise from images. Next, image-based crop feature extraction techniques are utilized to perform an extensive study. The proposal is to use the Adaptive deep neural network (ADNN) to address the overlapping issue. In ADNN, the total accuracy is 97.3%. A wide range of rice blast diseases are correctly identified with the desired accuracy, regardless of the obscured settings, according to the evaluation results of the suggested method.
ER  - 


TY  - JOUR
TI  - Land-Use Mapping Using Edge Density Texture Measures on Thematic Mapper Simulator Data
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 104
EP  - 108
AU  - C. A. Hlavka
PY  - 1987
KW  - Density measurement
KW  - Computational modeling
KW  - Data analysis
KW  - Image analysis
KW  - Image texture analysis
KW  - Information analysis
KW  - Performance analysis
KW  - Satellites
KW  - Remote sensing
KW  - Aircraft
DO  - 10.1109/TGRS.1987.289786
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 1
SN  - 1558-0644
VO  - GE-25
VL  - GE-25
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - Jan. 1987
AB  - Texture analysis was performed as part of an investigation of the information content of Landsat Thematic Mapper (TM) imagery. High-altitude aircraft scanner imagery from the Airborne Thematic Mapper (ATM) instrument was acquired over central California and used to simulate TM data. Edge density texture images were constructed by computation of proportions of edge pixels in a 31×31 moving window on a near-infrared ATM band. A training technique was employed to select computational parameters to maximize the difference between edge density measurements in urban and in rural areas. The results of classification of the texture images showed that urban and rural areas could be distinguished with texture alone, indicating that inclusion of texture in automated classification procedures could significantly improve their accuracy.
ER  - 


TY  - CONF
TI  - Improving automated detection of land cover change for large areas using Landsat data
T2  - International Workshop on the Analysis of Multi-Temporal Remote Sensing Images, 2005.
SP  - 39
EP  - 43
AU  - Kuan Song
AU  - J. R. Townshend
AU  - Sunghee Kim
AU  - P. Davis
AU  - R. Clay
AU  - O. Rodas
PY  - 2005
KW  - Satellites
KW  - Remote sensing
KW  - Testing
KW  - Layout
KW  - Humans
KW  - Agriculture
KW  - Training data
KW  - Classification tree analysis
KW  - Aircraft
KW  - Performance analysis
DO  - 10.1109/AMTRSI.2005.1469836
JO  - International Workshop on the Analysis of Multi-Temporal Remote Sensing Images, 2005.
IS  - 
SN  - 
VO  - 
VL  - 
JA  - International Workshop on the Analysis of Multi-Temporal Remote Sensing Images, 2005.
Y1  - 16-18 May 2005
AB  - 
ER  - 


TY  - CONF
TI  - Increasingly Specialized Ensemble of Convolutional Neural Networks for Fine-Grained Recognition
T2  - 2018 25th IEEE International Conference on Image Processing (ICIP)
SP  - 594
EP  - 598
AU  - A. Simonelli
AU  - F. De Natale
AU  - S. Messelodi
AU  - S. R. Bulo
PY  - 2018
KW  - Feature extraction
KW  - Training
KW  - Zinc
KW  - Automobiles
KW  - Birds
KW  - Heating systems
KW  - Convolutional neural networks
KW  - Fine-grained recognition
KW  - weakly supervised classification
KW  - attention analysis
DO  - 10.1109/ICIP.2018.8451097
JO  - 2018 25th IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2018 25th IEEE International Conference on Image Processing (ICIP)
Y1  - 7-10 Oct. 2018
AB  - Finegrained recognition focuses on the challenging task of automatically identifying the subtle differences between similar categories. Current state-of-the-art approaches require elaborated feature learning procedures, involving tuning several hyper-parameters, or rely on expensive human annotations such as objects or parts location. In this paper we propose a simple method for fine-grained recognition that exploits a nearly cost-free attention-based focus operation to construct an ensemble of increasingly specialized Convolutional Neural Networks. Our method achieves state-of-the-art results on three of the most popular datasets used for fine-grained classification namely CUB Birds 200-2011, FGVC-Aircraft and Stanford Cars requiring minimal hyperparameter tuning and no annotations.
ER  - 


TY  - CONF
TI  - Database Generation for Non-Cooperative Air Target Identification
T2  - 2006 IET Seminar on High Resolution Imaging and Target Classification
SP  - 81
EP  - 82
AU  - G. Williams
AU  - B. O'Neill
AU  - J. Chadwick
PY  - 2006
DO  - 
JO  - 2006 IET Seminar on High Resolution Imaging and Target Classification
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2006 IET Seminar on High Resolution Imaging and Target Classification
Y1  - 21-21 Nov. 2006
AB  - Summary form only given. In this paper we will discuss the approach that we have evolved for populating a database with representative training data for the purpose of non-cooperative target identification (NCTI) of air targets by radar. Such NCTI should help to augment procedures like IFF to reduce the risk of fratricide and allow early designation of targets such that threats can be engaged before significant risk to vulnerable assets. This paper will focus on the use of synthetic data generated from limited information, such as photographs, to represent target types for which we don't have access to real radar measurements over any or a sufficiently representative range of aspects and configurations. This can often be the case for hostile aircraft types. Subscale models and computer electromagnetic predictions are both common methods that researchers have explored to simulate the radar returns from real aircraft. Both have advantages and disadvantages but the former is thought to be a feasible approach with current technology. We will discuss the methodology required for subscale model generation and subsequent use of the data to populate classifier databases and how to quantify the fidelity of the data such that confidence in classification result can be quantified and thus classifier performance as a whole estimated
ER  - 


TY  - CONF
TI  - Hinotori-C2 Mission : CN235MPA Aircraft Onboard Circularly Polarized Synthetic Aperture Radar (CP-SAR)
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
SP  - 8538
EP  - 8541
AU  - J. T. Sri Sumantyo
AU  - M. Y. Chua
AU  - C. E. Santosa
AU  - G. F. Panggabean
AU  - T. Watanabe
AU  - B. Setiadi
AU  - K. Tsushima
AU  - F. D. Sri Sumantyo
AU  - K. Sasmita
AU  - A. Mardiyanto
AU  - E. Supartono
AU  - E. T. Rahardjo
AU  - G. Wibisono
AU  - R. H. Jatmiko
AU  - Sudaryatno
AU  - T. H. Purwanto
AU  - B. S. Widartono
AU  - M. Kamal
AU  - R. H. Triharjanto
AU  - S. Gao
AU  - K. Ito
PY  - 2019
KW  - CP-SAR
KW  - Airborne
KW  - Circular polarization
KW  - Hinotori-C2
KW  - axial ratio
KW  - CN235MPA
DO  - 10.1109/IGARSS.2019.8899244
JO  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 28 July-2 Aug. 2019
AB  - Chiba University proposed and developed airborne C band circularly polarized synthetic aperture radar (CP-SAR) for environment and disaster monitoring. This paper explains concept, system configuration, RF system and antenna, and flight test of CP-SAR in Hinotori-C2 (Firebird-C2) mission onboard CN235MPA aircraft on 14-15 March 2018 at Indonesia. The result of flight test depicts full polarimetric CP images that shows good performance of CP-SAR. The assessment of CP image analysis, and novel image classification using axial ratio (AR), ellipticity (ε), and polarization ratio (ρ) are discussed.
ER  - 


TY  - CONF
TI  - Utilizing quadcopter as LARS image platform to determine the paddy spectral and growth parameter
T2  - 2014 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology
SP  - 210
EP  - 215
AU  - K. Zainuddin
AU  - S. N. Bohari
AU  - N. Ghazali
AU  - M. A. C. Aziz
AU  - A. M. Samad
PY  - 2014
KW  - Production
KW  - Equations
KW  - Mathematical model
KW  - Remote sensing
KW  - Estimation
KW  - Accuracy
KW  - Aerospace electronics
KW  - Low Altitude Remote Sensing (LARS) Image
KW  - Normalized Difference Vegetation Index (NDVI)
KW  - Quadratic Equation
KW  - Total NDVI
KW  - R2
DO  - 10.1109/ICARES.2014.7024403
JO  - 2014 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2014 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology
Y1  - 13-14 Nov. 2014
AB  - Low Altitude Remote Sensing (LARS) images from the unmanned radio-controlled quadcopter platform has been utilized in assessing relationship between paddy production and paddy spectral by using Normalized Difference Vegetation Index (NDVI) classification method. Sampling result from six well distributed area were the main feeder in establishing the relationship of paddy growth parameters (paddy age and coverage) with NDVI. Quadratic equation is the best fitted equation to describe the relationship between NDVI and paddy age with R2 values ranging from 0.960 to 0.974. Relationship between paddy coverage and NDVI by linear equation with correlation coefficient (R2) from 0.734 to 0.861. From both relationships, it shows that NDVI has the same proclivity with the paddy coverage in its life stage. The climax of paddy index in NDVI is 2 months paddy age. This period is the transition from vegetative to generative stage. Two growth variables were assessed in this research with the total NDVI presented the strongest exponential relationship to paddy production with an R2 value of 0.947 and the percentage of paddy estimation precision to 92%. This research enable the prediction of paddy production by using y = 0.0496e0.0978x equation, with x is the total NDVI and y is the paddy production. The result obtained reflect parameters sufficiency thus acknowledge the potential of light platform Unmanned Aerial Imaging System (UAIS) to estimate paddy production.
ER  - 


TY  - CONF
TI  - Researches on radar target classification based on high resolution range profiles
T2  - Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997
SP  - 951
EP  - 955 vol.2
AU  - Rong Hu
AU  - Zhaoda Zhu
PY  - 1997
KW  - Radar imaging
KW  - Radar scattering
KW  - Image resolution
KW  - Aircraft
KW  - Feature extraction
KW  - Electromagnetic scattering
KW  - Signal resolution
KW  - Signal processing algorithms
KW  - Fourier transforms
KW  - Discrete wavelet transforms
DO  - 10.1109/NAECON.1997.622757
JO  - Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997
Y1  - 14-17 July 1997
AB  - Radar target identification based on High Resolution Range Profiles (HRRPs) has received much attention because of its reduced complexity compared to those using two-dimensional (2D) ISAR images. Some preprocessing and classifying algorithms are examined in this paper for discriminating air-targets from signals of an experimental C-band radar. The real-world data of three different aircraft are used: Citation, An-26 and Yak-42. Several sets of features are derived from HRRPs and their effectiveness is several classifiers. Good and large reduction in dimensionality of feature vectors is achieved in the classification problem of multiple aircraft at multiple aspects.
ER  - 


TY  - JOUR
TI  - Large-Scale Dense 3-D Mapping Using Submaps Derived From Orthogonal Imaging Sonars
T2  - IEEE Journal of Oceanic Engineering
SP  - 354
EP  - 369
AU  - J. McConnell
AU  - I. Collado-Gonzalez
AU  - P. Szenher
AU  - B. Englot
PY  - 2025
KW  - Sonar
KW  - Imaging
KW  - Simultaneous localization and mapping
KW  - Robot sensing systems
KW  - Apertures
KW  - Feature extraction
KW  - Robot kinematics
KW  - Image segmentation
KW  - Underwater vehicles
KW  - Three-dimensional displays
KW  - Autonomous underwater vehicles (AUVs)
KW  - simultaneous localization and mapping (SLAM)
KW  - sonar imaging and ranging
DO  - 10.1109/JOE.2024.3458108
JO  - IEEE Journal of Oceanic Engineering
IS  - 1
SN  - 1558-1691
VO  - 50
VL  - 50
JA  - IEEE Journal of Oceanic Engineering
Y1  - Jan. 2025
AB  - 3-D situational awareness is critical for any autonomous system. However, when operating underwater, environmental conditions often dictate the use of acoustic sensors. These acoustic sensors are plagued by high noise and a lack of 3-D information in sonar imagery, motivating the use of an orthogonal pair of imaging sonars to recover 3-D perceptual data. Thus far, mapping systems in this area only use a subset of the available data at discrete timesteps and rely on object-level prior information in the environment to develop high-coverage 3-D maps. Moreover, simple repeating objects must be present to build high-coverage maps. In this work, we propose a submap-based mapping system integrated with a simultaneous localization and mapping system to produce dense, 3-D maps of complex unknown environments with varying densities of simple repeating objects. We compare this submapping approach to our previous works in this area, analyzing simple and highly complex environments, such as submerged aircraft. We analyze the tradeoffs between a submapping-based approach and our previous work leveraging simple repeating objects. We show where each method is well-motivated and where they fall short. Importantly, our proposed use of submapping achieves an advance in underwater situational awareness with wide aperture multibeam imaging sonar, moving toward generalized large-scale dense 3-D mapping capability for fully unknown complex environments.
ER  - 


TY  - CONF
TI  - Radar Scattering Classification Maps From Multifrequency Imaging Radar Polarimetric Data
T2  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
SP  - 24
EP  - 24
AU  - H. A. Zebker
AU  - J. J. van Zyl
AU  - T. G. Farr
PY  - 1989
KW  - Radar scattering
KW  - Radar imaging
KW  - Radar polarimetry
KW  - Sea surface
KW  - Airborne radar
KW  - Rough surfaces
KW  - Surface roughness
KW  - Aircraft propulsion
KW  - Radar measurements
KW  - Ice surface
DO  - 10.1109/IGARSS.1989.567140
JO  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 12th Canadian Symposium on Remote Sensing Geoscience and Remote Sensing Symposium,
Y1  - 10-14 July 1989
AB  - 
ER  - 


TY  - JOUR
TI  - Operational oil-slick characterization by SAR imagery and synergistic data
T2  - IEEE Journal of Oceanic Engineering
SP  - 487
EP  - 495
AU  - F. Girard-Ardhuin
AU  - G. Mercier
AU  - F. Collard
AU  - R. Garello
PY  - 2005
KW  - Radar detection
KW  - Synthetic aperture radar
KW  - Satellites
KW  - Automatic testing
KW  - Accidents
KW  - Filters
KW  - Image segmentation
KW  - Shape
KW  - Oils
KW  - Aircraft
KW  - Image analysis
KW  - oil pollution
KW  - satellite measurement
KW  - synthetic aperture radar (SAR)
DO  - 10.1109/JOE.2005.857526
JO  - IEEE Journal of Oceanic Engineering
IS  - 3
SN  - 1558-1691
VO  - 30
VL  - 30
JA  - IEEE Journal of Oceanic Engineering
Y1  - July 2005
AB  - A methodology is proposed for the semiautomatic detection, characterization, and classification of slicks detected in C-band Synthetic Aperture Radar (SAR). For the first detection step, automatic algorithms were tested on Environmental Research Satellite (ERS) and Environmental Satellite (EnviSat) images acquired during the Prestige tanker accident. These tests reveal that simple filter or segmentation methods efficiently detect slicks with high contrasts and simple shapes, while a new and more complex multiscale method is able to detect a wider range of slicks. The characteristics of automatically detected slicks are then combined with meteooceanic data in order to eliminate slicks related to wind anomalies and current fronts. The data suggest that slicks in cold upwelling waters are natural, and confirm that slicks are heavy oils when high sea states are present. This detection-classification methodology is validated with aircraft slick-tracking maps. In most cases, joint SAR and environmental data are sufficient to classify the slicks.
ER  - 


TY  - CONF
TI  - Terrain Classification from UAV Flights Using Monocular Vision
T2  - 2015 12th Latin American Robotics Symposium and 2015 3rd Brazilian Symposium on Robotics (LARS-SBR)
SP  - 271
EP  - 276
AU  - I. S. G. Campos
AU  - E. R. Nascimento
AU  - L. Chaimowicz
PY  - 2015
KW  - Optical imaging
KW  - Optical sensors
KW  - Biomedical optical imaging
KW  - Cameras
KW  - Robots
KW  - Histograms
KW  - Adaptive optics
DO  - 10.1109/LARS-SBR.2015.49
JO  - 2015 12th Latin American Robotics Symposium and 2015 3rd Brazilian Symposium on Robotics (LARS-SBR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2015 12th Latin American Robotics Symposium and 2015 3rd Brazilian Symposium on Robotics (LARS-SBR)
Y1  - 29-31 Oct. 2015
AB  - With the popularization of small Unmanned Aerial Vehicles (UAVs) and their usage diversification among various fields, such as aerial mapping applications, it is important to develop better terrain following techniques that rely solely on the vehicle's sensing capabilities. The objective of this paper is to evaluate whether is possible to gather information about terrain inclination and elevation from monocular video captured from such aircrafts. Our approach is biologically inspired by trying to reproduce some insects behaviour with the use of optical flow to infer about the terrain. We built an UAV specifically for this research which uses a gimbal stabilized down-facing camera and flew it at a fixed Above Sea Level (ASL) altitude. After performing preliminary analysis on sparse optical flow data and validating the concept, we moved towards a dense optical flow algorithm and created different descriptors to feed multiple decision trees in order to infer about terrain characteristics. We achieved accuracies of 77.34%, 86.75% and 91.85% depending on the evaluated characteristic, showing that our approach is valid.
ER  - 


TY  - JOUR
TI  - Deep Learning Approach for Fixed and Rotary-Wing Target Detection and Classification in Radars
T2  - IEEE Aerospace and Electronic Systems Magazine
SP  - 32
EP  - 42
AU  - S. M. D. Rizvi
AU  - S. Ahmad
AU  - K. Khan
AU  - A. Hasan
AU  - A. Masood
PY  - 2022
KW  - Deep learning
KW  - Radar detection
KW  - Radar imaging
KW  - Object detection
KW  - Doppler radar
KW  - Convolutional neural networks
KW  - Doppler effect
KW  - Neural networks
KW  - Radar signal processing
KW  - Detection algorithms
KW  - CFAR
KW  - Convolutional neural network (CNN)
KW  - Deep Neural Networks (DNN)
KW  - feature extraction
KW  - micro-doppler effects
KW  - radar signal processing
KW  - radar target recognition
DO  - 10.1109/MAES.2021.3140064
JO  - IEEE Aerospace and Electronic Systems Magazine
IS  - 3
SN  - 1557-959X
VO  - 37
VL  - 37
JA  - IEEE Aerospace and Electronic Systems Magazine
Y1  - 1 March 2022
AB  - Recent advancements demonstrate the application of machine learning techniques to radar signal processing, particularly the use of neural networks in one-dimensional (1-D) radar data to assist constant false alarm rate (CFAR) based target detection. However, the efficacy of using neural networks alone on 2-D data for target extraction remains to be explored. This research aims to develop a novel approach of designing a convolution neural network (CNN) based target detection system for real radar data. We propose a dual neural network detection scheme in which the first neural net filters out noisy images. Thereafter, we employ a second CNN-based target detector that extracts information from both radar data axes (fast-time and slow-time), thus utilizing additional information for better detection of targets even with low SNR. Moreover, the same CNN, in conjunction with a region-based convolutional network, exploits micro-Doppler features of rotary-wing aircraft in radar returns and uses them to further classify detected targets as fixed-wing or rotary-wing aircraft. To the best of our knowledge, no generalized methodology exists to address these two issues together. The proposed solution replaces the critical CFAR and peak detection algorithms from the radar signal processing chain. This article also introduces a labeled range-Doppler images dataset obtained from real-world air traffic control radar data. Experimental results demonstrate the superior performance of the proposed technique over 1-D and 2-D CFAR and peak detection algorithms.
ER  - 


TY  - CONF
TI  - Shape recognition by distributed recursive learning of multiscale trees
T2  - 12th International Conference on Image Analysis and Processing, 2003.Proceedings.
SP  - 26
EP  - 30
AU  - L. Lombardi
AU  - A. Petrosino
PY  - 2003
KW  - Shape
KW  - Neural networks
KW  - Object recognition
KW  - Pattern recognition
KW  - Image edge detection
KW  - Automata
KW  - Optical computing
KW  - Image resolution
KW  - Recurrent neural networks
KW  - Aircraft
DO  - 10.1109/ICIAP.2003.1234020
JO  - 12th International Conference on Image Analysis and Processing, 2003.Proceedings.
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 12th International Conference on Image Analysis and Processing, 2003.Proceedings.
Y1  - 17-19 Sept. 2003
AB  - We present an efficient and fully parallel 2D object recognition method based on the use of a multiscale tree representation of the object boundary and recursive learning of trees. Specifically, the object is represented by means of a tree where each node, corresponding to a boundary segment at some level of resolution, is characterized by a real vector containing curvature, length, and symmetry of the boundary segment, while the nodes are connected by arcs when segments at successive levels are spatially related. The recognition procedure is formulated as a training procedure made by recursive neural networks followed by a testing procedure over unknown tree structured patterns.
ER  - 


TY  - CONF
TI  - Visual Tracking Algorithm for Aircrafts in Airport
T2  - 2018 11th International Symposium on Computational Intelligence and Design (ISCID)
SP  - 311
EP  - 314
AU  - X. Zhang
AU  - M. Ding
AU  - W. Wang
PY  - 2018
KW  - Target tracking
KW  - Aircraft
KW  - Correlation
KW  - Airports
KW  - Feature extraction
KW  - Video sequences
KW  - object tracking
KW  - correlation filter
KW  - convolution neural network
KW  - airport surface surveillance
DO  - 10.1109/ISCID.2018.00077
JO  - 2018 11th International Symposium on Computational Intelligence and Design (ISCID)
IS  - 
SN  - 2473-3547
VO  - 01
VL  - 01
JA  - 2018 11th International Symposium on Computational Intelligence and Design (ISCID)
Y1  - 8-9 Dec. 2018
AB  - Visual tracking for aircraft is an important part of the airport surface surveillance. However, the current tracking algorithms do not perform well in complex environment like the airport. Aiming at this problem, this paper proposes a target tracking algorithm based on the correlation filter using deep conventional feature. Firstly, a convolution neural network is trained for the classification of aircraft. Then the shallow and deep features of the target are extracted by the network. Finally, these features are fused into the correlation filter tracking method. The proposed algorithm is compared with other trackers on ten video sequences with different weather conditions and different locations in the airport. Experimental results show that the proposed method can achieve high accuracy and success rate, and the overall performance is superior to other comparative algorithms.
ER  - 


TY  - CONF
TI  - Image processing for image understanding with neural nets
T2  - International 1989 Joint Conference on Neural Networks
SP  - 111
EP  - 115 vol.1
AU  - Barnard
AU  - Casasent
PY  - 1989
KW  - Neural networks
KW  - Pattern recognition
KW  - Image processing
DO  - 10.1109/IJCNN.1989.118567
JO  - International 1989 Joint Conference on Neural Networks
IS  - 
SN  - 
VO  - 
VL  - 
JA  - International 1989 Joint Conference on Neural Networks
Y1  - 18-22 June 1989
AB  - The image-processing techniques required for an image-understanding system are considered. Although many of the image-processing steps can be implemented with neural nets, it is found that only the classification stage is currently optimally implemented in this way. Conventional methods of image processing that can be used in conjunction with neural net classifiers are described. A case study employing these concepts is presented. Optical realizations of many operations are noted.<>
ER  - 


TY  - CONF
TI  - Sar Target Detection Based on Psift Feature Clustering
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
SP  - 17
EP  - 20
AU  - L. Zeng
AU  - D. Zhou
AU  - Q. Pan
AU  - C. Lu
AU  - Y. Zhou
PY  - 2019
KW  - Feature extraction
KW  - Radar polarimetry
KW  - Object detection
KW  - Synthetic aperture radar
KW  - Aircraft
KW  - Classification algorithms
KW  - Clustering algorithms
KW  - SAR images
KW  - PSIFT feature
KW  - NCM clustering
KW  - target detection
DO  - 10.1109/IGARSS.2019.8900284
JO  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 28 July-2 Aug. 2019
AB  - Aiming at the problem that it is difficult to obtain a big SAR target set with different angles, a sample-free SAR target detection method is proposed in this paper. This new method adopts PSIFT features with rotation invariance to describe the texture of potential targets, and divides the features of potential targets into target regions and non-target regions through NCM clustering. This method proposed of this paper can realize the automatic detection of SAR targets, and is also effective for targets with different orientation. The experimental results verify the feasibility and validity of the proposed method.
ER  - 


TY  - JOUR
TI  - A Time-Efficient Method for Anomaly Detection in Hyperspectral Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 3894
EP  - 3904
AU  - O. Duran
AU  - M. Petrou
PY  - 2007
KW  - Hyperspectral imaging
KW  - Hyperspectral sensors
KW  - Layout
KW  - Clustering algorithms
KW  - Signal processing algorithms
KW  - Pixel
KW  - Military aircraft
KW  - Gaussian distribution
KW  - Robustness
KW  - Data mining
KW  - Anomaly detection
KW  - clustering
KW  - hyperspectral data
KW  - ISODATA
KW  - $k$-means
KW  - self-organising maps
KW  - subsampling
DO  - 10.1109/TGRS.2007.909205
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 12
SN  - 1558-0644
VO  - 45
VL  - 45
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - Dec. 2007
AB  - We propose a computationally efficient method for determining anomalies in hyperspectral data. In the first stage of the algorithm, the background classes, which are the dominant classes in the image, are found. The method consists of robust clustering of a randomly chosen small percentage of the image pixels. The clusters are the representatives of the background classes. By using a subset of the pixels instead of the whole image, the computation is sped up, and the probability of including outliers in the background model is reduced. Anomalous pixels are the pixels with spectra that have large relative distances from the cluster centers. Several clustering techniques are investigated, and experimental results using realistic hyperspectral data are presented. A self-organizing map clustered using the local minima of the U-matrix (unified distance matrix) is identified as the most reliable method for background class extraction. The proposed algorithm for anomaly detection is evaluated using realistic hyperspectral data, is compared with a state-of-the-art anomaly detection algorithm, and is shown to perform significantly better.
ER  - 


TY  - JOUR
TI  - Adaptive Component Discrimination Network for Airplane Detection in Remote Sensing Images
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
SP  - 7699
EP  - 7713
AU  - H. Jia
AU  - Q. Guo
AU  - J. Chen
AU  - F. Wang
AU  - H. Wang
AU  - F. Xu
PY  - 2021
KW  - Airplanes
KW  - Feature extraction
KW  - Object detection
KW  - Task analysis
KW  - Target recognition
KW  - Detectors
KW  - Image resolution
KW  - Airplane detection and recognition
KW  - component discrimination
KW  - target orientation adaptive adjustment
DO  - 10.1109/JSTARS.2021.3098296
JO  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
IS  - 
SN  - 2151-1535
VO  - 14
VL  - 14
JA  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Y1  - 2021
AB  - Airplane detection and recognition in the high-resolution remote sensing images (RSIs) remain a challenging task due to the factors of multiple view angles, multiple scales, multiple orientations, etc. This article proposes an adaptive component discrimination network for airplane detection and recognition in RSIs, which focuses on various scales from global to local, making full use of the overall contour as well as the dominant component features of airplanes. First, a standardization processing module is proposed for image projection conversion and resolution uniform to alleviate the confusion of different types of airplanes in different resolution images. Second, the rotatable boundingbox-based pyramid network is utilized to extract candidate airplane coordinates and categories. Furthermore, an adaptive aircraft component discrimination method is established for confusing few-shot airplane targets recognition, which consists of a target orientation adaptive adjustment module (OAAM) and a component discrimination module (CDM). OAAM obtains airplanes with the same orientations by predicting the orientation of the slices and rotating them adaptively. All the uniformed slices are then fed into the CDM for dominant components detection, which corrects the target preclassification results, improving the classification performance. Experiments conducted on the 2020 Gaofen Challenge demonstrate the efficacy and superiority of the proposed method.
ER  - 


TY  - CONF
TI  - Military Based Object Detection in Satellite Imagery by Optimising YOLOv8
T2  - 2024 IEEE Space, Aerospace and Defence Conference (SPACE)
SP  - 165
EP  - 168
AU  - S. Singh
AU  - R. G N
PY  - 2024
KW  - Accuracy
KW  - Satellites
KW  - Urban planning
KW  - Object detection
KW  - Military aircraft
KW  - Real-time systems
KW  - Satellite images
KW  - Satellite Object Detection
KW  - YOLOv8
KW  - xView Dataset
KW  - mAP50
KW  - Class-Specific Variations
KW  - Dataset Augmentation
KW  - Military object detection
DO  - 10.1109/SPACE63117.2024.10667819
JO  - 2024 IEEE Space, Aerospace and Defence Conference (SPACE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE Space, Aerospace and Defence Conference (SPACE)
Y1  - 22-23 July 2024
AB  - Identifying objects from satellite images can be challenging because of their varying visible qualities, hampering their value in some scenarios such as urban planning or responses to emergencies. The ability to detect vehicles and other elements in satellite imagery can give vital information on latest military developments of adversaries, especially when planning for military operations. This study addresses the detection of military objects with more accuracy by studying YOLOv8 with optimized YOLOv8 architecture improving real-time object detection capabilities.
ER  - 


TY  - CONF
TI  - Optimal radar cross section estimation in synthetic aperture radar
T2  - 2017 IEEE First Ukraine Conference on Electrical and Computer Engineering (UKRCON)
SP  - 189
EP  - 193
AU  - V. K. Volosyuk
AU  - S. S. Zhyla
PY  - 2017
KW  - Synthetic aperture radar
KW  - Radar cross-sections
KW  - Signal processing algorithms
KW  - Radar imaging
KW  - Scattering
KW  - Surface treatment
KW  - radar cross section
KW  - synthetic aperture radar
KW  - statistical optimization
DO  - 10.1109/UKRCON.2017.8100471
JO  - 2017 IEEE First Ukraine Conference on Electrical and Computer Engineering (UKRCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 IEEE First Ukraine Conference on Electrical and Computer Engineering (UKRCON)
Y1  - 29 May-2 June 2017
AB  - Synthetic aperture radar (SAR) is a widely used technique suited for real-time and all-weather imaging of natural surfaces and artificial objects. To improve image resolution and increase accuracy of radar cross section estimation the problem of statistical synthesis of signal processing algorithm in SAR is solved. Proposed method allows to form images with super-resolution in azimuth and range. Synthesis is performed using modern theory of radio engineering systems statistical optimization.
ER  - 


TY  - CONF
TI  - Sound and dynamics of targets — Fusion technologies in radar target classification
T2  - 2008 11th International Conference on Information Fusion
SP  - 1
EP  - 7
AU  - G. Kouemou
AU  - C. Neumann
AU  - F. Opitz
PY  - 2008
KW  - Radar tracking
KW  - Hidden Markov models
KW  - Doppler effect
KW  - Target tracking
KW  - Vehicles
KW  - Bayesian methods
KW  - Support vector machine classification
DO  - 
JO  - 2008 11th International Conference on Information Fusion
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2008 11th International Conference on Information Fusion
Y1  - 30 June-3 July 2008
AB  - The challenge of modern sensor systems is besides the tracking of targets more and more their classification. The knowledge of the target class has significant influence on the identification, threat evaluation and weapon assignment process of large systems. Especially, considering new types of threats in anti asymmetric warfare the knowledge of a target class has an important drawback. Also the target class is used to optimize track and resource management of todaypsilas agile sensor systems. A technology is presented that fuses different classifiers to decide between persons, tracked vehicles, wheeled vehicles, helicopters, propeller aircrafts and clutter for a 2 dimensional, electronically scanned radar system. A first classifier analyses the Doppler sound of the target to decide its target class. Therefore, a cepstrum based feature extractor and a hidden Markov model (HMM) is applied. Similar to techniques that have been well proven in speech and image recognition, the time-varying nature of radar Doppler data is exploited. A second type of classifier extracts dynamic features of the target found by the underlying target tracking methodology. Through a fusion of these classifiers a highly reliable classification result is established.
ER  - 


TY  - CONF
TI  - Target Detection Improvement by Using a Class Decision Algorithm for Synthetic Aperture Radar
T2  - 7th European Conference on Synthetic Aperture Radar
SP  - 1
EP  - 4
AU  - M. Kartal
AU  - S. Kent
AU  - S. Kargin
PY  - 2008
KW  - Data models
KW  - Accuracy
KW  - Radar imaging
KW  - Noise measurement
KW  - Artificial neural networks
KW  - Niobium
DO  - 
JO  - 7th European Conference on Synthetic Aperture Radar
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 7th European Conference on Synthetic Aperture Radar
Y1  - 2-5 June 2008
AB  - This paper proposes a classification method to improve the target detection accuracy in a synthetic aperture radar processing algorithm. In the imaging algorithm, Fourier based processing algorithm is used to obtain the processed image from the measured 2D Cartesian backscattered frequency domain data. In case of measured data with limited frequency band and aspect angle interval, radar target detection accuracy will be reduced. Besides, the noise in measured data affects the result and thus it is hard to identify the target. In this paper, a decision rule, which is based on a classification algorithm by using neural networks, is proposed to improve the target identifi-cation accuracy by comparing the processed image with the images in the data bank. In this work, the effect of noise, frequency and aspect angle limitation in the decision accuracy are investigated and the results are pre-sented. Some other decision methods are also given to compare the results.
ER  - 


TY  - CONF
TI  - Neural Network Design Based on Differentiable Attention Module Search
T2  - 2024 43rd Chinese Control Conference (CCC)
SP  - 8643
EP  - 8648
AU  - M. Yang
AU  - Y. Zhou
AU  - H. Wang
AU  - S. Li
PY  - 2024
KW  - Deep learning
KW  - Attention mechanisms
KW  - Neural networks
KW  - Feature extraction
KW  - Search problems
KW  - Image categorization
KW  - Classification algorithms
KW  - Deep learning
KW  - Attention mechanism
KW  - Differentiable
KW  - Neural architecture search
KW  - Fine-grained classification
DO  - 10.23919/CCC63176.2024.10662130
JO  - 2024 43rd Chinese Control Conference (CCC)
IS  - 
SN  - 1934-1768
VO  - 
VL  - 
JA  - 2024 43rd Chinese Control Conference (CCC)
Y1  - 28-31 July 2024
AB  - Attention mechanisms constitute vital techniques in deep learning, where neural network architectures based on these mechanisms proficiently mine essential information from data and demonstrate robust feature extraction capabilities. The utilization of various attention mechanisms within convolutional neural networks to design more efficient architectures represents a significant research area in the field of deep learning. To address this issue, this paper proposes a neural network design based on differentiable attention module search. Firstly, this paper analyzes existing attention mechanisms and designs a multi-attention search space. Subsequently, a differentiable search strategy is employed to identify attention modules within this multi-attention search space. Finally, the proposed search algorithm is applied to fine-grained image classification tasks to verify the effectiveness of the proposed method. The experimental results show that the searched network models can fully leverage the advantages of different attention mechanisms and exhibit excellent performance on various datasets. On the CUB-200-2011, Stanford Cars, and FGVC-Aircraft fine-grained classification datasets, the proposed method achieved classification accuracies of 87.4%, 92.8%, and 94.0%, respectively.
ER  - 


TY  - CONF
TI  - Circular and Polarimetric ISAR Imaging of Ships Using Airborne SAR Sensors
T2  - EUSAR 2018; 12th European Conference on Synthetic Aperture Radar
SP  - 1
EP  - 6
AU  - S. V. Baumgartner
PY  - 2018
DO  - 
JO  - EUSAR 2018; 12th European Conference on Synthetic Aperture Radar
IS  - 
SN  - 
VO  - 
VL  - 
JA  - EUSAR 2018; 12th European Conference on Synthetic Aperture Radar
Y1  - 4-7 June 2018
AB  - With circular flight tracks ships can be observed over the full aspect angle range of 360deg with long observation times limited only by the endurance of the aircraft. Additionally to the improved probability of detection an inverse synthetic aperture radar (ISAR) image sequence of the ships as well as a sequence of high range resolution profiles (HRRP) can be obtained. The ISAR images and HRRPs can be used for target classification and recognition purposes in a post-processing step. In the paper a generic ISAR processing chain is discussed which can be used for ISAR imaging and HRRP generation for arbitrary radar platform flight tracks. Furthermore, polarimetric ISAR imaging results of a controlled police ship and of ships of opportunity are presented. The used fully polarimetric X- and L-band radar data were simultaneously acquired with DLR's airborne sensor F-SAR [9] during a North Sea flight campaign in 2016.
ER  - 


TY  - JOUR
TI  - AP-CNN: Weakly Supervised Attention Pyramid Convolutional Neural Network for Fine-Grained Visual Classification
T2  - IEEE Transactions on Image Processing
SP  - 2826
EP  - 2836
AU  - Y. Ding
AU  - Z. Ma
AU  - S. Wen
AU  - J. Xie
AU  - D. Chang
AU  - Z. Si
AU  - M. Wu
AU  - H. Ling
PY  - 2021
KW  - Task analysis
KW  - Feature extraction
KW  - Visualization
KW  - Annotations
KW  - Semantics
KW  - Proposals
KW  - Birds
KW  - Fine-grained visual classification
KW  - weakly supervised
KW  - attention pyramid
KW  - deep learning
DO  - 10.1109/TIP.2021.3055617
JO  - IEEE Transactions on Image Processing
IS  - 
SN  - 1941-0042
VO  - 30
VL  - 30
JA  - IEEE Transactions on Image Processing
Y1  - 2021
AB  - Classifying the sub-categories of an object from the same super-category (e.g., bird species and cars) in fine-grained visual classification (FGVC) highly relies on discriminative feature representation and accurate region localization. Existing approaches mainly focus on distilling information from high-level features. In this article, by contrast, we show that by integrating low-level information (e.g., color, edge junctions, texture patterns), performance can be improved with enhanced feature representation and accurately located discriminative regions. Our solution, named Attention Pyramid Convolutional Neural Network (AP-CNN), consists of 1) a dual pathway hierarchy structure with a top-down feature pathway and a bottom-up attention pathway, hence learning both high-level semantic and low-level detailed feature representation, and 2) an ROI-guided refinement strategy with ROI-guided dropblock and ROI-guided zoom-in operation, which refines features with discriminative local regions enhanced and background noises eliminated. The proposed AP-CNN can be trained end-to-end, without the need of any additional bounding box/part annotation. Extensive experiments on three popularly tested FGVC datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that our approach achieves state-of-the-art performance. Models and code are available at https://github.com/PRIS-CV/AP-CNN_Pytorch-master.
ER  - 


TY  - CONF
TI  - Visual salient sift keypoints descriptors for automatic target recognition
T2  - 2016 6th European Workshop on Visual Information Processing (EUVIP)
SP  - 1
EP  - 5
AU  - A. Karine
AU  - A. Toumi
AU  - A. Khenchaf
AU  - M. El Hassouni
PY  - 2016
KW  - Target recognition
KW  - Support vector machines
KW  - Feature extraction
KW  - Visualization
KW  - Databases
KW  - Histograms
KW  - Kernel
KW  - Automatic target recognition
KW  - inverse synthetic aperture radar
KW  - SIFT
KW  - visual attention model
KW  - classification
DO  - 10.1109/EUVIP.2016.7764596
JO  - 2016 6th European Workshop on Visual Information Processing (EUVIP)
IS  - 
SN  - 2471-8963
VO  - 
VL  - 
JA  - 2016 6th European Workshop on Visual Information Processing (EUVIP)
Y1  - 25-27 Oct. 2016
AB  - This paper addresses the problem of automatic target recognition (ATR) using inverse synthetic aperture radar (ISAR) images. In this context, we propose a novel approach for feature extraction to describe precisely an aircraft target from ISAR images. In our approach, a visual attention model is adopted to separate the salient regions from the background. After that, the scale invariant feature transform (SIFT) method is used to extract the keypoints and their descriptors. Then, a local salient feature is built by considering only the keypoints located in the salient region. For the classification step, the support vector machines (SVM) classifier is adopted. To validate the proposed approach, ISAR images database which was collected from anechoic chamber is used.
ER  - 


TY  - CONF
TI  - Imaging radar for navigation and surveillance on an autonomous unmanned ground vehicle capable of detecting obstacles obscured by vegetation
T2  - 2019 IEEE Radar Conference (RadarConf)
SP  - 1
EP  - 6
AU  - D. Gusland
AU  - B. Torvik
AU  - E. Finden
AU  - F. Gulbrandsen
AU  - R. Smestad
PY  - 2019
KW  - Vegetation mapping
KW  - Radar antennas
KW  - Radar imaging
KW  - Attenuation
KW  - Radar detection
KW  - Radar
KW  - MIMO
KW  - obstacle detection
KW  - Unmanned autonomous vehicles
DO  - 10.1109/RADAR.2019.8835514
JO  - 2019 IEEE Radar Conference (RadarConf)
IS  - 
SN  - 2375-5318
VO  - 
VL  - 
JA  - 2019 IEEE Radar Conference (RadarConf)
Y1  - 22-26 April 2019
AB  - The Norwegian Defence Research Establishment (FFI), has developed a multi purpose radar demonstrator for Intelligence, Surveillance and Reconnaissance (ISR) on the Off-road Light Autonomous Vehicle (OLAV) platform. The radar is designed to aid OLAV in navigation, detection and classification. Operating in off-road environments is a considerable challenge for autonomous vehicles, especially in the presence of vegetation. While existing perception sensors such as cameras and LIght Detection And Ranging (LiDAR) work well in clear weather and areas without vegetation, they are impaired by rain, fog, smoke and vegetation. This paper presents the development of the Multiple Usage Radar - S-band (MURA-S), developed to aid the autonomous platforms in challenging conditions. Initial considerations and frequency selection for the radar is presented in addition to a detailed explanation of the antenna configuration and the utilization of time-domain multiplexed Multiple-Input Multiple-Output (MIMO) techniques to increase the cross-range resolution of the radar. Preliminary experimental results for detecting obstacles obscured by vegetation are presented and compared with obstacle maps created by the LiDAR, showing that the radar enhances the capabilities of the perception system.
ER  - 


TY  - JOUR
TI  - Attentional Information Fusion Networks for Cross-Scene Power Line Detection
T2  - IEEE Geoscience and Remote Sensing Letters
SP  - 1635
EP  - 1639
AU  - Y. Li
AU  - Z. Xiao
AU  - X. Zhen
AU  - X. Cao
PY  - 2019
KW  - Feature extraction
KW  - Task analysis
KW  - Semantics
KW  - Visualization
KW  - Image edge detection
KW  - Aircraft
KW  - Safety
KW  - Attention mechanism
KW  - convolutional neural network (CNN)
KW  - cross-scene detection
KW  - power line detection
DO  - 10.1109/LGRS.2019.2903217
JO  - IEEE Geoscience and Remote Sensing Letters
IS  - 10
SN  - 1558-0571
VO  - 16
VL  - 16
JA  - IEEE Geoscience and Remote Sensing Letters
Y1  - Oct. 2019
AB  - The power line is one of the most hazardous obstacles for low-altitude aircrafts. As aircrafts usually encounter scenes like never before during the flight, cross-scene power line detection is the key for their flight safety. However, compared to regular object detection tasks, cross-scene power line detection is extremely challenging due to its weak visual appearance and widespread existence. In this letter, we propose a cross-scene power line detection method based on attentional information fusion networks. Specifically, we construct a fully convolutional network with attention and information fusion mechanism for cross-scene detection. The two main modules make full use of the semantic and location information, which enables the model to focus more on power lines rather than the unexpected scenes. To the best of author knowledge, our method establishes the first end-to-end convolutional architecture for pixelwise power line detection. Experimental results have shown that our method outperforms previous methods by large margins for cross-scene power line detection.
ER  - 


TY  - CONF
TI  - Fine-grained Visual Classification by Progressive Training via Jigsaw Puzzle Permutation Learning
T2  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
SP  - 155
EP  - 161
AU  - L. Ma
AU  - F. Zhao
AU  - H. Hong
AU  - J. Wu
AU  - Y. Shi
AU  - X. Li
PY  - 2021
KW  - Training
KW  - Visualization
KW  - Codes
KW  - Semantics
KW  - Signal processing
KW  - Feature extraction
KW  - Data mining
KW  - Fine-grained visual classification
KW  - progressively trained
KW  - jigsaw puzzle solver
DO  - 10.1109/CCISP52774.2021.9639255
JO  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 6th International Conference on Communication, Image and Signal Processing (CCISP)
Y1  - 19-21 Nov. 2021
AB  - Fine-grained visual classification remains a challenging task due to small inter-class differences and large intraclass differences. Existing methods consider only local details of information or only global information at a certain stage. However, there methods do not consider the complementary relationship between local information of different granularity and global information of the overall object on the feature maps of different stages of the network. To solve the above problem, we propose a Progressively Trained Jigsaw Puzzle Permutation Learning network (PPL-Net). To obtain the local details of the object, we adopt a progressive training strategy to obtain the semantic information of the object. At different steps, we use different granularity of shuffled versions of the images as input. Then, the classification prediction is performed by fusing the feature representations of different granularities. We obtain the global information of the object by learning jigsaw puzzle visual permutation. Specifically, by introducing the Jigsaw Puzzle Solver module we explore the location information of the different granularity image patches. Recovery of the shuffled version of the image to the original image. Thus, at each step of the network, we supervise the network by complementary relationships between local information of different granularities and global information of the object. Experiments on three standard fine-grained classification datasets (CUB-200-2011, Stanford Cars, FGCV-Aircraft) show that the performance of our method exceeds that of most methods. The code will be available at https://github.com/Zhao-fan/PPL-Net.
ER  - 


TY  - CONF
TI  - A fast and adaptive method for estimating UAV attitude from the visual horizon
T2  - 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems
SP  - 4935
EP  - 4940
AU  - R. J. D. Moore
AU  - S. Thurrowgood
AU  - D. Bland
AU  - D. Soccol
AU  - M. V. Srinivasan
PY  - 2011
KW  - Aircraft
KW  - Kernel
KW  - Visualization
KW  - Training
KW  - Image color analysis
KW  - Machine vision
KW  - Vectors
DO  - 10.1109/IROS.2011.6094631
JO  - 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems
Y1  - 25-30 Sept. 2011
AB  - This study describes a novel method for automatically obtaining the attitude of an aircraft from the visual horizon. A wide-angle view of the environment, including the visual horizon, is captured and the input images are classified into fuzzy sky and ground regions using the spectral and intensity properties of the pixels. The classifier is updated continuously using an online reinforcement strategy and is therefore able to adapt to the changing appearance of the sky and ground, without requiring prior training offline. A novel approach to obtaining the attitude of the aircraft from the classified images is described, which is reliable, accurate, and computationally efficient to implement. This method is therefore suited to real-time operation and we present results from flight tests that demonstrate the ability of this vision-based approach to outperform an inexpensive inertial system.
ER  - 


TY  - CONF
TI  - Toward aircraft recognition with convolutional neural networks
T2  - 2016 IEEE National Aerospace and Electronics Conference (NAECON) and Ohio Innovation Summit (OIS)
SP  - 225
EP  - 232
AU  - R. Mash
AU  - N. Becherer
AU  - B. Woolley
AU  - J. Pecarina
PY  - 2016
KW  - Neurons
KW  - Kernel
KW  - Training
KW  - Biological neural networks
KW  - Visualization
KW  - Pattern recognition
KW  - Machine learning
KW  - CNN
KW  - Deep Learning
KW  - Aerial Refueling
KW  - Automatic Aircraft Identification
DO  - 10.1109/NAECON.2016.7856803
JO  - 2016 IEEE National Aerospace and Electronics Conference (NAECON) and Ohio Innovation Summit (OIS)
IS  - 
SN  - 2379-2027
VO  - 
VL  - 
JA  - 2016 IEEE National Aerospace and Electronics Conference (NAECON) and Ohio Innovation Summit (OIS)
Y1  - 25-29 July 2016
AB  - We summarize the history and state of the art in Convolutional Neural Networks (CNNs), which constitute a significant advancement in pattern recognition. As a demonstration of capability, we address the problem of automatic aircraft identification during refueling approach. In this paper we describe the history of CNN development and provide a high level overview of the state of the art and a summary of leading CNN libraries with CUDA support. Finally, we demonstrate an application of CNN technology to autonomous aerial refueling and identify areas of follow-on research.
ER  - 


TY  - CONF
TI  - VegFru: A Domain-Specific Dataset for Fine-Grained Visual Categorization
T2  - 2017 IEEE International Conference on Computer Vision (ICCV)
SP  - 541
EP  - 549
AU  - S. Hou
AU  - Y. Feng
AU  - Z. Wang
PY  - 2017
KW  - Taxonomy
KW  - Training
KW  - Visualization
KW  - Birds
KW  - Computer vision
KW  - Dogs
DO  - 10.1109/ICCV.2017.66
JO  - 2017 IEEE International Conference on Computer Vision (ICCV)
IS  - 
SN  - 2380-7504
VO  - 
VL  - 
JA  - 2017 IEEE International Conference on Computer Vision (ICCV)
Y1  - 22-29 Oct. 2017
AB  - In this paper, we propose a novel domain-specific dataset named VegFru for fine-grained visual categorization (FGVC). While the existing datasets for FGVC are mainly focused on animal breeds or man-made objects with limited labelled data, VegFru is a larger dataset consisting of vegetables and fruits which are closely associated with the daily life of everyone. Aiming at domestic cooking and food management, VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class. Accompanying the dataset, we also propose an effective framework called HybridNet to exploit the label hierarchy for FGVC. Specifically, multiple granularity features are first extracted by dealing with the hierarchical labels separately. And then they are fused through explicit operation, e.g., Compact Bilinear Pooling, to form a unified representation for the ultimate recognition. The experimental results on the novel VegFru, the public FGVC-Aircraft and CUB-200-2011 indicate that HybridNet achieves one of the top performance on these datasets. The dataset and code are available at https://github.com/ustc-vim/vegfru.
ER  - 


TY  - CONF
TI  - Problems of a trajectory planning in autonomous navigation systems based on technical vision and AI
T2  - 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
SP  - 1032
EP  - 1035
AU  - A. A. Zhilenkov
AU  - I. R. Epifantsev
PY  - 2018
KW  - Robots
KW  - Optical sensors
KW  - Optical imaging
KW  - Trajectory
KW  - Image recognition
KW  - Optical network units
KW  - Neural networks
KW  - motion capture
KW  - dynamic images recognition
KW  - deep learning
DO  - 10.1109/EIConRus.2018.8317265
JO  - 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)
Y1  - 29 Jan.-1 Feb. 2018
AB  - Problems of realization of completely autonomous systems of navigation are considered in a set of spheres of human activity today. The most important task is today the attempt to create on the basis of system of technical sight completely autonomous system of navigation for motor transport, the marine transport and aircraft. In article the problem of creation of such system for the drone which is carrying out research or rescue operations on the difficult area is considered. And it is concrete in the wood. The main objective of system is finding of footpaths among trees on which the drone can follow. The main tool of the solution of this problem offers use of artificial neural networks of deep training or convolution networks. The quantity of layers and dimensions of local networks at their use is proved in problems of autonomous navigation of drones on the basis of systems of technical sight.
ER  - 


TY  - CONF
TI  - Boosting Aircraft Monitoring and Security through Ground Surveillance Optimization with YOLOv9
T2  - 2024 12th International Symposium on Digital Forensics and Security (ISDFS)
SP  - 1
EP  - 6
AU  - M. Bakirci
AU  - I. Bayraktar
PY  - 2024
KW  - Training
KW  - Satellites
KW  - Surveillance
KW  - Digital forensics
KW  - Low earth orbit satellites
KW  - Object detection
KW  - Security
KW  - YOLOv9
KW  - aircraft security
KW  - satellite imagery
KW  - object detection
KW  - deep neural network
DO  - 10.1109/ISDFS60797.2024.10527349
JO  - 2024 12th International Symposium on Digital Forensics and Security (ISDFS)
IS  - 
SN  - 2768-1831
VO  - 
VL  - 
JA  - 2024 12th International Symposium on Digital Forensics and Security (ISDFS)
Y1  - 29-30 April 2024
AB  - The integration of object detection algorithms into aircraft tracking and ground surveillance systems presents a myriad of security advantages, bolstering the protection of critical infrastructure. These algorithms are instrumental in enforcing access control measures by continuously monitoring and discerning between authorized and unauthorized access to parked aircraft. With ongoing refinements, they serve as crucial components of intrusion detection systems, promptly alerting security personnel to any suspicious or unauthorized activities in the vicinity of grounded aircraft. Effective training of detection algorithms enhances their analytical capabilities, enabling them to discern between routine operations and security-threatening situations with greater precision. Notably, one of the pivotal applications lies in supporting digital forensic investigations, as these algorithms provide detailed activity logs, facilitating comprehensive post-incident analyses and bolstering forensic efforts to understand security incidents or breaches. In this investigation, we assessed the efficacy of the YOLOv9 detection algorithm in identifying aircraft situated on the ground surface. Furthermore, we highlight the significance of satellite imagery in dataset acquisition for object detection algorithms, particularly emphasizing the role of Low-Earth-Orbit (LEO) satellites in real-time image acquisition. Through this comprehensive analysis, we underscore the pivotal role of YOLOv9 in enhancing security measures and compliance with aviation security standards and regulations, ultimately fortifying the security posture within aviation environments.
ER  - 


TY  - CONF
TI  - Importance-weighted multi-scale texture and shape descriptor for object recognition in satellite imagery
T2  - 2012 IEEE International Geoscience and Remote Sensing Symposium
SP  - 79
EP  - 82
AU  - G. J. Scott
AU  - D. T. Anderson
PY  - 2012
KW  - Feature extraction
KW  - Remote sensing
KW  - Context
KW  - Object recognition
KW  - Support vector machines
KW  - Databases
KW  - Image segmentation
KW  - Object recognition
KW  - feature weighting
KW  - texture and shape descriptors
KW  - satellite imagery
DO  - 10.1109/IGARSS.2012.6351632
JO  - 2012 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - 2012 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 22-27 July 2012
AB  - We present a sliding window-based, per-pixel importance-weighted, multi-scale, cell-structured feature descriptor and demonstrate its performance for recognizing different aircraft from remotely sensed imagery. Opening and closing differential morphological profiles are constructed, then fused with the Choquet integral to create a soft segmentation. A per-pixel importance map is derived from the soft segmentation and used in the calculation of histogram of oriented gradients, local binary patterns, invariant object moments, and Haar-like features. Superiority is demonstrated in comparison to flat single-scale and non-importance weighted representations with encouraging results for both cross-validation and blind testing. Results show that the pyramid, cell-structured, importance-weighting performs better than traditional approaches in the difficult problem space of recognizing objects in remote sensing imagery.
ER  - 


TY  - CONF
TI  - Aircraft ground monitoring with high performance computing multicore enabled video tracking
T2  - 2014 IEEE/AIAA 33rd Digital Avionics Systems Conference (DASC)
SP  - 6B2-1
EP  - 6B2-9
AU  - B. Jia
AU  - H. Ling
AU  - E. Blasch
AU  - C. Sheaff
AU  - G. Chen
AU  - Z. Wang
PY  - 2014
KW  - Target tracking
KW  - Feature extraction
KW  - Surveillance
KW  - Algorithm design and analysis
KW  - Multicore processing
KW  - Vehicles
KW  - Context
DO  - 10.1109/DASC.2014.6979498
JO  - 2014 IEEE/AIAA 33rd Digital Avionics Systems Conference (DASC)
IS  - 
SN  - 2155-7209
VO  - 
VL  - 
JA  - 2014 IEEE/AIAA 33rd Digital Avionics Systems Conference (DASC)
Y1  - 5-9 Oct. 2014
AB  - Safety of aircraft requires resiliency from onboard and off-board threats that could come from natural or man-made disturbances. Maintaining situation awareness requires monitoring and coordination of threats from the air and the ground. Threat detection of people, vehicles, and person-vehicle interactions of possible harm to an aircraft operations is a difficult problem due to the complexity of the coverage area, varying sensor capabilities (e.g., resolutions), and cultural factors of disruption. Methods and techniques can be incorporated to aid analysts (e.g., airport traffic controllers) to track and identify entities using modern large scale visual sensors such as the Wide Area Motion Imagery (WAMI) systems. Such systems typically produce an overwhelmingly large amount of information. The lack of computationally efficient algorithms has become a bottleneck for utilizing WAMI data in surveillance. To facilitate the application of such surveillance system development for safe operations, in this paper, three different strategies are implemented based on the on-board multicore technology to speed up video tracking algorithms. A complete tool chain to implement the video tracking, such as registration, detection, and multiple target association, is presented. Experimental results are illustrated. We demonstrate aircraft ground monitoring using the Columbus Large Image Format (CLIF) dataset to show the performance improvement by using the proposed high performance computing enabled video tracking algorithm to facilitate safe air travel.
ER  - 


TY  - JOUR
TI  - Fine-Grained Recognition With Learnable Semantic Data Augmentation
T2  - IEEE Transactions on Image Processing
SP  - 3130
EP  - 3144
AU  - Y. Pu
AU  - Y. Han
AU  - Y. Wang
AU  - J. Feng
AU  - C. Deng
AU  - G. Huang
PY  - 2024
KW  - Data augmentation
KW  - Semantics
KW  - Training
KW  - Visualization
KW  - Metalearning
KW  - Covariance matrices
KW  - Task analysis
KW  - Fine-grained recognition
KW  - data augmentation
KW  - meta-learning
KW  - deep learning
DO  - 10.1109/TIP.2024.3364500
JO  - IEEE Transactions on Image Processing
IS  - 
SN  - 1941-0042
VO  - 33
VL  - 33
JA  - IEEE Transactions on Image Processing
Y1  - 2024
AB  - Fine-grained image recognition is a longstanding computer vision challenge that focuses on differentiating objects belonging to multiple subordinate categories within the same meta-category. Since images belonging to the same meta-category usually share similar visual appearances, mining discriminative visual cues is the key to distinguishing fine-grained categories. Although commonly used image-level data augmentation techniques have achieved great success in generic image classification problems, they are rarely applied in fine-grained scenarios, because their random editing-region behavior is prone to destroy the discriminative visual cues residing in the subtle regions. In this paper, we propose diversifying the training data at the feature-level to alleviate the discriminative region loss problem. Specifically, we produce diversified augmented samples by translating image features along semantically meaningful directions. The semantic directions are estimated with a covariance prediction network, which predicts a sample-wise covariance matrix to adapt to the large intra-class variation inherent in fine-grained images. Furthermore, the covariance prediction network is jointly optimized with the classification network in a meta-learning manner to alleviate the degenerate solution problem. Experiments on four competitive fine-grained recognition benchmarks (CUB-200-2011, Stanford Cars, FGVC Aircrafts, NABirds) demonstrate that our method significantly improves the generalization performance on several popular classification networks (e.g., ResNets, DenseNets, EfficientNets, RegNets and ViT). Combined with a recently proposed method, our semantic data augmentation approach achieves state-of-the-art performance on the CUB-200-2011 dataset. Source code is available at https://github.com/LeapLabTHU/LearnableISDA.
ER  - 


TY  - CONF
TI  - Aerial Thermal Image based Convolutional Neural Networks for Human Detection in SubT Environments
T2  - 2021 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 536
EP  - 541
AU  - A. Koval
AU  - S. S. Mansouri
AU  - C. Kanellakis
AU  - G. Nikolakopoulos
PY  - 2021
KW  - Training
KW  - Transfer learning
KW  - Streaming media
KW  - Cameras
KW  - Computational efficiency
KW  - Convolutional neural networks
KW  - Feeds
DO  - 10.1109/ICUAS51884.2021.9476853
JO  - 2021 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2021 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 15-18 June 2021
AB  - This article proposes a novel strategy for detecting humans in harsh Sub-terranean (SubT) environments, with a thermal camera mounted on an aerial platform, based on the AlexNet Convolutional Neural Network (CNN). A transfer learning framework will be utilized for detecting the humans, where the aerial thermal images are fed to the trained network, which binary classifies them image content into two categories: a) human, and b) no human. Moreover, the AlexNet based framework is compared with two related popular CNN approaches as the GoogleNet and the Inception3Net. The efficacy of the proposed scheme has been experimentally evaluated through multiple data-sets, collected from a FLIR thermal camera during flights on an underground mining environment, fully demonstrating the performance and merits of the proposed module.
ER  - 


TY  - CONF
TI  - FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene Reconstruction
T2  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1050
EP  - 1055
AU  - M. Dronova
AU  - V. Cheremnykh
AU  - A. Kotcov
AU  - A. Fedoseev
AU  - D. Tsetserukou
PY  - 2024
KW  - Three-dimensional displays
KW  - PSNR
KW  - Surveillance
KW  - Neural networks
KW  - Image capture
KW  - Autonomous aerial vehicles
KW  - Quality assessment
DO  - 10.1109/ICUAS60882.2024.10556906
JO  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2024 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 4-7 June 2024
AB  - Current methods for 3D reconstruction and environmental mapping frequently face challenges in achieving high precision, highlighting the need for practical and effective solutions. In response to this issue, our study introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with drone-based data acquisition for high-quality 3D reconstruction. Utilizing unmanned aerial vehicles (UAV) for capturing images and corresponding spatial coordinates, the obtained data is subsequently used for the initial NeRF-based 3D reconstruction of the environment. Further evaluation of the reconstruction render quality is accomplished by the image evaluation neural network developed within the scope of our system. Depending on the results of the image evaluation module, our algorithm determines the position for additional image capture, thereby improving the reconstruction quality. The neural network introduced for render quality assessment demonstrates an accuracy of 97%. Furthermore, our adaptive methodology enhances the overall reconstruction quality, resulting in an average improvement of 2.5 dB in Peak Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates promising results, offering advancements in such fields as environmental monitoring, surveillance, and reconstruction of digital twins, where high-fidelity 3D reconstructions are crucial.
ER  - 


TY  - CONF
TI  - The recent advances of data imaging and fusion processing for airborne X-SAR with high resolution
T2  - 2016 Progress in Electromagnetic Research Symposium (PIERS)
SP  - 2843
EP  - 2848
AU  - Ting Shen
AU  - Jun Li
AU  - Zhirui Wang
AU  - Lei Huang
AU  - Liwei Li
AU  - Ping Zhang
PY  - 2016
KW  - Spatial resolution
KW  - Synthetic aperture radar
KW  - Radiometry
KW  - Imaging
KW  - Radar imaging
KW  - Calibration
DO  - 10.1109/PIERS.2016.7735138
JO  - 2016 Progress in Electromagnetic Research Symposium (PIERS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2016 Progress in Electromagnetic Research Symposium (PIERS)
Y1  - 8-11 Aug. 2016
AB  - X-SAR system is the airborne imaging radar with multi-mode synthetic aperture radar (SAR) at high-resolution, interferometer and full-polarization, which has been developed by the Institute of Remote Sensing and Digital Earth (RADI), Chinese Academy of Sciences (CAS), funded by the CAS Large Research Infrastructures. The first-stage form 2009 to 2015, X-SAR was successfully implemented to an operational SAR in X-band with high resolution (up to 0.5 m).The system performances and data imaging quality have verified by the flight tests. Many valuable results of the visual interpretation in typical images, particularly SAR image fusion processing have emphasized the X-SAR's target recognition capabilities. This paper presents the core characteristics of X-SAR images, having achieved by the spatial resolution optimized by low side-lobe, exact geographical precision and radiometric accuracy. The visual inspection of typical targets in example images is described such as the surface of desert hill, the vehicle discrimination and aircraft recognition. Meanwhile, the image fusion processing for target recognition has been implemented. The recent advances of SAR-optical image fusion used to target classification and SAR-SAR image fusion processing for change detection are also presented.
ER  - 


TY  - CONF
TI  - RCPNet: Deep-Learning based Relative Camera Pose Estimation for UAVs
T2  - 2020 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1085
EP  - 1092
AU  - C. Yang
AU  - Y. Liu
AU  - A. Zell
PY  - 2020
KW  - Cameras
KW  - Pose estimation
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Robot vision systems
KW  - Visualization
KW  - Training
DO  - 10.1109/ICUAS48674.2020.9214000
JO  - 2020 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2020 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 1-4 Sept. 2020
AB  - In this paper, we propose a deep neural-network based regression approach, combined with a 3D structure based computer vision method, to solve the relative camera pose estimation problem for autonomous navigation of UAVs. Different from existing learning-based methods that train and test camera pose estimation in the same scene, our method succeeds in estimating relative camera poses across various urban scenes via a single trained model. We also built a Tuebingen Buildings database of RGB images collected by a drone in eight urban scenes. Over 10,000 images with corresponding 6DoF poses as well as 300,000 image pairs with their relative translational and rotational information are included in the dataset. We evaluate the accuracy of our method in the same scene and across scenes, using the Cambridge Landmarks dataset and the Tuebingen Buildings dataset. We compare the performance with existing learning-based pose regression methods PoseNet and RPNet on these two benchmark datasets.
ER  - 


TY  - JOUR
TI  - GCWNet: A Global Context-Weaving Network for Object Detection in Remote Sensing Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 12
AU  - Y. Wu
AU  - K. Zhang
AU  - J. Wang
AU  - Y. Wang
AU  - Q. Wang
AU  - X. Li
PY  - 2022
KW  - Object detection
KW  - Remote sensing
KW  - Feature extraction
KW  - Task analysis
KW  - Semantics
KW  - Proposals
KW  - Convolution
KW  - Deep learning
KW  - feature enhancement
KW  - global context
KW  - object detection
KW  - remote sensing images
DO  - 10.1109/TGRS.2022.3155899
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 60
VL  - 60
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2022
AB  - With practical applications such as environment surveillance, agricultural production, and disaster assessment, accurate object detection in remote sensing images is in high demand. Precise detection of object instances in remote sensing images remains considerably challenging due to dense instance stacking, large-scale variations, and complex backgrounds. To solve the mentioned issues, a novel global context-weaving network (GCWNet) is developed for object detection in remote sensing images. We propose two novel modules for feature extraction and refinement, which include the global context aggregation module (GCAM) and the feature refinement module (FRM). GCAM assembles a global context with high-level and low-level features through feature weaving, which facilitates dense object detection. Meanwhile, FRM convolves multiple receptive fields by combining different branches, thereby further refining the features and improving the feature distinction at different scales. Furthermore, we design to alleviate the sample imbalanced problem during training using focal loss and balanced L1 loss to improve object classification and regression, respectively. The experimental results indicate that GCWNet achieves superior performance in object classification and localization on the DOTA-v1.5 dataset, which illustrates the superiority of GCWNet.
ER  - 


TY  - CONF
TI  - Direction of Arrival Assessment in Airborne Ice-Sounding Synthetic Aperture Radar
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
SP  - 1148
EP  - 1151
AU  - A. Arenas-Pingarron
AU  - P. V. Brennan
AU  - H. Corr
PY  - 2019
KW  - Direction-of-arrival estimation
KW  - Estimation
KW  - Antennas
KW  - Sea surface
KW  - Calibration
KW  - Aircraft
KW  - Multiple signal classification
KW  - Direction-of-arrival estimation
KW  - ice-sounding
KW  - MIMO radar
KW  - MUSIC
KW  - SAR
DO  - 10.1109/IGARSS.2019.8898009
JO  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
IS  - 
SN  - 2153-7003
VO  - 
VL  - 
JA  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
Y1  - 28 July-2 Aug. 2019
AB  - We propose an algorithm to evaluate the direction of arrival (DoA) estimation in the cross-track dimension of an airborne pulsed ice-sounding synthetic aperture radar (SAR), formed by a non-linear array. The conventional methods process either simulated data, meaning a lack of real scenario, or collected over regions previously mapped, where a limited number of DoAs is found. In our technique, we combine the echoes of different transmitted pulses from a real data take above the sea surface, planned to vary the roll angle of the aircraft. Due to the mirror-like behavior of the sea, when echoes at different roll angles are summed, we obtain raw data or images with several DoAs, relative to the main axes of the aircraft. This method is used to assess the estimation algorithms and choose the optimal for Antarctic bedrock 3D-imaging, identifying the DoAs to shape the true bed topography.
ER  - 


TY  - JOUR
TI  - The Joint Service Imagery Processing System (JSIPS)
T2  - IEEE Aerospace and Electronic Systems Magazine
SP  - 12
EP  - 36
AU  - R. L. Davis
PY  - 1992
KW  - Space technology
KW  - Image processing
KW  - Intelligent sensors
KW  - Isolation technology
KW  - Image storage
KW  - Digital images
KW  - Image sensors
KW  - Sensor phenomena and characterization
KW  - Sensor systems
KW  - Real time systems
DO  - 10.1109/62.177446
JO  - IEEE Aerospace and Electronic Systems Magazine
IS  - 12
SN  - 1557-959X
VO  - 7
VL  - 7
JA  - IEEE Aerospace and Electronic Systems Magazine
Y1  - Dec. 1992
AB  - A description is given of a modular, tactically deployable image processing system, capable of receiving, processing, exploiting, and disseminating digital imagery from multiple sensor types and sources and providing intelligence reports to the field commander in an average of 15 min from the receipt of real-time imagery. Most JSIPS systems are packaged in 8*8*10-ft. or 8*8*20-ft. shelters that are transportable by truck, rail, ship, aircraft, or helicopter. Since tactically deployable image processing systems demand high data rates and large data storage capacities in very small spaces, key JSIPS technologies include near lossless image compression, high-speed LANs, and a storage hierarchy with the highest bits/in/sup 3/ densities possible. Other key technologies are shock isolation to meet stringent transportability requirements and a security design that allows the system to operate in a classified environment while connected to communication links operating at lower classification levels.<>
ER  - 


TY  - CONF
TI  - Resilient Communication, Object Classification and Data Fusion in Unmanned Aerial Systems
T2  - 2018 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 779
EP  - 787
AU  - J. A. Allison
AU  - R. Ptucha
AU  - S. E. Lyshevski
PY  - 2018
KW  - Silicon
KW  - Intelligent sensors
KW  - Task analysis
KW  - Sensor fusion
KW  - Kernel
KW  - Electronic warfare
DO  - 10.1109/ICUAS.2018.8453309
JO  - 2018 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 2575-7296
VO  - 
VL  - 
JA  - 2018 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 12-15 June 2018
AB  - This paper researches robust data fusion, descriptive reality and control solutions. Cognitive autonomy, vehicle-to-vehicle and vehicle-to-ground communication, as well as wireless hybrid mobile mesh networks are considered. In dynamic and contested environments, the nodes and autonomous swarms must transmit and assess states, evaluate shared data, adapt to dynamic countermeasures, reconfigure, and, ensure secure communication. Robust algorithms and protocols must accomplish data management, perform vulnerability analysis, support data trustworthiness assessment, support physical-virtual reality, and evaluate functionality. Reliability of information sources and communication should be guaranteed despite adversary electronic warfare and disruptive actions. We examine science and technology of communication and data management in distributed processing, control and autonomy applications typifying adversarial environments. Our results ensure sufficient throughput, low latency, robustness, connectivity and secure communication. Topics on target recognition, object classification and dynamic tracking are researched with experimental substantiations.
ER  - 


TY  - CONF
TI  - Self-supervised segmentation of river scenes
T2  - 2011 IEEE International Conference on Robotics and Automation
SP  - 6227
EP  - 6232
AU  - S. Achar
AU  - B. Sankaran
AU  - S. Nuske
AU  - S. Scherer
AU  - S. Singh
PY  - 2011
KW  - Rivers
KW  - Image segmentation
KW  - Image color analysis
KW  - Training
KW  - Support vector machines
KW  - Roads
KW  - Labeling
DO  - 10.1109/ICRA.2011.5980157
JO  - 2011 IEEE International Conference on Robotics and Automation
IS  - 
SN  - 1050-4729
VO  - 
VL  - 
JA  - 2011 IEEE International Conference on Robotics and Automation
Y1  - 9-13 May 2011
AB  - Here we consider the problem of automatically segmenting images taken from a boat or low-flying aircraft. Such a capability is important for autonomous river following and mapping. The need for accurate segmentation in a wide variety of riverine environments challenges the state of the art vision-based methods that have been used in more structured environments such as roads and highways. Apart from the lack of structure, the principal difficulty is the large spatial and temporal variations in the appearance of water in the presence of nearby vegetation and with reflections from the sky. We propose a self-supervised method to segment images into ‘sky’, ‘river’ and ‘shore’ (vegetation + structures) regions. Our approach uses assumptions about river scene structure to learn appearance models based on features like color, texture and image location which are used to segment the image. We validated our algorithm by testing on four datasets captured under varying conditions on different rivers. Our self-supervised algorithm had higher accuracy rates than a supervised alternative, often significantly more accurate, and does not need to be retrained to work under different conditions.
ER  - 


TY  - JOUR
TI  - Two-Stage Convolutional Neural Network for Ship and Spill Detection Using SLAR Images
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 5217
EP  - 5230
AU  - M. Nieto-Hidalgo
AU  - A. -J. Gallego
AU  - P. Gil
AU  - A. Pertusa
PY  - 2018
KW  - Oils
KW  - Marine vehicles
KW  - Synthetic aperture radar
KW  - Sensors
KW  - Aircraft
KW  - Task analysis
KW  - Feature extraction
KW  - Neural networks
KW  - oil spill detection
KW  - radar detection
KW  - side-looking airborne radar (SLAR)
KW  - supervised learning
DO  - 10.1109/TGRS.2018.2812619
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 9
SN  - 1558-0644
VO  - 56
VL  - 56
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - Sept. 2018
AB  - This paper presents a system for the detection of ships and oil spills using side-looking airborne radar (SLAR) images. The proposed method employs a two-stage architecture composed of three pairs of convolutional neural networks (CNNs). Each pair of networks is trained to recognize a single class (ship, oil spill, and coast) by following two steps: a first network performs a coarse detection, and then, a second specialized CNN obtains the precise localization of the pixels belonging to each class. After classification, a postprocessing stage is performed by applying a morphological opening filter in order to eliminate small look-alikes, and removing those oil spills and ships that are surrounded by a minimum amount of coast. Data augmentation is performed to increase the number of samples, owing to the difficulty involved in obtaining a sufficient number of correctly labeled SLAR images. The proposed method is evaluated and compared to a single multiclass CNN architecture and to previous state-of-the-art methods using accuracy, precision, recall, F-measure, and intersection over union. The results show that the proposed method is efficient and competitive, and outperforms the approaches previously used for this task.
ER  - 


TY  - JOUR
TI  - Boosting Adversarial Transferability by Batchwise Amplitude Spectrum Normalization
T2  - IEEE Transactions on Geoscience and Remote Sensing
SP  - 1
EP  - 14
AU  - Q. Dang
AU  - T. Zhan
AU  - M. Gong
AU  - X. He
PY  - 2025
KW  - Perturbation methods
KW  - Frequency-domain analysis
KW  - Closed box
KW  - Semantic segmentation
KW  - Remote sensing
KW  - Semantics
KW  - Noise
KW  - Glass box
KW  - Artificial neural networks
KW  - Training
KW  - Adversarial attack
KW  - adversarial transferability
KW  - amplitude normalization
KW  - black-box attack
KW  - remote sensing images
DO  - 10.1109/TGRS.2025.3535697
JO  - IEEE Transactions on Geoscience and Remote Sensing
IS  - 
SN  - 1558-0644
VO  - 63
VL  - 63
JA  - IEEE Transactions on Geoscience and Remote Sensing
Y1  - 2025
AB  - We consider the black-box adversarial attack problem in the field of remote sensing images (RSIs) to reveal the vulnerabilities of various deep neural networks (DNNs), including classification and semantic segmentation models. Existing adversarial attack methods typically focus solely on maximizing attack success rates (ASRs) under given perturbation constraints, neglecting the differences between adversarial samples and clean images. We propose a batchwise amplitude spectrum normalization (BAMPN) method, which is a plug-and-play and transfer-based black-box attack strategy. Using Fourier transform, we convert RSIs from the spatial domain into the frequency domain to obtain the amplitude spectrum, which is normalized within the batch. Moreover, we use a moving average strategy to retain the historical amplitudes of the batch, enhancing input diversity. By mixing the low-level statistical features of RSIs, BAMPN reduces the differences between adversarial samples and clean images while improving attack transferability. In addition, BAMPN is applicable not only to RSIs classification tasks but also directly to semantic segmentation tasks, as it preserves the spatial semantic structure of RSIs. We conduct extensive experiments using 20 DNN models and four benchmark RSIs’ datasets, comparing our method against 14 state-of-the-art (SOTA) approaches. The results demonstrate that BAMPN achieves superior attack performance while ensuring a promising similarity between adversarial and clean samples.
ER  - 


TY  - CONF
TI  - New high resolution SAR modes for an airborne maritime patrol radar — Implementation and results
T2  - 2013 Signal Processing Symposium (SPS)
SP  - 1
EP  - 4
AU  - D. Gromek
AU  - P. Samczyński
AU  - J. Misiurewicz
AU  - M. Malanowski
AU  - K. Kulpa
AU  - A. Gromek
AU  - A. Gadoś
AU  - A. Jarzębska
AU  - M. Smolarczyk
PY  - 2013
KW  - Synthetic aperture radar
KW  - Image resolution
KW  - Signal resolution
KW  - Radar imaging
KW  - Radar antennas
KW  - Airborne radar
KW  - high resolution SAR
KW  - real-time SAR imaging
KW  - ISAR
KW  - surveillance Radar
DO  - 10.1109/SPS.2013.6623571
JO  - 2013 Signal Processing Symposium (SPS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2013 Signal Processing Symposium (SPS)
Y1  - 5-7 June 2013
AB  - The paper presents new high resolution SAR results of real-life measurements using an updated ARS-400/ARS-800 SAR sensor installed on the maritime patrol aircraft M-28. The main role for such radars is surveying the sea surface, and the imaging of selected targets (e.g. ships, roads, vehicles, buildings, etc.) to help the operator in classifying them. In the present day increasing computing power, improved algorithms and general technological progress has allowed the obtaining of better results in SAR imagery.
ER  - 


TY  - CONF
TI  - Peanut Seed Germination Detection from Aerial Images
T2  - 2022 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
SP  - 1
EP  - 6
AU  - S. Ma
AU  - Y. Zhou
AU  - K. C. Flynn
AU  - S. N. Aakur
PY  - 2022
KW  - Computational modeling
KW  - Oils
KW  - Object detection
KW  - Cameras
KW  - Real-time systems
KW  - Seeds (agriculture)
KW  - Sensors
KW  - peanut germination
KW  - remote sensing
KW  - deep learning
KW  - object detection
KW  - UAS
DO  - 10.1109/AIPR57179.2022.10092219
JO  - 2022 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
IS  - 
SN  - 2332-5615
VO  - 
VL  - 
JA  - 2022 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)
Y1  - 11-13 Oct. 2022
AB  - Peanut is an essential economic oil crop around the world. Therefore, accurate and real-time detection of peanut seed germination is necessary for peanut field management. However, traditional peanut seedlings’ germination monitoring is time-consuming and labor intense, especially for large fields. In this work, we propose to reduce the time lag in detecting peanut germination failures by combining the power of deep learning-based object detection (OD) and unmanned aerial systems (UAS) to identify early in-field peanut germination. To find the most suitable object detection model, we first compared the performance of two representative OD models, Faster RCNN and SSD, to identify peanut seedlings from UAS imagery obtained through a multispectral camera setup (MicaSense Rededge). The results showed that the F1 score of the SSD model is 0.82, while it is 0.85 for Faster RCNN at an Intersection over Union (IoU) of 0.5. Through extensive ablations, we find that deeper models only marginally improved the performance but were more expensive in computation and inference times. Interestingly we find that the performance of RGB-based seedling detection (0.917 mAP) is comparable to that of R-RedEdge-NIR (0.919 mAP), indicating that a remote sensing setup with a regular RGB camera can perform as well as a more expensive, multispectral camera system to detect peanut seedlings. With extensive experimentation, we infer that cheaper remote sensing mechanisms with the rapid acquisition of UAS-based imagery and the efficiency of OD methods are practical for early peanut germination detection.
ER  - 


TY  - CONF
TI  - Detection Of Oil Spill in Satellite-Based Synthetic Aperture Radar Images by Neural Network
T2  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
SP  - 1
EP  - 6
AU  - T. Raju
AU  - A. Narayan
AU  - A. R
AU  - C. Joseph
PY  - 2024
KW  - Training
KW  - Technological innovation
KW  - Accuracy
KW  - Oils
KW  - Surveillance
KW  - Neural networks
KW  - Supervised learning
KW  - Radar polarimetry
KW  - Synthetic aperture radar
KW  - Remote sensing
KW  - Machine Learning
KW  - Neural Networks
KW  - Synthetic Radar Aperture (SAR)
KW  - MATLAB
DO  - 10.1109/ASIANCON62057.2024.10837956
JO  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th Asian Conference on Innovation in Technology (ASIANCON)
Y1  - 23-25 Aug. 2024
AB  - A combination of airborne and satellite-based remote sensing techniques is used to monitor oil spills globally. Satellite-based synthetic aperture radar (SAR) imagery offers a broad view of extensive ocean areas. Along with surveillance aircrafts, SAR imagery is used to accurately detect the region of oil-spillage. SAR imaging stands out for its effectiveness in detecting oil spills on a large scale, due to its all-weather capabilities and extensive scope. Neural network methodology has been utilized to identify oil spill percentage from SAR images.
ER  - 


TY  - CONF
TI  - A Convolutional Neural Network-based Approach For Image Analysis and Injection Detection
T2  - 2024 IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)
SP  - 1
EP  - 6
AU  - C. Titouna
AU  - F. Naït-Abdesselam
PY  - 2024
KW  - Interpolation
KW  - Smart cities
KW  - Surveillance
KW  - Streaming media
KW  - Convolutional neural networks
KW  - Proposals
KW  - Protection
KW  - Unmanned Aerial Vehicles
KW  - Convolutional Neural Network
KW  - False Data Injection
DO  - 10.1109/AVSS61716.2024.10672581
JO  - 2024 IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)
IS  - 
SN  - 2643-6213
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)
Y1  - 15-16 July 2024
AB  - Due to their usefulness in smart cities and other civilian uses, drones are becoming increasingly popular. With the ability to be organized into networks, they can be used to gather various kinds of data, including images and videos with multimedia characteristics, and then forward it to processing centers for additional handling. They also become a fresh target for a variety of attacks, such as GPS spoofing, denial of service, and false data injection. It is therefore vital and required to create new systems and protection mechanisms against these threats. In this research, we highlight the risk associated with the so-called False Data Injection (FDI) and present a deep learning-based approach for detecting it. An injection of misleading data into the data (images) gathered by the drones is regarded as a serious and potent attack that has the potential to significantly change a final judgment made by the processing center. Our strategy uses deep learning for image analysis and classification in order to thwart this attack. Using Nearest Neighbor Interpolation (NNI) to scale the incoming image to match the classifier, we next feed the image to a Convolutional Neural Network (CNN) for image classification. Finally, we use the Mahalanobis Distance to compare each class of classification results to a neighborhood. Our solution performs well, irrespective of image size, as evidenced by numerical findings on the current dataset, which show an accuracy of 97.71%, a precision of 96.69%, a recall of 94.33%, and an F-score of 0.941%.
ER  - 


TY  - CONF
TI  - Orientational Spatial Part Modeling for Fine-Grained Visual Categorization
T2  - 2015 IEEE International Conference on Mobile Services
SP  - 360
EP  - 367
AU  - H. Yao
AU  - S. Zhang
AU  - F. Xie
AU  - Y. Zhang
AU  - D. Zhang
AU  - Y. Su
AU  - Q. Tian
PY  - 2015
KW  - Feature extraction
KW  - Training
KW  - Visualization
KW  - Testing
KW  - Aircraft
KW  - Histograms
KW  - Computational modeling
KW  - OSP
KW  - Fine-Grained Visual Categorization
KW  - dCNN
DO  - 10.1109/MobServ.2015.56
JO  - 2015 IEEE International Conference on Mobile Services
IS  - 
SN  - 2329-6453
VO  - 
VL  - 
JA  - 2015 IEEE International Conference on Mobile Services
Y1  - 27 June-2 July 2015
AB  - Although significant success has been achieved in fine-grained visual categorization, most of existing methods require bounding boxes or part annotations for training and test, resulting in limited usability and flexibility. To conquer these limitations, we aim to automatically detect the bounding box and parts for fine-grained object classification. The bounding boxes are acquired by a transferring strategy which infers the locations of objects from a set of annotated training images. Based on the generated bounding box, we propose a multiple-layer Orientational Spatial Part (OSP) model to generate a refined description for the object. Finally, we employ the output of deep Convolutional Neural Network (dCNN) as the feature and train a linear SVM as object classifier. Extensive experiments on public benchmark datasets manifest the impressive performance of our method, i.e., Classification accuracy achieves 63.9% on CUB-200-2011 and 75.6% on Aircraft, which are actually higher than many existing methods using manual annotations.
ER  - 


TY  - CONF
TI  - Satellite Hyperspectral Imagery Compression Algorithm Based on Adaptive Band Regrouping
T2  - 2006 International Conference on Wireless Communications, Networking and Mobile Computing
SP  - 1
EP  - 4
AU  - Z. Zhou
AU  - Y. Tan
AU  - J. Liu
PY  - 2006
KW  - Hyperspectral imaging
KW  - Compression algorithms
KW  - Image coding
KW  - Hyperspectral sensors
KW  - Military satellites
KW  - Decorrelation
KW  - Transform coding
KW  - Karhunen-Loeve transforms
KW  - Military aircraft
KW  - Vector quantization
DO  - 10.1109/WiCOM.2006.311
JO  - 2006 International Conference on Wireless Communications, Networking and Mobile Computing
IS  - 
SN  - 2161-9654
VO  - 
VL  - 
JA  - 2006 International Conference on Wireless Communications, Networking and Mobile Computing
Y1  - 22-24 Sept. 2006
AB  - Hyperspectral image from satellite is important for earth observation and military reconnaissance. However there exists big contradictory between the transmission capacity of satellite channel and large amount hyperspectral data. There are spatial and spectrum redundancy in hyperspectral image. As to exploit spectrum correlation sufficiently, it must be to pre-process hyperspectral image. In this paper, we propose a novel acceptable complexity lossy hyperspectral image compression scheme which combines the prediction based on adaptive band regrouping and JPEG2000-based coding algorithm. The regrouping includes band classification and reference frame selection. Our experiments show that the proposed approach has a good performance in quality and fidelity. It is effective and not complicated, which may be a good choice for hyperspectral compression in the embedded processor of satellite platform
ER  - 


TY  - CONF
TI  - A Vision Based Forced Landing Site Selection System for an Autonomous UAV
T2  - 2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing
SP  - 397
EP  - 402
AU  - D. Fitzgerald
AU  - R. Walker
AU  - D. Campbell
PY  - 2005
KW  - Unmanned aerial vehicles
KW  - Aircraft
KW  - Safety
KW  - Aerospace electronics
KW  - Australia
KW  - Explosives
KW  - Machine vision
KW  - Image processing
KW  - Neural networks
KW  - Government
DO  - 10.1109/ISSNIP.2005.1595612
JO  - 2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing
Y1  - 5-8 Dec. 2005
AB  - This paper presents a system overview of the UAV forced landing site selection system and the results to date. The forced landing problem is a new field of research for UAVs and this paper will show the machine vision approach taken to address this problem. The results are based on aerial imagery collected from a series of flight trials in a Cessna 172. The aim of this research is to locate candidate landing sites for UAV forced landings, from aerial imagery. Output image frames highlight the algorithm's selected safe landing locations. The algorithms for the problem use image processing techniques and neural networks for the classification problem. The system is capable of locating areas that are large enough to land in and that are free of obstacles 92.3% ± 2% (95% confidence) of the time. These areas identified are then further classified as to their surface type to a classification accuracy of 90% ± 3% (98% confidence). It should be noted that although the system is being designed primarily for the forced landing problem for UAVs, the research can also be applied to forced landings or glider applications for piloted aircraft.
ER  - 


TY  - CONF
TI  - Study on Evaluation System of Invasive Plant Micrantha Herbicide Guided by Computer Remote Sensing
T2  - 2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)
SP  - 1057
EP  - 1061
AU  - L. Shuqiang
AU  - C. Wei
PY  - 2023
KW  - Resistance
KW  - Image segmentation
KW  - Artificial neural networks
KW  - Cameras
KW  - Classification algorithms
KW  - Remote sensing
KW  - Chemicals
KW  - Computer
KW  - remote sensing
KW  - deep learning automatic recognition
KW  - herbicides
KW  - mikania micrantha
DO  - 10.1109/ICEACE60673.2023.10441862
JO  - 2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)
Y1  - 29-31 Dec. 2023
AB  - Micrantha was effectively controlled by computer remote sensing technology, and its herbicide effectiveness was evaluated. This project intends to use the unmanned aircraft carrying RGB cameras to collect the orthographic images of this region, and realize the accurate location of the outbreak point of Mikania micrantha in different seasons in this region through various algorithms such as band operation, image segmentation and deep neural network. Using the established classification system, rapid and efficient identification of the outbreak point of Mikania micrantha was realized. Using deep neural network (DeeplabV3+) method, the outbreak location and range of purple chamomile can be accurately determined. In the chemical experiment, 70% of glyphosate · triclopidine, 24% of drip acid · dichlopidine, 88.8% of glyphosate ammonium salt, 90% of glyphosate ammonium salt + phenylsulfuron, and 36% of 2 methyl glyphosate were rotated during the process of removing Mikania phenylsulfonyl.
ER  - 


TY  - CONF
TI  - Soil / crop segmentation from remotely sensed data acquired by Unmanned Aerial System
T2  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
SP  - 1410
EP  - 1417
AU  - A. Mancini
AU  - J. Dyson
AU  - E. Frontoni
AU  - P. Zingaretti
PY  - 2017
KW  - Soil
KW  - Agriculture
KW  - Vegetation
KW  - Image segmentation
KW  - Kernel
KW  - Convolution
KW  - Correlation
KW  - Segmentation
KW  - multi-spectral camera
KW  - soil
KW  - crop
KW  - UAV application
DO  - 10.1109/ICUAS.2017.7991526
JO  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 International Conference on Unmanned Aircraft Systems (ICUAS)
Y1  - 13-16 June 2017
AB  - Today Unmanned Aerial Systems (UAS) are widely used for many applications that involve advanced payload as is found to be the case for mounted remote sensing apparatus. Remote sensing from UAS platforms is now common and the use of light and smart multi/hyper-spectral cameras has opened the field to novel applications. These sensors can operate in cloudy conditions ensuring ultra high resolution images while at the same time overcoming the limitations of satellite photography. In this paper we focus on just one such advanced payload application, namely, the segmentation of treecover / canopies over soil terrain. This task is mandatory in order to mask-out areas that are not of direct interest. The approaches studied are based on both supervised and unsupervised algorithms which take into account multi-spectral as well as synthetic features derived from the Digital Surface Model (DSM). We process the DSM by testing 2D convolution kernels together with a pseudo-random image slicing that tries to derive/model the ground/soil profile. Global thresholding is not able the segment tree / canopy area over the soil because the terrain slope is subject to significant change over small areas as is often seen to be the case with vineyards. The proposed approach takes into account such local variability to ensure a correct segmentation analysis in presence of slopes or other undulatory terrain variations. The results obtained show that the proposed method enables the segmentation of tree / canopy vs soil with an overall accuracy greater than 95%.
ER  - 


TY  - CONF
TI  - An Aircraft Identification System Using Convolution Neural Networks
T2  - 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)
SP  - 1211
EP  - 1218
AU  - V. Turchenko
AU  - I. Turchenko
AU  - N. Vasylkiv
PY  - 2023
KW  - YOLO
KW  - Training
KW  - Visualization
KW  - Optical character recognition
KW  - Neural networks
KW  - Convolutional neural networks
KW  - Air traffic control
KW  - convolutional neural networks
KW  - detection
KW  - deep learning
KW  - transfer learning
KW  - YOLOv7
KW  - aircraft
DO  - 10.1109/IDAACS58523.2023.10348936
JO  - 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)
IS  - 
SN  - 2770-4254
VO  - 1
VL  - 1
JA  - 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)
Y1  - 7-9 Sept. 2023
AB  - The task of aircraft identification using their registration number is important as visual identification remains the only method of their identification in case of air traffic control systems or radar failure. In this paper, an analysis of known solutions was carried out and an aircraft identification system was developed using a pre-trained YOLO (You Only Look Once) deep convolutional neural network and Optical Character Recognition (OCR) technology. The system consists of three modules: the M1 module is designed for the detection of region of interest-tail, the M2 module is designed for the detection of region of interest-registration number, the M3 module is designed for the classification of the registration number. 2309 aircraft images were used for training, 426 for validation and 408 for testing from the Fine-Grained Visual Classification of Aircraft (FGVC-Aircraft) dataset. Our results show that the mean average precision for the M1 and M2 YOLOv7 models on the test set were 0.92 and 0.85 respectively, and for the module M3 OCR technology the classification error was 0.2.
ER  - 


TY  - CONF
TI  - Targeted Image Transformation for Improving Robustness in Long Range Aircraft Detection
T2  - 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 10431
EP  - 10438
AU  - R. Martin
AU  - C. Fung
AU  - N. Keetha
AU  - L. Bauer
AU  - S. Scherer
PY  - 2024
KW  - Visualization
KW  - Navigation
KW  - Pipelines
KW  - Lighting
KW  - LoRa
KW  - Object detection
KW  - Robustness
KW  - Standards
KW  - Signal to noise ratio
KW  - Meteorology
DO  - 10.1109/IROS58592.2024.10801304
JO  - 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 14-18 Oct. 2024
AB  - In the field of aviation, the Detect and Avoid (DAA) problem deals with incorporating collision avoidance capabilities into current autopilot navigation systems. As an application of the Small Object Detection (SOD) problem, DAA presents the difficulties of a low signal-to-noise ratio and far range detection. Visual DAA is also susceptible to changing weather and lighting conditions at deployment. While current literature has presented many solutions for this, prior work has yet to study the robustness of the learning-based models for DAA. In this work, we show that standard techniques for improving robustness for object detection do not produce the desired results for DAA given the SOD constraints. We present targeted transformations, a zero-shot technique that can significantly improve robustness with minimal impact on accuracy. We demonstrate how to construct these transformations and evaluate our method on the current SOTA model for DAA, showing a 53.6% increase in recall. This makes our pipeline more robust to changes in lighting and environmental factors, and better able to detect potential threats. In the future, we hope to automate the transformation selection process, making it easier to adopt in different use cases.
ER  - 


